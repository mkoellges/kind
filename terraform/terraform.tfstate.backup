{
  "version": 4,
  "terraform_version": "1.5.7",
  "serial": 596,
  "lineage": "4dc7ec77-3975-22bc-301d-767514bbfcdd",
  "outputs": {
    "cluster_name": {
      "value": "dev-1",
      "type": "string"
    },
    "kubeconfig_path": {
      "value": "~/.kube/config",
      "type": "string"
    },
    "kubernetes_version": {
      "value": "kindest/node:v1.27.3",
      "type": "string"
    }
  },
  "resources": [
    {
      "mode": "data",
      "type": "kubectl_file_documents",
      "name": "metallb",
      "provider": "provider[\"registry.terraform.io/gavinbunney/kubectl\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "content": "---\napiVersion: metallb.io/v1beta1\nkind: IPAddressPool\nmetadata:\n  name: example\n  namespace: metallb\nspec:\n  addresses:\n  - 172.18.0.200-172.18.0.250\n---\napiVersion: metallb.io/v1beta1\nkind: L2Advertisement\nmetadata:\n  name: empty\n  namespace: metallb\nspec: {}",
            "documents": [
              "---\napiVersion: metallb.io/v1beta1\nkind: IPAddressPool\nmetadata:\n  name: example\n  namespace: metallb\nspec:\n  addresses:\n  - 172.18.0.200-172.18.0.250",
              "apiVersion: metallb.io/v1beta1\nkind: L2Advertisement\nmetadata:\n  name: empty\n  namespace: metallb\nspec: {}"
            ],
            "id": "fdb5fedc6f517bb2a4fb82d20242e798536a5788c114cb65e1c4461900e57155",
            "manifests": {
              "/apis/metallb.io/v1beta1/namespaces/metallb/ipaddresspools/example": "apiVersion: metallb.io/v1beta1\nkind: IPAddressPool\nmetadata:\n  name: example\n  namespace: metallb\nspec:\n  addresses:\n  - 172.18.0.200-172.18.0.250\n",
              "/apis/metallb.io/v1beta1/namespaces/metallb/l2advertisements/empty": "apiVersion: metallb.io/v1beta1\nkind: L2Advertisement\nmetadata:\n  name: empty\n  namespace: metallb\nspec: {}\n"
            }
          },
          "sensitive_attributes": []
        }
      ]
    },
    {
      "mode": "managed",
      "type": "helm_release",
      "name": "argocd",
      "provider": "provider[\"registry.terraform.io/hashicorp/helm\"]",
      "instances": [
        {
          "schema_version": 1,
          "attributes": {
            "atomic": false,
            "chart": "argo-cd",
            "cleanup_on_fail": false,
            "create_namespace": true,
            "dependency_update": false,
            "description": null,
            "devel": null,
            "disable_crd_hooks": false,
            "disable_openapi_validation": false,
            "disable_webhooks": false,
            "force_update": false,
            "id": "argocd",
            "keyring": null,
            "lint": false,
            "manifest": null,
            "max_history": 0,
            "metadata": [
              {
                "app_version": "v2.9.3",
                "chart": "argo-cd",
                "name": "argocd",
                "namespace": "argocd",
                "revision": 1,
                "values": "{\"apiVersionOverrides\":{\"cloudgoogle\":\"\"},\"applicationSet\":{\"affinity\":{},\"args\":{},\"certificate\":{\"additionalHosts\":[],\"annotations\":{},\"domain\":\"argocd.example.com\",\"duration\":\"\",\"enabled\":false,\"issuer\":{\"group\":\"\",\"kind\":\"\",\"name\":\"\"},\"privateKey\":{\"algorithm\":\"RSA\",\"encoding\":\"PKCS1\",\"rotationPolicy\":\"Never\",\"size\":2048},\"renewBefore\":\"\",\"secretName\":\"argocd-application-controller-tls\"},\"containerPorts\":{\"metrics\":8080,\"probe\":8081,\"webhook\":7000},\"containerSecurityContext\":{\"allowPrivilegeEscalation\":false,\"capabilities\":{\"drop\":[\"ALL\"]},\"readOnlyRootFilesystem\":true,\"runAsNonRoot\":true,\"seccompProfile\":{\"type\":\"RuntimeDefault\"}},\"deploymentAnnotations\":{},\"deploymentStrategy\":{},\"dnsConfig\":{},\"dnsPolicy\":\"ClusterFirst\",\"enabled\":true,\"extraArgs\":[],\"extraContainers\":[],\"extraEnv\":[],\"extraEnvFrom\":[],\"extraVolumeMounts\":[],\"extraVolumes\":[],\"image\":{\"imagePullPolicy\":\"\",\"repository\":\"\",\"tag\":\"\"},\"imagePullSecrets\":[],\"initContainers\":[],\"livenessProbe\":{\"enabled\":false,\"failureThreshold\":3,\"initialDelaySeconds\":10,\"periodSeconds\":10,\"successThreshold\":1,\"timeoutSeconds\":1},\"metrics\":{\"enabled\":false,\"service\":{\"annotations\":{},\"clusterIP\":\"\",\"labels\":{},\"portName\":\"http-metrics\",\"servicePort\":8080,\"type\":\"ClusterIP\"},\"serviceMonitor\":{\"additionalLabels\":{},\"annotations\":{},\"enabled\":false,\"interval\":\"30s\",\"metricRelabelings\":[],\"namespace\":\"\",\"relabelings\":[],\"scheme\":\"\",\"selector\":{},\"tlsConfig\":{}}},\"name\":\"applicationset-controller\",\"nodeSelector\":{},\"pdb\":{\"annotations\":{},\"enabled\":false,\"labels\":{},\"maxUnavailable\":\"\",\"minAvailable\":\"\"},\"podAnnotations\":{},\"podLabels\":{},\"priorityClassName\":\"\",\"readinessProbe\":{\"enabled\":false,\"failureThreshold\":3,\"initialDelaySeconds\":10,\"periodSeconds\":10,\"successThreshold\":1,\"timeoutSeconds\":1},\"replicas\":1,\"resources\":{},\"service\":{\"annotations\":{},\"labels\":{},\"port\":7000,\"portName\":\"webhook\",\"type\":\"ClusterIP\"},\"serviceAccount\":{\"annotations\":{},\"automountServiceAccountToken\":true,\"create\":true,\"labels\":{},\"name\":\"argocd-applicationset-controller\"},\"terminationGracePeriodSeconds\":30,\"tolerations\":[],\"topologySpreadConstraints\":[],\"webhook\":{\"ingress\":{\"annotations\":{},\"enabled\":false,\"extraPaths\":[],\"hosts\":[],\"ingressClassName\":\"\",\"labels\":{},\"pathType\":\"Prefix\",\"paths\":[\"/api/webhook\"],\"tls\":[]}}},\"configs\":{\"clusterCredentials\":[],\"cm\":{\"admin.enabled\":true,\"annotations\":{},\"application.instanceLabelKey\":\"argocd.argoproj.io/instance\",\"create\":true,\"exec.enabled\":false,\"server.rbac.log.enforce.enable\":false,\"timeout.hard.reconciliation\":\"0s\",\"timeout.reconciliation\":\"180s\",\"url\":\"\"},\"cmp\":{\"annotations\":{},\"create\":false,\"plugins\":{}},\"credentialTemplates\":{},\"credentialTemplatesAnnotations\":{},\"gpg\":{\"annotations\":{},\"keys\":{}},\"params\":{\"annotations\":{},\"application.namespaces\":\"\",\"applicationsetcontroller.enable.progressive.syncs\":false,\"applicationsetcontroller.policy\":\"sync\",\"controller.operation.processors\":10,\"controller.repo.server.timeout.seconds\":60,\"controller.self.heal.timeout.seconds\":5,\"controller.status.processors\":20,\"create\":true,\"otlp.address\":\"\",\"reposerver.parallelism.limit\":0,\"server.basehref\":\"/\",\"server.disable.auth\":false,\"server.enable.gzip\":true,\"server.insecure\":true,\"server.rootpath\":\"\",\"server.staticassets\":\"/shared/app\",\"server.x.frame.options\":\"sameorigin\"},\"rbac\":{\"annotations\":{},\"create\":true,\"policy.csv\":\"\",\"policy.default\":\"\",\"scopes\":\"[groups]\"},\"repositories\":{},\"repositoriesAnnotations\":{},\"secret\":{\"annotations\":{},\"argocdServerAdminPassword\":\"\",\"argocdServerAdminPasswordMtime\":\"\",\"bitbucketServerSecret\":\"\",\"bitbucketUUID\":\"\",\"createSecret\":true,\"extra\":{},\"githubSecret\":\"\",\"gitlabSecret\":\"\",\"gogsSecret\":\"\",\"labels\":{}},\"ssh\":{\"annotations\":{},\"extraHosts\":\"\",\"knownHosts\":\"[ssh.github.com]:443 ecdsa-sha2-nistp256 AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBEmKSENjQEezOmxkZMy7opKgwFB9nkt5YRrYMjNuG5N87uRgg6CLrbo5wAdT/y6v0mKV0U2w0WZ2YB/++Tpockg=\\n[ssh.github.com]:443 ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIOMqqnkVzrm0SdG6UOoqKLsabgH5C9okWi0dh2l9GKJl\\n[ssh.github.com]:443 ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCj7ndNxQowgcQnjshcLrqPEiiphnt+VTTvDP6mHBL9j1aNUkY4Ue1gvwnGLVlOhGeYrnZaMgRK6+PKCUXaDbC7qtbW8gIkhL7aGCsOr/C56SJMy/BCZfxd1nWzAOxSDPgVsmerOBYfNqltV9/hWCqBywINIR+5dIg6JTJ72pcEpEjcYgXkE2YEFXV1JHnsKgbLWNlhScqb2UmyRkQyytRLtL+38TGxkxCflmO+5Z8CSSNY7GidjMIZ7Q4zMjA2n1nGrlTDkzwDCsw+wqFPGQA179cnfGWOWRVruj16z6XyvxvjJwbz0wQZ75XK5tKSb7FNyeIEs4TT4jk+S4dhPeAUC5y+bDYirYgM4GC7uEnztnZyaVWQ7B381AK4Qdrwt51ZqExKbQpTUNn+EjqoTwvqNj4kqx5QUCI0ThS/YkOxJCXmPUWZbhjpCg56i+2aB6CmK2JGhn57K5mj0MNdBXA4/WnwH6XoPWJzK5Nyu2zB3nAZp+S5hpQs+p1vN1/wsjk=\\nbitbucket.org ecdsa-sha2-nistp256 AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBPIQmuzMBuKdWeF4+a2sjSSpBK0iqitSQ+5BM9KhpexuGt20JpTVM7u5BDZngncgrqDMbWdxMWWOGtZ9UgbqgZE=\\nbitbucket.org ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIIazEu89wgQZ4bqs3d63QSMzYVa0MuJ2e2gKTKqu+UUO\\nbitbucket.org ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDQeJzhupRu0u0cdegZIa8e86EG2qOCsIsD1Xw0xSeiPDlCr7kq97NLmMbpKTX6Esc30NuoqEEHCuc7yWtwp8dI76EEEB1VqY9QJq6vk+aySyboD5QF61I/1WeTwu+deCbgKMGbUijeXhtfbxSxm6JwGrXrhBdofTsbKRUsrN1WoNgUa8uqN1Vx6WAJw1JHPhglEGGHea6QICwJOAr/6mrui/oB7pkaWKHj3z7d1IC4KWLtY47elvjbaTlkN04Kc/5LFEirorGYVbt15kAUlqGM65pk6ZBxtaO3+30LVlORZkxOh+LKL/BvbZ/iRNhItLqNyieoQj/uh/7Iv4uyH/cV/0b4WDSd3DptigWq84lJubb9t/DnZlrJazxyDCulTmKdOR7vs9gMTo+uoIrPSb8ScTtvw65+odKAlBj59dhnVp9zd7QUojOpXlL62Aw56U4oO+FALuevvMjiWeavKhJqlR7i5n9srYcrNV7ttmDw7kf/97P5zauIhxcjX+xHv4M=\\ngithub.com ecdsa-sha2-nistp256 AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBEmKSENjQEezOmxkZMy7opKgwFB9nkt5YRrYMjNuG5N87uRgg6CLrbo5wAdT/y6v0mKV0U2w0WZ2YB/++Tpockg=\\ngithub.com ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIOMqqnkVzrm0SdG6UOoqKLsabgH5C9okWi0dh2l9GKJl\\ngithub.com ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCj7ndNxQowgcQnjshcLrqPEiiphnt+VTTvDP6mHBL9j1aNUkY4Ue1gvwnGLVlOhGeYrnZaMgRK6+PKCUXaDbC7qtbW8gIkhL7aGCsOr/C56SJMy/BCZfxd1nWzAOxSDPgVsmerOBYfNqltV9/hWCqBywINIR+5dIg6JTJ72pcEpEjcYgXkE2YEFXV1JHnsKgbLWNlhScqb2UmyRkQyytRLtL+38TGxkxCflmO+5Z8CSSNY7GidjMIZ7Q4zMjA2n1nGrlTDkzwDCsw+wqFPGQA179cnfGWOWRVruj16z6XyvxvjJwbz0wQZ75XK5tKSb7FNyeIEs4TT4jk+S4dhPeAUC5y+bDYirYgM4GC7uEnztnZyaVWQ7B381AK4Qdrwt51ZqExKbQpTUNn+EjqoTwvqNj4kqx5QUCI0ThS/YkOxJCXmPUWZbhjpCg56i+2aB6CmK2JGhn57K5mj0MNdBXA4/WnwH6XoPWJzK5Nyu2zB3nAZp+S5hpQs+p1vN1/wsjk=\\ngitlab.com ecdsa-sha2-nistp256 AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBFSMqzJeV9rUzU4kWitGjeR4PWSa29SPqJ1fVkhtj3Hw9xjLVXVYrU9QlYWrOLXBpQ6KWjbjTDTdDkoohFzgbEY=\\ngitlab.com ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIAfuCHKVTjquxvt6CM6tdG4SLp1Btn/nOeHHE5UOzRdf\\ngitlab.com ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCsj2bNKTBSpIYDEGk9KxsGh3mySTRgMtXL583qmBpzeQ+jqCMRgBqB98u3z++J1sKlXHWfM9dyhSevkMwSbhoR8XIq/U0tCNyokEi/ueaBMCvbcTHhO7FcwzY92WK4Yt0aGROY5qX2UKSeOvuP4D6TPqKF1onrSzH9bx9XUf2lEdWT/ia1NEKjunUqu1xOB/StKDHMoX4/OKyIzuS0q/T1zOATthvasJFoPrAjkohTyaDUz2LN5JoH839hViyEG82yB+MjcFV5MU3N1l1QL3cVUCh93xSaua1N85qivl+siMkPGbO5xR/En4iEY6K2XPASUEMaieWVNTRCtJ4S8H+9\\nssh.dev.azure.com ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC7Hr1oTWqNqOlzGJOfGJ4NakVyIzf1rXYd4d7wo6jBlkLvCA4odBlL0mDUyZ0/QUfTTqeu+tm22gOsv+VrVTMk6vwRU75gY/y9ut5Mb3bR5BV58dKXyq9A9UeB5Cakehn5Zgm6x1mKoVyf+FFn26iYqXJRgzIZZcZ5V6hrE0Qg39kZm4az48o0AUbf6Sp4SLdvnuMa2sVNwHBboS7EJkm57XQPVU3/QpyNLHbWDdzwtrlS+ez30S3AdYhLKEOxAG8weOnyrtLJAUen9mTkol8oII1edf7mWWbWVf0nBmly21+nZcmCTISQBtdcyPaEno7fFQMDD26/s0lfKob4Kw8H\\nvs-ssh.visualstudio.com ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC7Hr1oTWqNqOlzGJOfGJ4NakVyIzf1rXYd4d7wo6jBlkLvCA4odBlL0mDUyZ0/QUfTTqeu+tm22gOsv+VrVTMk6vwRU75gY/y9ut5Mb3bR5BV58dKXyq9A9UeB5Cakehn5Zgm6x1mKoVyf+FFn26iYqXJRgzIZZcZ5V6hrE0Qg39kZm4az48o0AUbf6Sp4SLdvnuMa2sVNwHBboS7EJkm57XQPVU3/QpyNLHbWDdzwtrlS+ez30S3AdYhLKEOxAG8weOnyrtLJAUen9mTkol8oII1edf7mWWbWVf0nBmly21+nZcmCTISQBtdcyPaEno7fFQMDD26/s0lfKob4Kw8H\\n\"},\"styles\":\"\",\"tls\":{\"annotations\":{},\"certificates\":{}}},\"controller\":{\"affinity\":{},\"args\":{},\"clusterRoleRules\":{\"enabled\":false,\"rules\":[]},\"containerPorts\":{\"metrics\":8082},\"containerSecurityContext\":{\"allowPrivilegeEscalation\":false,\"capabilities\":{\"drop\":[\"ALL\"]},\"readOnlyRootFilesystem\":true,\"runAsNonRoot\":true,\"seccompProfile\":{\"type\":\"RuntimeDefault\"}},\"dnsConfig\":{},\"dnsPolicy\":\"ClusterFirst\",\"env\":[],\"envFrom\":[],\"extraArgs\":[],\"extraContainers\":[],\"hostNetwork\":false,\"image\":{\"imagePullPolicy\":\"\",\"repository\":\"\",\"tag\":\"\"},\"imagePullSecrets\":[],\"initContainers\":[],\"metrics\":{\"applicationLabels\":{\"enabled\":false,\"labels\":[]},\"enabled\":false,\"rules\":{\"additionalLabels\":{},\"annotations\":{},\"enabled\":false,\"namespace\":\"monitoring\",\"selector\":{},\"spec\":[]},\"service\":{\"annotations\":{},\"clusterIP\":\"\",\"labels\":{},\"portName\":\"http-metrics\",\"servicePort\":8082,\"type\":\"ClusterIP\"},\"serviceMonitor\":{\"additionalLabels\":{},\"annotations\":{},\"enabled\":false,\"interval\":\"30s\",\"metricRelabelings\":[],\"namespace\":\"monitoring\",\"relabelings\":[],\"scheme\":\"\",\"selector\":{},\"tlsConfig\":{}}},\"name\":\"application-controller\",\"nodeSelector\":{},\"pdb\":{\"annotations\":{},\"enabled\":false,\"labels\":{},\"maxUnavailable\":\"\",\"minAvailable\":\"\"},\"podAnnotations\":{},\"podLabels\":{},\"priorityClassName\":\"\",\"readinessProbe\":{\"failureThreshold\":3,\"initialDelaySeconds\":10,\"periodSeconds\":10,\"successThreshold\":1,\"timeoutSeconds\":1},\"replicas\":1,\"resources\":{},\"serviceAccount\":{\"annotations\":{},\"automountServiceAccountToken\":true,\"create\":true,\"labels\":{},\"name\":\"argocd-application-controller\"},\"statefulsetAnnotations\":{},\"terminationGracePeriodSeconds\":30,\"tolerations\":[],\"topologySpreadConstraints\":[],\"volumeMounts\":[],\"volumes\":[]},\"crds\":{\"additionalLabels\":{},\"annotations\":{},\"install\":true,\"keep\":true},\"createAggregateRoles\":false,\"createClusterRoles\":true,\"dex\":{\"affinity\":{},\"certificateSecret\":{\"annotations\":{},\"ca\":\"\",\"crt\":\"\",\"enabled\":false,\"key\":\"\",\"labels\":{}},\"containerPorts\":{\"grpc\":5557,\"http\":5556,\"metrics\":5558},\"containerSecurityContext\":{\"allowPrivilegeEscalation\":false,\"capabilities\":{\"drop\":[\"ALL\"]},\"readOnlyRootFilesystem\":true,\"runAsNonRoot\":true,\"seccompProfile\":{\"type\":\"RuntimeDefault\"}},\"deploymentAnnotations\":{},\"deploymentStrategy\":{},\"dnsConfig\":{},\"dnsPolicy\":\"ClusterFirst\",\"enabled\":true,\"env\":[],\"envFrom\":[],\"extraArgs\":[],\"extraContainers\":[],\"image\":{\"imagePullPolicy\":\"\",\"repository\":\"ghcr.io/dexidp/dex\",\"tag\":\"v2.37.0\"},\"imagePullSecrets\":[],\"initContainers\":[],\"initImage\":{\"imagePullPolicy\":\"\",\"repository\":\"\",\"resources\":{},\"tag\":\"\"},\"livenessProbe\":{\"enabled\":false,\"failureThreshold\":3,\"initialDelaySeconds\":10,\"periodSeconds\":10,\"successThreshold\":1,\"timeoutSeconds\":1},\"logFormat\":\"\",\"logLevel\":\"\",\"metrics\":{\"enabled\":false,\"service\":{\"annotations\":{},\"labels\":{},\"portName\":\"http-metrics\"},\"serviceMonitor\":{\"additionalLabels\":{},\"annotations\":{},\"enabled\":false,\"interval\":\"30s\",\"metricRelabelings\":[],\"namespace\":\"monitoring\",\"relabelings\":[],\"scheme\":\"\",\"selector\":{},\"tlsConfig\":{}}},\"name\":\"dex-server\",\"nodeSelector\":{},\"pdb\":{\"annotations\":{},\"enabled\":false,\"labels\":{},\"maxUnavailable\":\"\",\"minAvailable\":\"\"},\"podAnnotations\":{},\"podLabels\":{},\"priorityClassName\":\"\",\"readinessProbe\":{\"enabled\":false,\"failureThreshold\":3,\"initialDelaySeconds\":10,\"periodSeconds\":10,\"successThreshold\":1,\"timeoutSeconds\":1},\"resources\":{},\"serviceAccount\":{\"annotations\":{},\"automountServiceAccountToken\":true,\"create\":true,\"name\":\"argocd-dex-server\"},\"servicePortGrpc\":5557,\"servicePortGrpcName\":\"grpc\",\"servicePortHttp\":5556,\"servicePortHttpName\":\"http\",\"servicePortMetrics\":5558,\"terminationGracePeriodSeconds\":30,\"tolerations\":[],\"topologySpreadConstraints\":[],\"volumeMounts\":[],\"volumes\":[]},\"externalRedis\":{\"existingSecret\":\"\",\"host\":\"\",\"password\":\"\",\"port\":6379,\"secretAnnotations\":{},\"username\":\"\"},\"extraObjects\":[],\"fullnameOverride\":\"\",\"global\":{\"addPrometheusAnnotations\":false,\"additionalLabels\":{},\"affinity\":{\"nodeAffinity\":{\"matchExpressions\":[],\"type\":\"hard\"},\"podAntiAffinity\":\"soft\"},\"certificateAnnotations\":{},\"deploymentAnnotations\":{},\"deploymentStrategy\":{},\"env\":[],\"hostAliases\":[],\"image\":{\"imagePullPolicy\":\"IfNotPresent\",\"repository\":\"quay.io/argoproj/argocd\",\"tag\":\"\"},\"imagePullSecrets\":[],\"logging\":{\"format\":\"text\",\"level\":\"info\"},\"networkPolicy\":{\"create\":false,\"defaultDenyIngress\":false},\"nodeSelector\":{},\"podAnnotations\":{},\"podLabels\":{},\"priorityClassName\":\"\",\"revisionHistoryLimit\":3,\"securityContext\":{},\"statefulsetAnnotations\":{},\"tolerations\":[],\"topologySpreadConstraints\":[]},\"kubeVersionOverride\":\"\",\"nameOverride\":\"argocd\",\"notifications\":{\"affinity\":{},\"argocdUrl\":null,\"clusterRoleRules\":{\"rules\":[]},\"cm\":{\"create\":true},\"containerPorts\":{\"metrics\":9001},\"containerSecurityContext\":{\"allowPrivilegeEscalation\":false,\"capabilities\":{\"drop\":[\"ALL\"]},\"readOnlyRootFilesystem\":true,\"runAsNonRoot\":true,\"seccompProfile\":{\"type\":\"RuntimeDefault\"}},\"context\":{},\"deploymentAnnotations\":{},\"deploymentStrategy\":{\"type\":\"Recreate\"},\"dnsConfig\":{},\"dnsPolicy\":\"ClusterFirst\",\"enabled\":true,\"extraArgs\":[],\"extraContainers\":[],\"extraEnv\":[],\"extraEnvFrom\":[],\"extraVolumeMounts\":[],\"extraVolumes\":[],\"image\":{\"imagePullPolicy\":\"\",\"repository\":\"\",\"tag\":\"\"},\"imagePullSecrets\":[],\"initContainers\":[],\"logFormat\":\"\",\"logLevel\":\"\",\"metrics\":{\"enabled\":false,\"port\":9001,\"service\":{\"annotations\":{},\"clusterIP\":\"\",\"labels\":{},\"portName\":\"http-metrics\",\"type\":\"ClusterIP\"},\"serviceMonitor\":{\"additionalLabels\":{},\"annotations\":{},\"enabled\":false,\"metricRelabelings\":[],\"relabelings\":[],\"scheme\":\"\",\"selector\":{},\"tlsConfig\":{}}},\"name\":\"notifications-controller\",\"nodeSelector\":{},\"notifiers\":{},\"pdb\":{\"annotations\":{},\"enabled\":false,\"labels\":{},\"maxUnavailable\":\"\",\"minAvailable\":\"\"},\"podAnnotations\":{},\"podLabels\":{},\"priorityClassName\":\"\",\"resources\":{},\"secret\":{\"annotations\":{},\"create\":true,\"items\":{},\"labels\":{}},\"serviceAccount\":{\"annotations\":{},\"automountServiceAccountToken\":true,\"create\":true,\"labels\":{},\"name\":\"argocd-notifications-controller\"},\"subscriptions\":[],\"templates\":{},\"terminationGracePeriodSeconds\":30,\"tolerations\":[],\"topologySpreadConstraints\":[],\"triggers\":{}},\"openshift\":{\"enabled\":false},\"redis\":{\"affinity\":{},\"containerPorts\":{\"metrics\":9121,\"redis\":6379},\"containerSecurityContext\":{\"allowPrivilegeEscalation\":false,\"capabilities\":{\"drop\":[\"ALL\"]},\"readOnlyRootFilesystem\":true},\"deploymentAnnotations\":{},\"dnsConfig\":{},\"dnsPolicy\":\"ClusterFirst\",\"enabled\":true,\"env\":[],\"envFrom\":[],\"exporter\":{\"containerSecurityContext\":{\"allowPrivilegeEscalation\":false,\"capabilities\":{\"drop\":[\"ALL\"]},\"readOnlyRootFilesystem\":true,\"runAsNonRoot\":true,\"seccompProfile\":{\"type\":\"RuntimeDefault\"}},\"enabled\":false,\"env\":[],\"image\":{\"imagePullPolicy\":\"\",\"repository\":\"public.ecr.aws/bitnami/redis-exporter\",\"tag\":\"1.53.0\"},\"resources\":{}},\"extraArgs\":[],\"extraContainers\":[],\"image\":{\"imagePullPolicy\":\"\",\"repository\":\"public.ecr.aws/docker/library/redis\",\"tag\":\"7.0.13-alpine\"},\"imagePullSecrets\":[],\"initContainers\":[],\"metrics\":{\"enabled\":false,\"service\":{\"annotations\":{},\"clusterIP\":\"None\",\"labels\":{},\"portName\":\"http-metrics\",\"servicePort\":9121,\"type\":\"ClusterIP\"},\"serviceMonitor\":{\"additionalLabels\":{},\"annotations\":{},\"enabled\":false,\"interval\":\"30s\",\"metricRelabelings\":[],\"namespace\":\"monitoring\",\"relabelings\":[],\"scheme\":\"\",\"selector\":{},\"tlsConfig\":{}}},\"name\":\"redis\",\"nodeSelector\":{},\"pdb\":{\"annotations\":{},\"enabled\":false,\"labels\":{},\"maxUnavailable\":\"\",\"minAvailable\":\"\"},\"podAnnotations\":{},\"podLabels\":{},\"priorityClassName\":\"\",\"resources\":{},\"securityContext\":{\"runAsNonRoot\":true,\"runAsUser\":999,\"seccompProfile\":{\"type\":\"RuntimeDefault\"}},\"service\":{\"annotations\":{},\"labels\":{}},\"serviceAccount\":{\"annotations\":{},\"automountServiceAccountToken\":false,\"create\":false,\"name\":\"\"},\"servicePort\":6379,\"terminationGracePeriodSeconds\":30,\"tolerations\":[],\"topologySpreadConstraints\":[],\"volumeMounts\":[],\"volumes\":[]},\"redis-ha\":{\"additionalAffinities\":{},\"affinity\":\"\",\"containerSecurityContext\":{\"readOnlyRootFilesystem\":true},\"enabled\":false,\"exporter\":{\"enabled\":false,\"image\":\"public.ecr.aws/bitnami/redis-exporter\",\"tag\":\"1.53.0\"},\"haproxy\":{\"additionalAffinities\":{},\"affinity\":\"\",\"containerSecurityContext\":{\"readOnlyRootFilesystem\":true},\"enabled\":true,\"hardAntiAffinity\":true,\"metrics\":{\"enabled\":true},\"tolerations\":[]},\"hardAntiAffinity\":true,\"image\":{\"repository\":\"redis\",\"tag\":\"7.0.13-alpine\"},\"persistentVolume\":{\"enabled\":false},\"redis\":{\"config\":{\"save\":\"\\\"\\\"\"},\"masterGroupName\":\"argocd\"},\"tolerations\":[],\"topologySpreadConstraints\":{\"enabled\":false,\"maxSkew\":\"\",\"topologyKey\":\"\",\"whenUnsatisfiable\":\"\"}},\"repoServer\":{\"affinity\":{},\"autoscaling\":{\"behavior\":{},\"enabled\":false,\"maxReplicas\":5,\"metrics\":[],\"minReplicas\":1,\"targetCPUUtilizationPercentage\":50,\"targetMemoryUtilizationPercentage\":50},\"certificateSecret\":{\"annotations\":{},\"ca\":\"\",\"crt\":\"\",\"enabled\":false,\"key\":\"\",\"labels\":{}},\"clusterRoleRules\":{\"enabled\":false,\"rules\":[]},\"containerPorts\":{\"metrics\":8084,\"server\":8081},\"containerSecurityContext\":{\"allowPrivilegeEscalation\":false,\"capabilities\":{\"drop\":[\"ALL\"]},\"readOnlyRootFilesystem\":true,\"runAsNonRoot\":true,\"seccompProfile\":{\"type\":\"RuntimeDefault\"}},\"deploymentAnnotations\":{},\"deploymentStrategy\":{},\"dnsConfig\":{},\"dnsPolicy\":\"ClusterFirst\",\"env\":[],\"envFrom\":[],\"extraArgs\":[],\"extraContainers\":[],\"hostNetwork\":false,\"image\":{\"imagePullPolicy\":\"\",\"repository\":\"\",\"tag\":\"\"},\"imagePullSecrets\":[],\"initContainers\":[],\"lifecycle\":{},\"livenessProbe\":{\"failureThreshold\":3,\"initialDelaySeconds\":10,\"periodSeconds\":10,\"successThreshold\":1,\"timeoutSeconds\":1},\"metrics\":{\"enabled\":false,\"service\":{\"annotations\":{},\"clusterIP\":\"\",\"labels\":{},\"portName\":\"http-metrics\",\"servicePort\":8084,\"type\":\"ClusterIP\"},\"serviceMonitor\":{\"additionalLabels\":{},\"annotations\":{},\"enabled\":false,\"interval\":\"30s\",\"metricRelabelings\":[],\"namespace\":\"\",\"relabelings\":[],\"scheme\":\"\",\"selector\":{},\"tlsConfig\":{}}},\"name\":\"repo-server\",\"nodeSelector\":{},\"pdb\":{\"annotations\":{},\"enabled\":false,\"labels\":{},\"maxUnavailable\":\"\",\"minAvailable\":\"\"},\"podAnnotations\":{},\"podLabels\":{},\"priorityClassName\":\"\",\"rbac\":[],\"readinessProbe\":{\"failureThreshold\":3,\"initialDelaySeconds\":10,\"periodSeconds\":10,\"successThreshold\":1,\"timeoutSeconds\":1},\"replicas\":1,\"resources\":{},\"service\":{\"annotations\":{},\"labels\":{},\"port\":8081,\"portName\":\"https-repo-server\"},\"serviceAccount\":{\"annotations\":{},\"automountServiceAccountToken\":true,\"create\":true,\"labels\":{},\"name\":\"\"},\"terminationGracePeriodSeconds\":30,\"tolerations\":[],\"topologySpreadConstraints\":[],\"useEphemeralHelmWorkingDir\":true,\"volumeMounts\":[],\"volumes\":[]},\"server\":{\"GKEbackendConfig\":{\"enabled\":false,\"spec\":{}},\"GKEfrontendConfig\":{\"enabled\":false,\"spec\":{}},\"GKEmanagedCertificate\":{\"domains\":[\"argocd.example.com\"],\"enabled\":false},\"affinity\":{},\"autoscaling\":{\"behavior\":{},\"enabled\":false,\"maxReplicas\":5,\"metrics\":[],\"minReplicas\":1,\"targetCPUUtilizationPercentage\":50,\"targetMemoryUtilizationPercentage\":50},\"certificate\":{\"additionalHosts\":[],\"annotations\":{},\"domain\":\"argocd.example.com\",\"duration\":\"\",\"enabled\":false,\"issuer\":{\"group\":\"\",\"kind\":\"\",\"name\":\"\"},\"privateKey\":{\"algorithm\":\"RSA\",\"encoding\":\"PKCS1\",\"rotationPolicy\":\"Never\",\"size\":2048},\"renewBefore\":\"\",\"secretName\":\"argocd-server-tls\",\"usages\":[]},\"certificateSecret\":{\"annotations\":{},\"crt\":\"\",\"enabled\":false,\"key\":\"\",\"labels\":{}},\"containerPorts\":{\"metrics\":8083,\"server\":8080},\"containerSecurityContext\":{\"allowPrivilegeEscalation\":false,\"capabilities\":{\"drop\":[\"ALL\"]},\"readOnlyRootFilesystem\":true,\"runAsNonRoot\":true,\"seccompProfile\":{\"type\":\"RuntimeDefault\"}},\"deploymentAnnotations\":{},\"deploymentStrategy\":{},\"dnsConfig\":{},\"dnsPolicy\":\"ClusterFirst\",\"env\":[],\"envFrom\":[],\"extensions\":{\"containerSecurityContext\":{\"allowPrivilegeEscalation\":false,\"capabilities\":{\"drop\":[\"ALL\"]},\"readOnlyRootFilesystem\":true,\"runAsNonRoot\":true,\"seccompProfile\":{\"type\":\"RuntimeDefault\"}},\"enabled\":false,\"image\":{\"imagePullPolicy\":\"\",\"repository\":\"ghcr.io/argoproj-labs/argocd-extensions\",\"tag\":\"v0.2.1\"},\"resources\":{}},\"extraArgs\":[],\"extraContainers\":[],\"hostNetwork\":false,\"image\":{\"imagePullPolicy\":\"\",\"repository\":\"\",\"tag\":\"\"},\"imagePullSecrets\":[],\"ingress\":{\"annotations\":{\"cert-manager.io/cluster-issuer\":\"letsencrypt-prod\"},\"enabled\":true,\"extraPaths\":[],\"hosts\":[\"argocd.koellgma.de\"],\"https\":false,\"ingressClassName\":\"nginx\",\"labels\":{},\"pathType\":\"Prefix\",\"paths\":[\"/\"],\"tls\":[{\"hosts\":[\"argocd.koellgma.de\"],\"secretName\":\"argocd-tls\"}]},\"ingressGrpc\":{\"annotations\":{},\"awsALB\":{\"backendProtocolVersion\":\"HTTP2\",\"serviceType\":\"NodePort\"},\"enabled\":false,\"extraPaths\":[],\"hosts\":[],\"https\":false,\"ingressClassName\":\"\",\"isAWSALB\":false,\"labels\":{},\"pathType\":\"Prefix\",\"paths\":[\"/\"],\"tls\":[]},\"initContainers\":[],\"lifecycle\":{},\"livenessProbe\":{\"failureThreshold\":3,\"initialDelaySeconds\":10,\"periodSeconds\":10,\"successThreshold\":1,\"timeoutSeconds\":1},\"metrics\":{\"enabled\":false,\"service\":{\"annotations\":{},\"clusterIP\":\"\",\"labels\":{},\"portName\":\"http-metrics\",\"servicePort\":8083,\"type\":\"ClusterIP\"},\"serviceMonitor\":{\"additionalLabels\":{},\"annotations\":{},\"enabled\":false,\"interval\":\"30s\",\"metricRelabelings\":[],\"namespace\":\"\",\"relabelings\":[],\"scheme\":\"\",\"selector\":{},\"tlsConfig\":{}}},\"name\":\"server\",\"nodeSelector\":{},\"pdb\":{\"annotations\":{},\"enabled\":false,\"labels\":{},\"maxUnavailable\":\"\",\"minAvailable\":\"\"},\"podAnnotations\":{},\"podLabels\":{},\"priorityClassName\":\"\",\"readinessProbe\":{\"failureThreshold\":3,\"initialDelaySeconds\":10,\"periodSeconds\":10,\"successThreshold\":1,\"timeoutSeconds\":1},\"replicas\":1,\"resources\":{},\"route\":{\"annotations\":{},\"enabled\":false,\"hostname\":\"\",\"termination_policy\":\"None\",\"termination_type\":\"passthrough\"},\"service\":{\"annotations\":{},\"externalIPs\":[],\"externalTrafficPolicy\":\"\",\"labels\":{},\"loadBalancerIP\":\"\",\"loadBalancerSourceRanges\":[],\"nodePortHttp\":30080,\"nodePortHttps\":30443,\"servicePortHttp\":80,\"servicePortHttpName\":\"http\",\"servicePortHttps\":443,\"servicePortHttpsName\":\"https\",\"sessionAffinity\":\"\",\"type\":\"ClusterIP\"},\"serviceAccount\":{\"annotations\":{},\"automountServiceAccountToken\":true,\"create\":true,\"labels\":{},\"name\":\"argocd-server\"},\"terminationGracePeriodSeconds\":30,\"tolerations\":[],\"topologySpreadConstraints\":[],\"volumeMounts\":[],\"volumes\":[]}}",
                "version": "5.52.0"
              }
            ],
            "name": "argocd",
            "namespace": "argocd",
            "pass_credentials": false,
            "postrender": [],
            "recreate_pods": false,
            "render_subchart_notes": true,
            "replace": false,
            "repository": "https://argoproj.github.io/argo-helm",
            "repository_ca_file": null,
            "repository_cert_file": null,
            "repository_key_file": null,
            "repository_password": null,
            "repository_username": null,
            "reset_values": false,
            "reuse_values": false,
            "set": [],
            "set_list": [],
            "set_sensitive": [],
            "skip_crds": false,
            "status": "deployed",
            "timeout": 300,
            "values": [
              "nameOverride: argocd\n# -- String to fully override `\"argo-cd.fullname\"`\nfullnameOverride: \"\"\n# -- Override the Kubernetes version, which is used to evaluate certain manifests\nkubeVersionOverride: \"\"\n# Override APIVersions\n# If you want to template helm charts but cannot access k8s API server\n# you can set api versions here\napiVersionOverrides:\n  # -- String to override apiVersion of GKE resources rendered by this helm chart\n  cloudgoogle: \"\" # cloud.google.com/v1\n\n# -- Create aggregated roles that extend existing cluster roles to interact with argo-cd resources\n## Ref: https://kubernetes.io/docs/reference/access-authn-authz/rbac/#aggregated-clusterroles\ncreateAggregateRoles: false\n# -- Create cluster roles for cluster-wide installation.\n## Used when you manage applications in the same cluster where Argo CD runs\ncreateClusterRoles: true\n\nopenshift:\n  # -- enables using arbitrary uid for argo repo server\n  enabled: false\n\n## Custom resource configuration\ncrds:\n  # -- Install and upgrade CRDs\n  install: true\n  # -- Keep CRDs on chart uninstall\n  keep: true\n  # -- Annotations to be added to all CRDs\n  annotations: {}\n  # -- Addtional labels to be added to all CRDs\n  additionalLabels: {}\n\n## Globally shared configuration\nglobal:\n  # -- Common labels for the all resources\n  additionalLabels: {}\n    # app: argo-cd\n\n  # -- Number of old deployment ReplicaSets to retain. The rest will be garbage collected.\n  revisionHistoryLimit: 3\n\n  # Default image used by all components\n  image:\n    # -- If defined, a repository applied to all Argo CD deployments\n    repository: quay.io/argoproj/argocd\n    # -- Overrides the global Argo CD image tag whose default is the chart appVersion\n    tag: \"\"\n    # -- If defined, a imagePullPolicy applied to all Argo CD deployments\n    imagePullPolicy: IfNotPresent\n\n  # -- Secrets with credentials to pull images from a private registry\n  imagePullSecrets: []\n\n  # Default logging options used by all components\n  logging:\n    # -- Set the global logging format. Either: `text` or `json`\n    format: text\n    # -- Set the global logging level. One of: `debug`, `info`, `warn` or `error`\n    level: info\n\n  # -- Annotations for the all deployed Statefulsets\n  statefulsetAnnotations: {}\n\n  # -- Annotations for the all deployed Deployments\n  deploymentAnnotations: {}\n\n  # -- Annotations for the all deployed pods\n  podAnnotations: {}\n\n  # -- Labels for the all deployed pods\n  podLabels: {}\n\n  # -- Add Prometheus scrape annotations to all metrics services. This can be used as an alternative to the ServiceMonitors.\n  addPrometheusAnnotations: false\n\n  # -- Toggle and define pod-level security context.\n  # @default -- `{}` (See [values.yaml])\n  securityContext: {}\n  #  runAsUser: 999\n  #  runAsGroup: 999\n  #  fsGroup: 999\n\n  # -- Mapping between IP and hostnames that will be injected as entries in the pod's hosts files\n  hostAliases: []\n  # - ip: 10.20.30.40\n  #   hostnames:\n  #   - git.myhostname\n\n  # Default network policy rules used by all components\n  networkPolicy:\n    # -- Create NetworkPolicy objects for all components\n    create: false\n    # -- Default deny all ingress traffic\n    defaultDenyIngress: false\n\n  # -- Default priority class for all components\n  priorityClassName: \"\"\n\n  # -- Default node selector for all components\n  nodeSelector: {}\n\n  # -- Default tolerations for all components\n  tolerations: []\n\n  # Default affinity preset for all components\n  affinity:\n    # -- Default pod anti-affinity rules. Either: `none`, `soft` or `hard`\n    podAntiAffinity: soft\n    # Node affinity rules\n    nodeAffinity:\n      # -- Default node affinity rules. Either: `none`, `soft` or `hard`\n      type: hard\n      # -- Default match expressions for node affinity\n      matchExpressions: []\n        # - key: topology.kubernetes.io/zone\n        #   operator: In\n        #   values:\n        #    - antarctica-east1\n        #    - antarctica-west1\n\n  # -- Default [TopologySpreadConstraints] rules for all components\n  ## Ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/\n  ## If labelSelector is left out, it will default to the labelSelector of the component\n  topologySpreadConstraints: []\n    # - maxSkew: 1\n    #   topologyKey: topology.kubernetes.io/zone\n    #   whenUnsatisfiable: DoNotSchedule\n\n  # -- Deployment strategy for the all deployed Deployments\n  deploymentStrategy: {}\n    # type: RollingUpdate\n    # rollingUpdate:\n    #   maxSurge: 25%\n    #   maxUnavailable: 25%\n\n  # -- Environment variables to pass to all deployed Deployments\n  env: []\n\n  # -- Annotations for the all deployed Certificates\n  certificateAnnotations: {}\n\n## Argo Configs\nconfigs:\n  # General Argo CD configuration\n  ## Ref: https://github.com/argoproj/argo-cd/blob/master/docs/operator-manual/argocd-cm.yaml\n  cm:\n    # -- Create the argocd-cm configmap for [declarative setup]\n    create: true\n\n    # -- Annotations to be added to argocd-cm configmap\n    annotations: {}\n\n    # -- Argo CD's externally facing base URL (optional). Required when configuring SSO\n    url: \"\"\n\n    # -- The name of tracking label used by Argo CD for resource pruning\n    # @default -- Defaults to app.kubernetes.io/instance\n    application.instanceLabelKey: argocd.argoproj.io/instance\n\n    # -- Enable logs RBAC enforcement\n    ## Ref: https://argo-cd.readthedocs.io/en/latest/operator-manual/upgrading/2.3-2.4/#enable-logs-rbac-enforcement\n    server.rbac.log.enforce.enable: false\n\n    # -- Enable exec feature in Argo UI\n    ## Ref: https://argo-cd.readthedocs.io/en/latest/operator-manual/rbac/#exec-resource\n    exec.enabled: false\n\n    # -- Enable local admin user\n    ## Ref: https://argo-cd.readthedocs.io/en/latest/faq/#how-to-disable-admin-user\n    admin.enabled: true\n\n    # -- Timeout to discover if a new manifests version got published to the repository\n    timeout.reconciliation: 180s\n\n    # -- Timeout to refresh application data as well as target manifests cache\n    timeout.hard.reconciliation: 0s\n\n    # Dex configuration\n    # dex.config: |\n    #   connectors:\n    #     # GitHub example\n    #     - type: github\n    #       id: github\n    #       name: GitHub\n    #       config:\n    #         clientID: aabbccddeeff00112233\n    #         clientSecret: $dex.github.clientSecret # Alternatively $\u003csome_K8S_secret\u003e:dex.github.clientSecret\n    #         orgs:\n    #         - name: your-github-org\n\n    # OIDC configuration as an alternative to dex (optional).\n    # oidc.config: |\n    #   name: AzureAD\n    #   issuer: https://login.microsoftonline.com/TENANT_ID/v2.0\n    #   clientID: CLIENT_ID\n    #   clientSecret: $oidc.azuread.clientSecret\n    #   rootCA: |\n    #     -----BEGIN CERTIFICATE-----\n    #     ... encoded certificate data here ...\n    #     -----END CERTIFICATE-----\n    #   requestedIDTokenClaims:\n    #     groups:\n    #       essential: true\n    #   requestedScopes:\n    #     - openid\n    #     - profile\n    #     - email\n\n  # Argo CD configuration parameters\n  ## Ref: https://github.com/argoproj/argo-cd/blob/master/docs/operator-manual/argocd-cmd-params-cm.yaml\n  params:\n    # -- Create the argocd-cmd-params-cm configmap\n    # If false, it is expected the configmap will be created by something else.\n    create: true\n\n    # -- Annotations to be added to the argocd-cmd-params-cm ConfigMap\n    annotations: {}\n\n    ## Generic parameters\n    # -- Open-Telemetry collector address: (e.g. \"otel-collector:4317\")\n    otlp.address: ''\n\n    ## Controller Properties\n    # -- Number of application status processors\n    controller.status.processors: 20\n    # -- Number of application operation processors\n    controller.operation.processors: 10\n    # -- Specifies timeout between application self heal attempts\n    controller.self.heal.timeout.seconds: 5\n    # -- Repo server RPC call timeout seconds.\n    controller.repo.server.timeout.seconds: 60\n\n    ## Server properties\n    # -- Run server without TLS\n    server.insecure: true\n    # -- Value for base href in index.html. Used if Argo CD is running behind reverse proxy under subpath different from /\n    server.basehref: /\n    # -- Used if Argo CD is running behind reverse proxy under subpath different from /\n    server.rootpath: ''\n    # -- Directory path that contains additional static assets\n    server.staticassets: /shared/app\n    # -- Disable Argo CD RBAC for user authentication\n    server.disable.auth: false\n    # -- Enable GZIP compression\n    server.enable.gzip: true\n    # -- Set X-Frame-Options header in HTTP responses to value. To disable, set to \"\".\n    server.x.frame.options: sameorigin\n\n    ## Repo-server properties\n    # -- Limit on number of concurrent manifests generate requests. Any value less the 1 means no limit.\n    reposerver.parallelism.limit: 0\n\n    ## ApplicationSet Properties\n    # -- Modify how application is synced between the generator and the cluster. One of: `sync`, `create-only`, `create-update`, `create-delete`\n    applicationsetcontroller.policy: sync\n    # -- Enables use of the Progressive Syncs capability\n    applicationsetcontroller.enable.progressive.syncs: false\n\n    # -- Enables [Applications in any namespace]\n    ## List of additional namespaces where applications may be created in and reconciled from.\n    ## The namespace where Argo CD is installed to will always be allowed.\n    ## Set comma-separated list. (e.g. app-team-one, app-team-two)\n    application.namespaces: \"\"\n\n  # Argo CD RBAC policy configuration\n  ## Ref: https://github.com/argoproj/argo-cd/blob/master/docs/operator-manual/rbac.md\n  rbac:\n    # -- Create the argocd-rbac-cm configmap with ([Argo CD RBAC policy]) definitions.\n    # If false, it is expected the configmap will be created by something else.\n    # Argo CD will not work if there is no configmap created with the name above.\n    create: true\n\n    # -- Annotations to be added to argocd-rbac-cm configmap\n    annotations: {}\n\n    # -- The name of the default role which Argo CD will falls back to, when authorizing API requests (optional).\n    # If omitted or empty, users may be still be able to login, but will see no apps, projects, etc...\n    policy.default: ''\n\n    # -- File containing user-defined policies and role definitions.\n    # @default -- `''` (See [values.yaml])\n    policy.csv: ''\n    # Policy rules are in the form:\n    #  p, subject, resource, action, object, effect\n    # Role definitions and bindings are in the form:\n    #  g, subject, inherited-subject\n    # policy.csv |\n    #   p, role:org-admin, applications, *, */*, allow\n    #   p, role:org-admin, clusters, get, *, allow\n    #   p, role:org-admin, repositories, *, *, allow\n    #   p, role:org-admin, logs, get, *, allow\n    #   p, role:org-admin, exec, create, */*, allow\n    #   g, your-github-org:your-team, role:org-admin\n\n    # -- OIDC scopes to examine during rbac enforcement (in addition to `sub` scope).\n    # The scope value can be a string, or a list of strings.\n    scopes: \"[groups]\"\n\n  # GnuPG public keys for commit verification\n  ## Ref: https://argo-cd.readthedocs.io/en/stable/user-guide/gpg-verification/\n  gpg:\n    # -- Annotations to be added to argocd-gpg-keys-cm configmap\n    annotations: {}\n\n    # -- [GnuPG] public keys to add to the keyring\n    # @default -- `{}` (See [values.yaml])\n    ## Note: Public keys should be exported with `gpg --export --armor \u003cKEY\u003e`\n    keys: {}\n      # 4AEE18F83AFDEB23: |\n      #   -----BEGIN PGP PUBLIC KEY BLOCK-----\n      #   ...\n      #   -----END PGP PUBLIC KEY BLOCK-----\n\n  # SSH known hosts for Git repositories\n  ## Ref: https://argo-cd.readthedocs.io/en/stable/operator-manual/declarative-setup/#ssh-known-host-public-keys\n  ssh:\n    # -- Annotations to be added to argocd-ssh-known-hosts-cm configmap\n    annotations: {}\n\n    # -- Known hosts to be added to the known host list by default.\n    # @default -- See [values.yaml]\n    knownHosts: |\n      [ssh.github.com]:443 ecdsa-sha2-nistp256 AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBEmKSENjQEezOmxkZMy7opKgwFB9nkt5YRrYMjNuG5N87uRgg6CLrbo5wAdT/y6v0mKV0U2w0WZ2YB/++Tpockg=\n      [ssh.github.com]:443 ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIOMqqnkVzrm0SdG6UOoqKLsabgH5C9okWi0dh2l9GKJl\n      [ssh.github.com]:443 ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCj7ndNxQowgcQnjshcLrqPEiiphnt+VTTvDP6mHBL9j1aNUkY4Ue1gvwnGLVlOhGeYrnZaMgRK6+PKCUXaDbC7qtbW8gIkhL7aGCsOr/C56SJMy/BCZfxd1nWzAOxSDPgVsmerOBYfNqltV9/hWCqBywINIR+5dIg6JTJ72pcEpEjcYgXkE2YEFXV1JHnsKgbLWNlhScqb2UmyRkQyytRLtL+38TGxkxCflmO+5Z8CSSNY7GidjMIZ7Q4zMjA2n1nGrlTDkzwDCsw+wqFPGQA179cnfGWOWRVruj16z6XyvxvjJwbz0wQZ75XK5tKSb7FNyeIEs4TT4jk+S4dhPeAUC5y+bDYirYgM4GC7uEnztnZyaVWQ7B381AK4Qdrwt51ZqExKbQpTUNn+EjqoTwvqNj4kqx5QUCI0ThS/YkOxJCXmPUWZbhjpCg56i+2aB6CmK2JGhn57K5mj0MNdBXA4/WnwH6XoPWJzK5Nyu2zB3nAZp+S5hpQs+p1vN1/wsjk=\n      bitbucket.org ecdsa-sha2-nistp256 AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBPIQmuzMBuKdWeF4+a2sjSSpBK0iqitSQ+5BM9KhpexuGt20JpTVM7u5BDZngncgrqDMbWdxMWWOGtZ9UgbqgZE=\n      bitbucket.org ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIIazEu89wgQZ4bqs3d63QSMzYVa0MuJ2e2gKTKqu+UUO\n      bitbucket.org ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDQeJzhupRu0u0cdegZIa8e86EG2qOCsIsD1Xw0xSeiPDlCr7kq97NLmMbpKTX6Esc30NuoqEEHCuc7yWtwp8dI76EEEB1VqY9QJq6vk+aySyboD5QF61I/1WeTwu+deCbgKMGbUijeXhtfbxSxm6JwGrXrhBdofTsbKRUsrN1WoNgUa8uqN1Vx6WAJw1JHPhglEGGHea6QICwJOAr/6mrui/oB7pkaWKHj3z7d1IC4KWLtY47elvjbaTlkN04Kc/5LFEirorGYVbt15kAUlqGM65pk6ZBxtaO3+30LVlORZkxOh+LKL/BvbZ/iRNhItLqNyieoQj/uh/7Iv4uyH/cV/0b4WDSd3DptigWq84lJubb9t/DnZlrJazxyDCulTmKdOR7vs9gMTo+uoIrPSb8ScTtvw65+odKAlBj59dhnVp9zd7QUojOpXlL62Aw56U4oO+FALuevvMjiWeavKhJqlR7i5n9srYcrNV7ttmDw7kf/97P5zauIhxcjX+xHv4M=\n      github.com ecdsa-sha2-nistp256 AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBEmKSENjQEezOmxkZMy7opKgwFB9nkt5YRrYMjNuG5N87uRgg6CLrbo5wAdT/y6v0mKV0U2w0WZ2YB/++Tpockg=\n      github.com ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIOMqqnkVzrm0SdG6UOoqKLsabgH5C9okWi0dh2l9GKJl\n      github.com ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCj7ndNxQowgcQnjshcLrqPEiiphnt+VTTvDP6mHBL9j1aNUkY4Ue1gvwnGLVlOhGeYrnZaMgRK6+PKCUXaDbC7qtbW8gIkhL7aGCsOr/C56SJMy/BCZfxd1nWzAOxSDPgVsmerOBYfNqltV9/hWCqBywINIR+5dIg6JTJ72pcEpEjcYgXkE2YEFXV1JHnsKgbLWNlhScqb2UmyRkQyytRLtL+38TGxkxCflmO+5Z8CSSNY7GidjMIZ7Q4zMjA2n1nGrlTDkzwDCsw+wqFPGQA179cnfGWOWRVruj16z6XyvxvjJwbz0wQZ75XK5tKSb7FNyeIEs4TT4jk+S4dhPeAUC5y+bDYirYgM4GC7uEnztnZyaVWQ7B381AK4Qdrwt51ZqExKbQpTUNn+EjqoTwvqNj4kqx5QUCI0ThS/YkOxJCXmPUWZbhjpCg56i+2aB6CmK2JGhn57K5mj0MNdBXA4/WnwH6XoPWJzK5Nyu2zB3nAZp+S5hpQs+p1vN1/wsjk=\n      gitlab.com ecdsa-sha2-nistp256 AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBFSMqzJeV9rUzU4kWitGjeR4PWSa29SPqJ1fVkhtj3Hw9xjLVXVYrU9QlYWrOLXBpQ6KWjbjTDTdDkoohFzgbEY=\n      gitlab.com ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIAfuCHKVTjquxvt6CM6tdG4SLp1Btn/nOeHHE5UOzRdf\n      gitlab.com ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCsj2bNKTBSpIYDEGk9KxsGh3mySTRgMtXL583qmBpzeQ+jqCMRgBqB98u3z++J1sKlXHWfM9dyhSevkMwSbhoR8XIq/U0tCNyokEi/ueaBMCvbcTHhO7FcwzY92WK4Yt0aGROY5qX2UKSeOvuP4D6TPqKF1onrSzH9bx9XUf2lEdWT/ia1NEKjunUqu1xOB/StKDHMoX4/OKyIzuS0q/T1zOATthvasJFoPrAjkohTyaDUz2LN5JoH839hViyEG82yB+MjcFV5MU3N1l1QL3cVUCh93xSaua1N85qivl+siMkPGbO5xR/En4iEY6K2XPASUEMaieWVNTRCtJ4S8H+9\n      ssh.dev.azure.com ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC7Hr1oTWqNqOlzGJOfGJ4NakVyIzf1rXYd4d7wo6jBlkLvCA4odBlL0mDUyZ0/QUfTTqeu+tm22gOsv+VrVTMk6vwRU75gY/y9ut5Mb3bR5BV58dKXyq9A9UeB5Cakehn5Zgm6x1mKoVyf+FFn26iYqXJRgzIZZcZ5V6hrE0Qg39kZm4az48o0AUbf6Sp4SLdvnuMa2sVNwHBboS7EJkm57XQPVU3/QpyNLHbWDdzwtrlS+ez30S3AdYhLKEOxAG8weOnyrtLJAUen9mTkol8oII1edf7mWWbWVf0nBmly21+nZcmCTISQBtdcyPaEno7fFQMDD26/s0lfKob4Kw8H\n      vs-ssh.visualstudio.com ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC7Hr1oTWqNqOlzGJOfGJ4NakVyIzf1rXYd4d7wo6jBlkLvCA4odBlL0mDUyZ0/QUfTTqeu+tm22gOsv+VrVTMk6vwRU75gY/y9ut5Mb3bR5BV58dKXyq9A9UeB5Cakehn5Zgm6x1mKoVyf+FFn26iYqXJRgzIZZcZ5V6hrE0Qg39kZm4az48o0AUbf6Sp4SLdvnuMa2sVNwHBboS7EJkm57XQPVU3/QpyNLHbWDdzwtrlS+ez30S3AdYhLKEOxAG8weOnyrtLJAUen9mTkol8oII1edf7mWWbWVf0nBmly21+nZcmCTISQBtdcyPaEno7fFQMDD26/s0lfKob4Kw8H\n\n    # -- Additional known hosts for private repositories\n    extraHosts: ''\n\n  # Repository TLS certificates\n  # Ref: https://argo-cd.readthedocs.io/en/stable/operator-manual/declarative-setup/#repositories-using-self-signed-tls-certificates-or-are-signed-by-custom-ca\n  tls:\n    # -- Annotations to be added to argocd-tls-certs-cm configmap\n    annotations: {}\n\n    # -- TLS certificates for Git repositories\n    # @default -- `{}` (See [values.yaml])\n    certificates: {}\n      # server.example.com: |\n      #   -----BEGIN CERTIFICATE-----\n      #   ...\n      #   -----END CERTIFICATE-----\n\n  # ConfigMap for Config Management Plugins\n  # Ref: https://argo-cd.readthedocs.io/en/stable/operator-manual/config-management-plugins/\n  cmp:\n    # -- Create the argocd-cmp-cm configmap\n    create: false\n\n    # -- Annotations to be added to argocd-cmp-cm configmap\n    annotations: {}\n\n    # -- Plugin yaml files to be added to argocd-cmp-cm\n    plugins: {}\n      # --- First plugin\n      # my-plugin:\n      #   init:\n      #     command: [sh]\n      #     args: [-c, 'echo \"Initializing...\"']\n      #   generate:\n      #     command: [sh, -c]\n      #     args:\n      #       - |\n      #         echo \"{\\\"kind\\\": \\\"ConfigMap\\\", \\\"apiVersion\\\": \\\"v1\\\", \\\"metadata\\\": { \\\"name\\\": \\\"$ARGOCD_APP_NAME\\\", \\\"namespace\\\": \\\"$ARGOCD_APP_NAMESPACE\\\", \\\"annotations\\\": {\\\"Foo\\\": \\\"$ARGOCD_ENV_FOO\\\", \\\"KubeVersion\\\": \\\"$KUBE_VERSION\\\", \\\"KubeApiVersion\\\": \\\"$KUBE_API_VERSIONS\\\",\\\"Bar\\\": \\\"baz\\\"}}}\"\n      #   discover:\n      #     fileName: \"./subdir/s*.yaml\"\n      #     find:\n      #       glob: \"**/Chart.yaml\"\n      #       command: [sh, -c, find . -name env.yaml]\n\n      # --- Second plugin\n      # my-plugin2:\n      #   init:\n      #     command: [sh]\n      #     args: [-c, 'echo \"Initializing...\"']\n      #   generate:\n      #     command: [sh, -c]\n      #     args:\n      #       - |\n      #         echo \"{\\\"kind\\\": \\\"ConfigMap\\\", \\\"apiVersion\\\": \\\"v1\\\", \\\"metadata\\\": { \\\"name\\\": \\\"$ARGOCD_APP_NAME\\\", \\\"namespace\\\": \\\"$ARGOCD_APP_NAMESPACE\\\", \\\"annotations\\\": {\\\"Foo\\\": \\\"$ARGOCD_ENV_FOO\\\", \\\"KubeVersion\\\": \\\"$KUBE_VERSION\\\", \\\"KubeApiVersion\\\": \\\"$KUBE_API_VERSIONS\\\",\\\"Bar\\\": \\\"baz\\\"}}}\"\n      #   discover:\n      #     fileName: \"./subdir/s*.yaml\"\n      #     find:\n      #       glob: \"**/Chart.yaml\"\n      #       command: [sh, -c, find . -name env.yaml]\n\n  # -- Provide one or multiple [external cluster credentials]\n  # @default -- `[]` (See [values.yaml])\n  ## Ref:\n  ## - https://argo-cd.readthedocs.io/en/stable/operator-manual/declarative-setup/#clusters\n  ## - https://argo-cd.readthedocs.io/en/stable/operator-manual/security/#external-cluster-credentials\n  ## - https://argo-cd.readthedocs.io/en/stable/user-guide/projects/#project-scoped-repositories-and-clusters\n  clusterCredentials: []\n    # - name: mycluster\n    #   server: https://mycluster.example.com\n    #   labels: {}\n    #   annotations: {}\n    #   config:\n    #     bearerToken: \"\u003cauthentication token\u003e\"\n    #     tlsClientConfig:\n    #       insecure: false\n    #       caData: \"\u003cbase64 encoded certificate\u003e\"\n    # - name: mycluster2\n    #   server: https://mycluster2.example.com\n    #   labels: {}\n    #   annotations: {}\n    #   namespaces: namespace1,namespace2\n    #   clusterResources: true\n    #   config:\n    #     bearerToken: \"\u003cauthentication token\u003e\"\n    #     tlsClientConfig:\n    #       insecure: false\n    #       caData: \"\u003cbase64 encoded certificate\u003e\"\n    # - name: mycluster3-project-scoped\n    #   server: https://mycluster3.example.com\n    #   labels: {}\n    #   annotations: {}\n    #   project: my-project1\n    #   config:\n    #     bearerToken: \"\u003cauthentication token\u003e\"\n    #     tlsClientConfig:\n    #       insecure: false\n    #       caData: \"\u003cbase64 encoded certificate\u003e\"\n\n  # DEPRECATED - Moved to configs.ssh.annotations\n  # knownHostsAnnotations: {}\n  # DEPRECATED - Moved to configs.ssh.knownHosts\n  # knownHosts: {}\n\n  # DEPRECATED - Moved to configs.tls.annotations\n  # tlsCertsAnnotations: {}\n  # DEPRECATED - Moved to configs.tls.certificates\n  # tlsCerts: {}\n\n  # -- Repository credentials to be used as Templates for other repos\n  ## Creates a secret for each key/value specified below to create repository credentials\n  credentialTemplates: {}\n    # github-enterprise-creds-1:\n    #   url: https://github.com/argoproj\n    #   githubAppID: 1\n    #   githubAppInstallationID: 2\n    #   githubAppEnterpriseBaseUrl: https://ghe.example.com/api/v3\n    #   githubAppPrivateKey: |\n    #     -----BEGIN OPENSSH PRIVATE KEY-----\n    #     ...\n    #     -----END OPENSSH PRIVATE KEY-----\n    # https-creds:\n    #   url: https://github.com/argoproj\n    #   password: my-password\n    #   username: my-username\n    # ssh-creds:\n    #  url: git@github.com:argoproj-labs\n    #  sshPrivateKey: |\n    #    -----BEGIN OPENSSH PRIVATE KEY-----\n    #    ...\n    #    -----END OPENSSH PRIVATE KEY-----\n\n  # -- Annotations to be added to `configs.credentialTemplates` Secret\n  credentialTemplatesAnnotations: {}\n\n  # -- Repositories list to be used by applications\n  ## Creates a secret for each key/value specified below to create repositories\n  ## Note: the last example in the list would use a repository credential template, configured under \"configs.credentialTemplates\".\n  repositories: {}\n    # istio-helm-repo:\n    #   url: https://storage.googleapis.com/istio-prerelease/daily-build/master-latest-daily/charts\n    #   name: istio.io\n    #   type: helm\n    # private-helm-repo:\n    #   url: https://my-private-chart-repo.internal\n    #   name: private-repo\n    #   type: helm\n    #   password: my-password\n    #   username: my-username\n    # private-repo:\n    #   url: https://github.com/argoproj/private-repo\n\n  # -- Annotations to be added to `configs.repositories` Secret\n  repositoriesAnnotations: {}\n\n  # Argo CD sensitive data\n  # Ref: https://argo-cd.readthedocs.io/en/stable/operator-manual/user-management/#sensitive-data-and-sso-client-secrets\n  secret:\n    # -- Create the argocd-secret\n    createSecret: true\n    # -- Labels to be added to argocd-secret\n    labels: {}\n    # -- Annotations to be added to argocd-secret\n    annotations: {}\n\n    # -- Shared secret for authenticating GitHub webhook events\n    githubSecret: \"\"\n    # -- Shared secret for authenticating GitLab webhook events\n    gitlabSecret: \"\"\n    # -- Shared secret for authenticating BitbucketServer webhook events\n    bitbucketServerSecret: \"\"\n    # -- UUID for authenticating Bitbucket webhook events\n    bitbucketUUID: \"\"\n    # -- Shared secret for authenticating Gogs webhook events\n    gogsSecret: \"\"\n\n    # -- add additional secrets to be added to argocd-secret\n    ## Custom secrets. Useful for injecting SSO secrets into environment variables.\n    ## Ref: https://argo-cd.readthedocs.io/en/stable/operator-manual/user-management/#sensitive-data-and-sso-client-secrets\n    ## Note that all values must be non-empty.\n    extra:\n      {}\n      # LDAP_PASSWORD: \"mypassword\"\n\n    # -- Argo TLS Data\n    # DEPRECATED - Use server.certificate or server.certificateSecret\n    # argocdServerTlsConfig:\n    #  key: ''\n    #  crt: ''\n\n    # -- Bcrypt hashed admin password\n    ## Argo expects the password in the secret to be bcrypt hashed. You can create this hash with\n    ## `htpasswd -nbBC 10 \"\" $ARGO_PWD | tr -d ':\\n' | sed 's/$2y/$2a/'`\n    argocdServerAdminPassword: \"\"\n    # -- Admin password modification time. Eg. `\"2006-01-02T15:04:05Z\"`\n    # @default -- `\"\"` (defaults to current time)\n    argocdServerAdminPasswordMtime: \"\"\n\n  # -- Define custom [CSS styles] for your argo instance.\n  # This setting will automatically mount the provided CSS and reference it in the argo configuration.\n  # @default -- `\"\"` (See [values.yaml])\n  ## Ref: https://argo-cd.readthedocs.io/en/stable/operator-manual/custom-styles/\n  styles: \"\"\n  # styles: |\n  #  .sidebar {\n  #    background: linear-gradient(to bottom, #999, #777, #333, #222, #111);\n  #  }\n\n# -- Array of extra K8s manifests to deploy\n## Note: Supports use of custom Helm templates\nextraObjects: []\n  # - apiVersion: secrets-store.csi.x-k8s.io/v1\n  #   kind: SecretProviderClass\n  #   metadata:\n  #     name: argocd-secrets-store\n  #   spec:\n  #     provider: aws\n  #     parameters:\n  #       objects: |\n  #         - objectName: \"argocd\"\n  #           objectType: \"secretsmanager\"\n  #           jmesPath:\n  #               - path: \"client_id\"\n  #                 objectAlias: \"client_id\"\n  #               - path: \"client_secret\"\n  #                 objectAlias: \"client_secret\"\n  #     secretObjects:\n  #     - data:\n  #       - key: client_id\n  #         objectName: client_id\n  #       - key: client_secret\n  #         objectName: client_secret\n  #       secretName: argocd-secrets-store\n  #       type: Opaque\n  #       labels:\n  #         app.kubernetes.io/part-of: argocd\n\n## Application controller\ncontroller:\n  # -- Application controller name string\n  name: application-controller\n\n  # -- The number of application controller pods to run.\n  # Additional replicas will cause sharding of managed clusters across number of replicas.\n  replicas: 1\n\n  ## Application controller Pod Disruption Budget\n  ## Ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb/\n  pdb:\n    # -- Deploy a [PodDisruptionBudget] for the application controller\n    enabled: false\n    # -- Labels to be added to application controller pdb\n    labels: {}\n    # -- Annotations to be added to application controller pdb\n    annotations: {}\n    # -- Number of pods that are available after eviction as number or percentage (eg.: 50%)\n    # @default -- `\"\"` (defaults to 0 if not specified)\n    minAvailable: \"\"\n    # -- Number of pods that are unavailable after eviction as number or percentage (eg.: 50%).\n    ## Has higher precedence over `controller.pdb.minAvailable`\n    maxUnavailable: \"\"\n\n  ## Application controller image\n  image:\n    # -- Repository to use for the application controller\n    # @default -- `\"\"` (defaults to global.image.repository)\n    repository: \"\"\n    # -- Tag to use for the application controller\n    # @default -- `\"\"` (defaults to global.image.tag)\n    tag: \"\"\n    # -- Image pull policy for the application controller\n    # @default -- `\"\"` (defaults to global.image.imagePullPolicy)\n    imagePullPolicy: \"\"\n\n  # -- Secrets with credentials to pull images from a private registry\n  # @default -- `[]` (defaults to global.imagePullSecrets)\n  imagePullSecrets: []\n\n  # -- DEPRECATED - Application controller commandline flags\n  args: {}\n  # DEPRECATED - Use configs.params to override\n  #  # -- define the application controller `--status-processors`\n  #  statusProcessors: \"20\"\n  #  # -- define the application controller `--operation-processors`\n  #  operationProcessors: \"10\"\n  #  # -- define the application controller `--app-hard-resync`\n  #  appHardResyncPeriod: \"0\"\n  #  # -- define the application controller `--app-resync`\n  #  appResyncPeriod: \"180\"\n  #  # -- define the application controller `--self-heal-timeout-seconds`\n  #  selfHealTimeout: \"5\"\n  #  # -- define the application controller `--repo-server-timeout-seconds`\n  #  repoServerTimeoutSeconds: \"60\"\n\n  # -- Additional command line arguments to pass to application controller\n  extraArgs: []\n\n  # -- Environment variables to pass to application controller\n  env: []\n\n  # -- envFrom to pass to application controller\n  # @default -- `[]` (See [values.yaml])\n  envFrom: []\n  # - configMapRef:\n  #     name: config-map-name\n  # - secretRef:\n  #     name: secret-name\n\n  # -- Additional containers to be added to the application controller pod\n  ## Note: Supports use of custom Helm templates\n  extraContainers: []\n\n  # -- Init containers to add to the application controller pod\n  ## If your target Kubernetes cluster(s) require a custom credential (exec) plugin\n  ## you could use this (and the same in the server pod) to provide such executable\n  ## Ref: https://kubernetes.io/docs/reference/access-authn-authz/authentication/#client-go-credential-plugins\n  ## Note: Supports use of custom Helm templates\n  initContainers: []\n  #  - name: download-tools\n  #    image: alpine:3\n  #    command: [sh, -c]\n  #    args:\n  #      - wget -qO kubelogin.zip https://github.com/Azure/kubelogin/releases/download/v0.0.25/kubelogin-linux-amd64.zip \u0026\u0026\n  #        unzip kubelogin.zip \u0026\u0026 mv bin/linux_amd64/kubelogin /custom-tools/\n  #    volumeMounts:\n  #      - mountPath: /custom-tools\n  #        name: custom-tools\n\n  # -- Additional volumeMounts to the application controller main container\n  volumeMounts: []\n  #  - mountPath: /usr/local/bin/kubelogin\n  #    name: custom-tools\n  #    subPath: kubelogin\n\n  # -- Additional volumes to the application controller pod\n  volumes: []\n  #  - name: custom-tools\n  #    emptyDir: {}\n\n  # -- Annotations for the application controller StatefulSet\n  statefulsetAnnotations: {}\n\n  # -- Annotations to be added to application controller pods\n  podAnnotations: {}\n\n  # -- Labels to be added to application controller pods\n  podLabels: {}\n\n  # -- Resource limits and requests for the application controller pods\n  resources: {}\n  #  limits:\n  #    cpu: 500m\n  #    memory: 512Mi\n  #  requests:\n  #    cpu: 250m\n  #    memory: 256Mi\n\n  # Application controller container ports\n  containerPorts:\n    # -- Metrics container port\n    metrics: 8082\n\n  # -- Host Network for application controller pods\n  hostNetwork: false\n\n  # -- [DNS configuration]\n  dnsConfig: {}\n  # -- Alternative DNS policy for application controller pods\n  dnsPolicy: \"ClusterFirst\"\n\n  # -- Application controller container-level security context\n  # @default -- See [values.yaml]\n  containerSecurityContext:\n    runAsNonRoot: true\n    readOnlyRootFilesystem: true\n    allowPrivilegeEscalation: false\n    seccompProfile:\n      type: RuntimeDefault\n    capabilities:\n      drop:\n      - ALL\n\n  # Readiness probe for application controller\n  ## Ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/\n  readinessProbe:\n    # -- Minimum consecutive failures for the [probe] to be considered failed after having succeeded\n    failureThreshold: 3\n    # -- Number of seconds after the container has started before [probe] is initiated\n    initialDelaySeconds: 10\n    # -- How often (in seconds) to perform the [probe]\n    periodSeconds: 10\n    # -- Minimum consecutive successes for the [probe] to be considered successful after having failed\n    successThreshold: 1\n    # -- Number of seconds after which the [probe] times out\n    timeoutSeconds: 1\n\n  # -- terminationGracePeriodSeconds for container lifecycle hook\n  terminationGracePeriodSeconds: 30\n\n  # -- Priority class for the application controller pods\n  # @default -- `\"\"` (defaults to global.priorityClassName)\n  priorityClassName: \"\"\n\n  # -- [Node selector]\n  # @default -- `{}` (defaults to global.nodeSelector)\n  nodeSelector: {}\n\n  # -- [Tolerations] for use with node taints\n  # @default -- `[]` (defaults to global.tolerations)\n  tolerations: []\n\n  # -- Assign custom [affinity] rules to the deployment\n  # @default -- `{}` (defaults to global.affinity preset)\n  affinity: {}\n\n  # -- Assign custom [TopologySpreadConstraints] rules to the application controller\n  # @default -- `[]` (defaults to global.topologySpreadConstraints)\n  ## Ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/\n  ## If labelSelector is left out, it will default to the labelSelector configuration of the deployment\n  topologySpreadConstraints: []\n    # - maxSkew: 1\n    #   topologyKey: topology.kubernetes.io/zone\n    #   whenUnsatisfiable: DoNotSchedule\n\n  serviceAccount:\n    # -- Create a service account for the application controller\n    create: true\n    # -- Service account name\n    name: argocd-application-controller\n    # -- Annotations applied to created service account\n    annotations: {}\n    # -- Labels applied to created service account\n    labels: {}\n    # -- Automount API credentials for the Service Account\n    automountServiceAccountToken: true\n\n  ## Application controller metrics configuration\n  metrics:\n    # -- Deploy metrics service\n    enabled: false\n    applicationLabels:\n      # -- Enables additional labels in argocd_app_labels metric\n      enabled: false\n      # -- Additional labels\n      labels: []\n    service:\n      # -- Metrics service type\n      type: ClusterIP\n      # -- Metrics service clusterIP. `None` makes a \"headless service\" (no virtual IP)\n      clusterIP: \"\"\n      # -- Metrics service annotations\n      annotations: {}\n      # -- Metrics service labels\n      labels: {}\n      # -- Metrics service port\n      servicePort: 8082\n      # -- Metrics service port name\n      portName: http-metrics\n    serviceMonitor:\n      # -- Enable a prometheus ServiceMonitor\n      enabled: false\n      # -- Prometheus ServiceMonitor interval\n      interval: 30s\n      # -- Prometheus [RelabelConfigs] to apply to samples before scraping\n      relabelings: []\n      # -- Prometheus [MetricRelabelConfigs] to apply to samples before ingestion\n      metricRelabelings: []\n      # -- Prometheus ServiceMonitor selector\n      selector: {}\n        # prometheus: kube-prometheus\n\n      # -- Prometheus ServiceMonitor scheme\n      scheme: \"\"\n      # -- Prometheus ServiceMonitor tlsConfig\n      tlsConfig: {}\n      # -- Prometheus ServiceMonitor namespace\n      namespace: \"monitoring\" # \"monitoring\"\n      # -- Prometheus ServiceMonitor labels\n      additionalLabels: {}\n      # -- Prometheus ServiceMonitor annotations\n      annotations: {}\n    rules:\n      # -- Deploy a PrometheusRule for the application controller\n      enabled: false\n      # -- PrometheusRule namespace\n      namespace: \"monitoring\" # \"monitoring\"\n      # -- PrometheusRule selector\n      selector: {}\n        # prometheus: kube-prometheus\n\n      # -- PrometheusRule labels\n      additionalLabels: {}\n      # -- PrometheusRule annotations\n      annotations: {}\n\n      # -- PrometheusRule.Spec for the application controller\n      spec: []\n      # - alert: ArgoAppMissing\n      #   expr: |\n      #     absent(argocd_app_info) == 1\n      #   for: 15m\n      #   labels:\n      #     severity: critical\n      #   annotations:\n      #     summary: \"[Argo CD] No reported applications\"\n      #     description: \u003e\n      #       Argo CD has not reported any applications data for the past 15 minutes which\n      #       means that it must be down or not functioning properly.  This needs to be\n      #       resolved for this cloud to continue to maintain state.\n      # - alert: ArgoAppNotSynced\n      #   expr: |\n      #     argocd_app_info{sync_status!=\"Synced\"} == 1\n      #   for: 12h\n      #   labels:\n      #     severity: warning\n      #   annotations:\n      #     summary: \"[{{`{{$labels.name}}`}}] Application not synchronized\"\n      #     description: \u003e\n      #       The application [{{`{{$labels.name}}`}} has not been synchronized for over\n      #       12 hours which means that the state of this cloud has drifted away from the\n      #       state inside Git.\n\n  ## Enable this and set the rules: to whatever custom rules you want for the Cluster Role resource.\n  ## Defaults to off\n  clusterRoleRules:\n    # -- Enable custom rules for the application controller's ClusterRole resource\n    enabled: false\n    # -- List of custom rules for the application controller's ClusterRole resource\n    rules: []\n\n## Dex\ndex:\n  # -- Enable dex\n  enabled: true\n  # -- Dex name\n  name: dex-server\n\n  # -- Additional command line arguments to pass to the Dex server\n  extraArgs: []\n\n  metrics:\n    # -- Deploy metrics service\n    enabled: false\n    service:\n      # -- Metrics service annotations\n      annotations: {}\n      # -- Metrics service labels\n      labels: {}\n      # -- Metrics service port name\n      portName: http-metrics\n    serviceMonitor:\n      # -- Enable a prometheus ServiceMonitor\n      enabled: false\n      # -- Prometheus ServiceMonitor interval\n      interval: 30s\n      # -- Prometheus [RelabelConfigs] to apply to samples before scraping\n      relabelings: []\n      # -- Prometheus [MetricRelabelConfigs] to apply to samples before ingestion\n      metricRelabelings: []\n      # -- Prometheus ServiceMonitor selector\n      selector: {}\n        # prometheus: kube-prometheus\n\n      # -- Prometheus ServiceMonitor scheme\n      scheme: \"\"\n      # -- Prometheus ServiceMonitor tlsConfig\n      tlsConfig: {}\n      # -- Prometheus ServiceMonitor namespace\n      namespace: \"monitoring\" # \"monitoring\"\n      # -- Prometheus ServiceMonitor labels\n      additionalLabels: {}\n      # -- Prometheus ServiceMonitor annotations\n      annotations: {}\n\n  ## Dex Pod Disruption Budget\n  ## Ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb/\n  pdb:\n    # -- Deploy a [PodDisruptionBudget] for the Dex server\n    enabled: false\n    # -- Labels to be added to Dex server pdb\n    labels: {}\n    # -- Annotations to be added to Dex server pdb\n    annotations: {}\n    # -- Number of pods that are available after eviction as number or percentage (eg.: 50%)\n    # @default -- `\"\"` (defaults to 0 if not specified)\n    minAvailable: \"\"\n    # -- Number of pods that are unavailble after eviction as number or percentage (eg.: 50%).\n    ## Has higher precedence over `dex.pdb.minAvailable`\n    maxUnavailable: \"\"\n\n  ## Dex image\n  image:\n    # -- Dex image repository\n    repository: ghcr.io/dexidp/dex\n    # -- Dex image tag\n    tag: v2.37.0\n    # -- Dex imagePullPolicy\n    # @default -- `\"\"` (defaults to global.image.imagePullPolicy)\n    imagePullPolicy: \"\"\n\n  # -- Secrets with credentials to pull images from a private registry\n  # @default -- `[]` (defaults to global.imagePullSecrets)\n  imagePullSecrets: []\n\n  # Argo CD init image that creates Dex config\n  initImage:\n    # -- Argo CD init image repository\n    # @default -- `\"\"` (defaults to global.image.repository)\n    repository: \"\"\n    # -- Argo CD init image tag\n    # @default -- `\"\"` (defaults to global.image.tag)\n    tag: \"\"\n    # -- Argo CD init image imagePullPolicy\n    # @default -- `\"\"` (defaults to global.image.imagePullPolicy)\n    imagePullPolicy: \"\"\n    # -- Argo CD init image resources\n    # @default -- `{}` (defaults to dex.resources)\n    resources: {}\n    #  requests:\n    #    cpu: 5m\n    #    memory: 96Mi\n    #  limits:\n    #    cpu: 10m\n    #    memory: 144Mi\n\n  # -- Environment variables to pass to the Dex server\n  env: []\n\n  # -- envFrom to pass to the Dex server\n  # @default -- `[]` (See [values.yaml])\n  envFrom: []\n  # - configMapRef:\n  #     name: config-map-name\n  # - secretRef:\n  #     name: secret-name\n\n  # -- Additional containers to be added to the dex pod\n  ## Note: Supports use of custom Helm templates\n  extraContainers: []\n\n  # -- Init containers to add to the dex pod\n  ## Note: Supports use of custom Helm templates\n  initContainers: []\n\n  # -- Additional volumeMounts to the dex main container\n  volumeMounts: []\n\n  # -- Additional volumes to the dex pod\n  volumes: []\n\n  # TLS certificate configuration via Secret\n  ## Ref: https://argo-cd.readthedocs.io/en/stable/operator-manual/tls/#configuring-tls-to-argocd-dex-server\n  ## Note: Issuing certificates via cert-manager in not supported right now because it's not possible to restart Dex automatically without extra controllers.\n  certificateSecret:\n    # -- Create argocd-dex-server-tls secret\n    enabled: false\n    # -- Labels to be added to argocd-dex-server-tls secret\n    labels: {}\n    # -- Annotations to be added to argocd-dex-server-tls secret\n    annotations: {}\n    # -- Certificate authority. Required for self-signed certificates.\n    ca: ''\n    # -- Certificate private key\n    key: ''\n    # -- Certificate data. Must contain SANs of Dex service (ie: argocd-dex-server, argocd-dex-server.argo-cd.svc)\n    crt: ''\n\n  # -- Annotations to be added to the Dex server Deployment\n  deploymentAnnotations: {}\n\n  # -- Annotations to be added to the Dex server pods\n  podAnnotations: {}\n\n  # -- Labels to be added to the Dex server pods\n  podLabels: {}\n\n  # -- Resource limits and requests for dex\n  resources: {}\n  #  limits:\n  #    cpu: 50m\n  #    memory: 64Mi\n  #  requests:\n  #    cpu: 10m\n  #    memory: 32Mi\n\n  # Dex container ports\n  # NOTE: These ports are currently hardcoded and cannot be changed\n  containerPorts:\n    # -- HTTP container port\n    http: 5556\n    # -- gRPC container port\n    grpc: 5557\n    # -- Metrics container port\n    metrics: 5558\n\n  # -- [DNS configuration]\n  dnsConfig: {}\n  # -- Alternative DNS policy for Dex server pods\n  dnsPolicy: \"ClusterFirst\"\n\n  # -- Dex container-level security context\n  # @default -- See [values.yaml]\n  containerSecurityContext:\n    runAsNonRoot: true\n    readOnlyRootFilesystem: true\n    allowPrivilegeEscalation: false\n    seccompProfile:\n      type: RuntimeDefault\n    capabilities:\n      drop:\n      - ALL\n\n  ## Probes for Dex server\n  ## Supported from Dex \u003e= 2.28.0\n  livenessProbe:\n    # -- Enable Kubernetes liveness probe for Dex \u003e= 2.28.0\n    enabled: false\n    # -- Minimum consecutive failures for the [probe] to be considered failed after having succeeded\n    failureThreshold: 3\n    # -- Number of seconds after the container has started before [probe] is initiated\n    initialDelaySeconds: 10\n    # -- How often (in seconds) to perform the [probe]\n    periodSeconds: 10\n    # -- Minimum consecutive successes for the [probe] to be considered successful after having failed\n    successThreshold: 1\n    # -- Number of seconds after which the [probe] times out\n    timeoutSeconds: 1\n\n  readinessProbe:\n    # -- Enable Kubernetes readiness probe for Dex \u003e= 2.28.0\n    enabled: false\n    # -- Minimum consecutive failures for the [probe] to be considered failed after having succeeded\n    failureThreshold: 3\n    # -- Number of seconds after the container has started before [probe] is initiated\n    initialDelaySeconds: 10\n    # -- How often (in seconds) to perform the [probe]\n    periodSeconds: 10\n    # -- Minimum consecutive successes for the [probe] to be considered successful after having failed\n    successThreshold: 1\n    # -- Number of seconds after which the [probe] times out\n    timeoutSeconds: 1\n\n  # -- terminationGracePeriodSeconds for container lifecycle hook\n  terminationGracePeriodSeconds: 30\n\n  serviceAccount:\n    # -- Create dex service account\n    create: true\n    # -- Dex service account name\n    name: argocd-dex-server\n    # -- Annotations applied to created service account\n    annotations: {}\n    # -- Automount API credentials for the Service Account\n    automountServiceAccountToken: true\n\n  # -- Service port for HTTP access\n  servicePortHttp: 5556\n  # -- Service port name for HTTP access\n  servicePortHttpName: http\n  # -- Service port for gRPC access\n  servicePortGrpc: 5557\n  # -- Service port name for gRPC access\n  servicePortGrpcName: grpc\n  # -- Service port for metrics access\n  servicePortMetrics: 5558\n\n  # -- Priority class for the dex pods\n  # @default -- `\"\"` (defaults to global.priorityClassName)\n  priorityClassName: \"\"\n\n  # -- [Node selector]\n  # @default -- `{}` (defaults to global.nodeSelector)\n  nodeSelector: {}\n\n  # -- [Tolerations] for use with node taints\n  # @default -- `[]` (defaults to global.tolerations)\n  tolerations: []\n\n  # -- Assign custom [affinity] rules to the deployment\n  # @default -- `{}` (defaults to global.affinity preset)\n  affinity: {}\n\n  # -- Assign custom [TopologySpreadConstraints] rules to dex\n  # @default -- `[]` (defaults to global.topologySpreadConstraints)\n  ## Ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/\n  ## If labelSelector is left out, it will default to the labelSelector configuration of the deployment\n  topologySpreadConstraints: []\n    # - maxSkew: 1\n    #   topologyKey: topology.kubernetes.io/zone\n    #   whenUnsatisfiable: DoNotSchedule\n\n  # -- Deployment strategy to be added to the Dex server Deployment\n  deploymentStrategy: {}\n    # type: RollingUpdate\n    # rollingUpdate:\n    #   maxSurge: 25%\n    #   maxUnavailable: 25%\n\n  # -- Dex log format. Either `text` or `json`\n  # @default -- `\"\"` (defaults to global.logging.format)\n  logFormat: \"\"\n  # -- Dex log level. One of: `debug`, `info`, `warn`, `error`\n  # @default -- `\"\"` (defaults to global.logging.level)\n  logLevel: \"\"\n\n## Redis\nredis:\n  # -- Enable redis\n  enabled: true\n  # -- Redis name\n  name: redis\n\n  ## Redis Pod Disruption Budget\n  ## Ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb/\n  pdb:\n    # -- Deploy a [PodDisruptionBudget] for the Redis\n    enabled: false\n    # -- Labels to be added to Redis pdb\n    labels: {}\n    # -- Annotations to be added to Redis pdb\n    annotations: {}\n    # -- Number of pods that are available after eviction as number or percentage (eg.: 50%)\n    # @default -- `\"\"` (defaults to 0 if not specified)\n    minAvailable: \"\"\n    # -- Number of pods that are unavailble after eviction as number or percentage (eg.: 50%).\n    ## Has higher precedence over `redis.pdb.minAvailable`\n    maxUnavailable: \"\"\n\n  ## Redis image\n  image:\n    # -- Redis repository\n    repository: public.ecr.aws/docker/library/redis\n    # -- Redis tag\n    tag: 7.0.13-alpine\n    # -- Redis image pull policy\n    # @default -- `\"\"` (defaults to global.image.imagePullPolicy)\n    imagePullPolicy: \"\"\n\n  ## Prometheus redis-exporter sidecar\n  exporter:\n    # -- Enable Prometheus redis-exporter sidecar\n    enabled: false\n    # -- Environment variables to pass to the Redis exporter\n    env: []\n    ## Prometheus redis-exporter image\n    image:\n      # -- Repository to use for the redis-exporter\n      repository: public.ecr.aws/bitnami/redis-exporter\n      # -- Tag to use for the redis-exporter\n      tag: 1.53.0\n      # -- Image pull policy for the redis-exporter\n      # @default -- `\"\"` (defaults to global.image.imagePullPolicy)\n      imagePullPolicy: \"\"\n\n    # -- Redis exporter security context\n    # @default -- See [values.yaml]\n    containerSecurityContext:\n      runAsNonRoot: true\n      readOnlyRootFilesystem: true\n      allowPrivilegeEscalation: false\n      seccompProfile:\n        type: RuntimeDefault\n      capabilities:\n        drop:\n        - ALL\n\n    # -- Resource limits and requests for redis-exporter sidecar\n    resources: {}\n      # limits:\n      #   cpu: 50m\n      #   memory: 64Mi\n      # requests:\n      #   cpu: 10m\n      #   memory: 32Mi\n\n  # -- Secrets with credentials to pull images from a private registry\n  # @default -- `[]` (defaults to global.imagePullSecrets)\n  imagePullSecrets: []\n\n  # -- Additional command line arguments to pass to redis-server\n  extraArgs: []\n  # - --bind\n  # - \"0.0.0.0\"\n\n  # -- Environment variables to pass to the Redis server\n  env: []\n\n  # -- envFrom to pass to the Redis server\n  # @default -- `[]` (See [values.yaml])\n  envFrom: []\n  # - configMapRef:\n  #     name: config-map-name\n  # - secretRef:\n  #     name: secret-name\n\n  # -- Additional containers to be added to the redis pod\n  ## Note: Supports use of custom Helm templates\n  extraContainers: []\n\n  # -- Init containers to add to the redis pod\n  ## Note: Supports use of custom Helm templates\n  initContainers: []\n\n  # -- Additional volumeMounts to the redis container\n  volumeMounts: []\n\n  # -- Additional volumes to the redis pod\n  volumes: []\n\n  # -- Annotations to be added to the Redis server Deployment\n  deploymentAnnotations: {}\n\n  # -- Annotations to be added to the Redis server pods\n  podAnnotations: {}\n\n  # -- Labels to be added to the Redis server pods\n  podLabels: {}\n\n  # -- Resource limits and requests for redis\n  resources: {}\n  #  limits:\n  #    cpu: 200m\n  #    memory: 128Mi\n  #  requests:\n  #    cpu: 100m\n  #    memory: 64Mi\n\n  # -- Redis pod-level security context\n  # @default -- See [values.yaml]\n  securityContext:\n    runAsNonRoot: true\n    runAsUser: 999\n    seccompProfile:\n      type: RuntimeDefault\n\n  # Redis container ports\n  containerPorts:\n    # -- Redis container port\n    redis: 6379\n    # -- Metrics container port\n    metrics: 9121\n\n  # -- [DNS configuration]\n  dnsConfig: {}\n  # -- Alternative DNS policy for Redis server pods\n  dnsPolicy: \"ClusterFirst\"\n\n  # -- Redis container-level security context\n  # @default -- See [values.yaml]\n  containerSecurityContext:\n    readOnlyRootFilesystem: true\n    allowPrivilegeEscalation: false\n    capabilities:\n      drop:\n      - ALL\n\n  # -- Redis service port\n  servicePort: 6379\n\n  # -- Priority class for redis pods\n  # @default -- `\"\"` (defaults to global.priorityClassName)\n  priorityClassName: \"\"\n\n  # -- [Node selector]\n  # @default -- `{}` (defaults to global.nodeSelector)\n  nodeSelector: {}\n\n  # -- [Tolerations] for use with node taints\n  # @default -- `[]` (defaults to global.tolerations)\n  tolerations: []\n\n  # -- Assign custom [affinity] rules to the deployment\n  # @default -- `{}` (defaults to global.affinity preset)\n  affinity: {}\n\n  # -- Assign custom [TopologySpreadConstraints] rules to redis\n  # @default -- `[]` (defaults to global.topologySpreadConstraints)\n  ## Ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/\n  ## If labelSelector is left out, it will default to the labelSelector configuration of the deployment\n  topologySpreadConstraints: []\n    # - maxSkew: 1\n    #   topologyKey: topology.kubernetes.io/zone\n    #   whenUnsatisfiable: DoNotSchedule\n\n  # -- terminationGracePeriodSeconds for container lifecycle hook\n  terminationGracePeriodSeconds: 30\n\n  serviceAccount:\n    # -- Create a service account for the redis pod\n    create: false\n    # -- Service account name for redis pod\n    name: \"\"\n    # -- Annotations applied to created service account\n    annotations: {}\n    # -- Automount API credentials for the Service Account\n    automountServiceAccountToken: false\n\n  service:\n    # -- Redis service annotations\n    annotations: {}\n    # -- Additional redis service labels\n    labels: {}\n\n  metrics:\n    # -- Deploy metrics service\n    enabled: false\n\n    # Redis metrics service configuration\n    service:\n      # -- Metrics service type\n      type: ClusterIP\n      # -- Metrics service clusterIP. `None` makes a \"headless service\" (no virtual IP)\n      clusterIP: None\n      # -- Metrics service annotations\n      annotations: {}\n      # -- Metrics service labels\n      labels: {}\n      # -- Metrics service port\n      servicePort: 9121\n      # -- Metrics service port name\n      portName: http-metrics\n\n    serviceMonitor:\n      # -- Enable a prometheus ServiceMonitor\n      enabled: false\n      # -- Interval at which metrics should be scraped\n      interval: 30s\n      # -- Prometheus [RelabelConfigs] to apply to samples before scraping\n      relabelings: []\n      # -- Prometheus [MetricRelabelConfigs] to apply to samples before ingestion\n      metricRelabelings: []\n      # -- Prometheus ServiceMonitor selector\n      selector: {}\n        # prometheus: kube-prometheus\n\n      # -- Prometheus ServiceMonitor scheme\n      scheme: \"\"\n      # -- Prometheus ServiceMonitor tlsConfig\n      tlsConfig: {}\n      # -- Prometheus ServiceMonitor namespace\n      namespace: \"monitoring\" # \"monitoring\"\n      # -- Prometheus ServiceMonitor labels\n      additionalLabels: {}\n      # -- Prometheus ServiceMonitor annotations\n      annotations: {}\n\n## Redis-HA subchart replaces custom redis deployment when `redis-ha.enabled=true`\n# Ref: https://github.com/DandyDeveloper/charts/blob/master/charts/redis-ha/values.yaml\nredis-ha:\n  # -- Enables the Redis HA subchart and disables the custom Redis single node deployment\n  enabled: false\n  ## Redis image\n  image:\n    # -- Redis repository\n    repository: redis\n    # -- Redis tag\n    tag: 7.0.13-alpine\n  ## Prometheus redis-exporter sidecar\n  exporter:\n    # -- Enable Prometheus redis-exporter sidecar\n    enabled: false\n    # -- Repository to use for the redis-exporter\n    image: public.ecr.aws/bitnami/redis-exporter\n    # -- Tag to use for the redis-exporter\n    tag: 1.53.0\n  persistentVolume:\n    # -- Configures persistence on Redis nodes\n    enabled: false\n  ## Redis specific configuration options\n  redis:\n    # -- Redis convention for naming the cluster group: must match `^[\\\\w-\\\\.]+$` and can be templated\n    masterGroupName: argocd\n    # -- Any valid redis config options in this section will be applied to each server (see `redis-ha` chart)\n    # @default -- See [values.yaml]\n    config:\n      # -- Will save the DB if both the given number of seconds and the given number of write operations against the DB occurred. `\"\"`  is disabled\n      # @default -- `'\"\"'`\n      save: '\"\"'\n  ## Enables a HA Proxy for better LoadBalancing / Sentinel Master support. Automatically proxies to Redis master.\n  haproxy:\n    # -- Enabled HAProxy LoadBalancing/Proxy\n    enabled: true\n    metrics:\n      # -- HAProxy enable prometheus metric scraping\n      enabled: true\n    # -- Whether the haproxy pods should be forced to run on separate nodes.\n    hardAntiAffinity: true\n    # -- Additional affinities to add to the haproxy pods.\n    additionalAffinities: {}\n    # -- Assign custom [affinity] rules to the haproxy pods.\n    affinity: |\n\n    # -- [Tolerations] for use with node taints for haproxy pods.\n    tolerations: []\n    # -- HAProxy container-level security context\n    # @default -- See [values.yaml]\n    containerSecurityContext:\n      readOnlyRootFilesystem: true\n\n  # -- Whether the Redis server pods should be forced to run on separate nodes.\n  hardAntiAffinity: true\n\n  # -- Additional affinities to add to the Redis server pods.\n  additionalAffinities: {}\n\n  # -- Assign custom [affinity] rules to the Redis pods.\n  affinity: |\n\n  # -- [Tolerations] for use with node taints for Redis pods.\n  tolerations: []\n\n  # -- Assign custom [TopologySpreadConstraints] rules to the Redis pods.\n  ## https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/\n  topologySpreadConstraints:\n    # -- Enable Redis HA topology spread constraints\n    enabled: false\n    # -- Max skew of pods tolerated\n    # @default -- `\"\"` (defaults to `1`)\n    maxSkew: \"\"\n    # -- Topology key for spread\n    # @default -- `\"\"` (defaults to `topology.kubernetes.io/zone`)\n    topologyKey: \"\"\n    # -- Enforcement policy, hard or soft\n    # @default -- `\"\"` (defaults to `ScheduleAnyway`)\n    whenUnsatisfiable: \"\"\n  # -- Redis HA statefulset container-level security context\n  # @default -- See [values.yaml]\n  containerSecurityContext:\n    readOnlyRootFilesystem: true\n\n# External Redis parameters\nexternalRedis:\n  # -- External Redis server host\n  host: \"\"\n  # -- External Redis username\n  username: \"\"\n  # -- External Redis password\n  password: \"\"\n  # -- External Redis server port\n  port: 6379\n  # -- The name of an existing secret with Redis credentials (must contain key `redis-password`).\n  # When it's set, the `externalRedis.password` parameter is ignored\n  existingSecret: \"\"\n  # -- External Redis Secret annotations\n  secretAnnotations: {}\n\n## Server\nserver:\n  # -- Argo CD server name\n  name: server\n\n  # -- The number of server pods to run\n  replicas: 1\n\n  ## Argo CD server Horizontal Pod Autoscaler\n  autoscaling:\n    # -- Enable Horizontal Pod Autoscaler ([HPA]) for the Argo CD server\n    enabled: false\n    # -- Minimum number of replicas for the Argo CD server [HPA]\n    minReplicas: 1\n    # -- Maximum number of replicas for the Argo CD server [HPA]\n    maxReplicas: 5\n    # -- Average CPU utilization percentage for the Argo CD server [HPA]\n    targetCPUUtilizationPercentage: 50\n    # -- Average memory utilization percentage for the Argo CD server [HPA]\n    targetMemoryUtilizationPercentage: 50\n    # -- Configures the scaling behavior of the target in both Up and Down directions.\n    behavior: {}\n      # scaleDown:\n      #  stabilizationWindowSeconds: 300\n      #  policies:\n      #   - type: Pods\n      #     value: 1\n      #     periodSeconds: 180\n      # scaleUp:\n      #   stabilizationWindowSeconds: 300\n      #   policies:\n      #   - type: Pods\n      #     value: 2\n      #     periodSeconds: 60\n    # -- Configures custom HPA metrics for the Argo CD server\n    # Ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/\n    metrics: []\n\n  ## Argo CD server Pod Disruption Budget\n  ## Ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb/\n  pdb:\n    # -- Deploy a [PodDisruptionBudget] for the Argo CD server\n    enabled: false\n    # -- Labels to be added to Argo CD server pdb\n    labels: {}\n    # -- Annotations to be added to Argo CD server pdb\n    annotations: {}\n    # -- Number of pods that are available after eviction as number or percentage (eg.: 50%)\n    # @default -- `\"\"` (defaults to 0 if not specified)\n    minAvailable: \"\"\n    # -- Number of pods that are unavailable after eviction as number or percentage (eg.: 50%).\n    ## Has higher precedence over `server.pdb.minAvailable`\n    maxUnavailable: \"\"\n\n  ## Argo CD server image\n  image:\n    # -- Repository to use for the Argo CD server\n    # @default -- `\"\"` (defaults to global.image.repository)\n    repository: \"\" # defaults to global.image.repository\n    # -- Tag to use for the Argo CD server\n    # @default -- `\"\"` (defaults to global.image.tag)\n    tag: \"\" # defaults to global.image.tag\n    # -- Image pull policy for the Argo CD server\n    # @default -- `\"\"` (defaults to global.image.imagePullPolicy)\n    imagePullPolicy: \"\" # IfNotPresent\n\n  # -- Secrets with credentials to pull images from a private registry\n  # @default -- `[]` (defaults to global.imagePullSecrets)\n  imagePullSecrets: []\n\n  # -- Additional command line arguments to pass to Argo CD server\n  extraArgs: []\n\n  # -- Environment variables to pass to Argo CD server\n  env: []\n\n  # -- envFrom to pass to Argo CD server\n  # @default -- `[]` (See [values.yaml])\n  envFrom: []\n  # - configMapRef:\n  #     name: config-map-name\n  # - secretRef:\n  #     name: secret-name\n\n  # -- Specify postStart and preStop lifecycle hooks for your argo-cd-server container\n  lifecycle: {}\n\n  ## Argo UI extensions\n  ## This function in tech preview stage, do expect instability or breaking changes in newer versions.\n  ## Ref: https://github.com/argoproj-labs/argocd-extensions\n  extensions:\n    # -- Enable support for Argo UI extensions\n    enabled: false\n\n    ## Argo UI extensions image\n    image:\n      # -- Repository to use for extensions image\n      repository: \"ghcr.io/argoproj-labs/argocd-extensions\"\n      # -- Tag to use for extensions image\n      tag: \"v0.2.1\"\n      # -- Image pull policy for extensions\n      # @default -- `\"\"` (defaults to global.image.imagePullPolicy)\n      imagePullPolicy: \"\"\n\n    # -- Server UI extensions container-level security context\n    # @default -- See [values.yaml]\n    containerSecurityContext:\n      runAsNonRoot: true\n      readOnlyRootFilesystem: true\n      allowPrivilegeEscalation: false\n      seccompProfile:\n        type: RuntimeDefault\n      capabilities:\n        drop:\n        - ALL\n\n    # -- Resource limits and requests for the argocd-extensions container\n    resources: {}\n    #  limits:\n    #    cpu: 50m\n    #    memory: 128Mi\n    #  requests:\n    #    cpu: 10m\n    #    memory: 64Mi\n\n  # -- Additional containers to be added to the server pod\n  ## Note: Supports use of custom Helm templates\n  extraContainers: []\n  # - name: my-sidecar\n  #   image: nginx:latest\n  # - name: lemonldap-ng-controller\n  #   image: lemonldapng/lemonldap-ng-controller:0.2.0\n  #   args:\n  #     - /lemonldap-ng-controller\n  #     - --alsologtostderr\n  #     - --configmap=$(POD_NAMESPACE)/lemonldap-ng-configuration\n  #   env:\n  #     - name: POD_NAME\n  #       valueFrom:\n  #         fieldRef:\n  #           fieldPath: metadata.name\n  #     - name: POD_NAMESPACE\n  #       valueFrom:\n  #         fieldRef:\n  #           fieldPath: metadata.namespace\n  #   volumeMounts:\n  #   - name: copy-portal-skins\n  #     mountPath: /srv/var/lib/lemonldap-ng/portal/skins\n\n  # -- Init containers to add to the server pod\n  ## If your target Kubernetes cluster(s) require a custom credential (exec) plugin\n  ## you could use this (and the same in the application controller pod) to provide such executable\n  ## Ref: https://kubernetes.io/docs/reference/access-authn-authz/authentication/#client-go-credential-plugins\n  initContainers: []\n  #  - name: download-tools\n  #    image: alpine:3\n  #    command: [sh, -c]\n  #    args:\n  #      - wget -qO kubelogin.zip https://github.com/Azure/kubelogin/releases/download/v0.0.25/kubelogin-linux-amd64.zip \u0026\u0026\n  #        unzip kubelogin.zip \u0026\u0026 mv bin/linux_amd64/kubelogin /custom-tools/\n  #    volumeMounts:\n  #      - mountPath: /custom-tools\n  #        name: custom-tools\n\n  # -- Additional volumeMounts to the server main container\n  volumeMounts: []\n  #  - mountPath: /usr/local/bin/kubelogin\n  #    name: custom-tools\n  #    subPath: kubelogin\n\n  # -- Additional volumes to the server pod\n  volumes: []\n  #  - name: custom-tools\n  #    emptyDir: {}\n\n  # -- Annotations to be added to server Deployment\n  deploymentAnnotations: {}\n\n  # -- Annotations to be added to server pods\n  podAnnotations: {}\n\n  # -- Labels to be added to server pods\n  podLabels: {}\n\n  # -- Resource limits and requests for the Argo CD server\n  resources: {}\n  #  limits:\n  #    cpu: 100m\n  #    memory: 128Mi\n  #  requests:\n  #    cpu: 50m\n  #    memory: 64Mi\n\n  # Server container ports\n  containerPorts:\n    # -- Server container port\n    server: 8080\n    # -- Metrics container port\n    metrics: 8083\n\n  # -- Host Network for Server pods\n  hostNetwork: false\n\n  # -- [DNS configuration]\n  dnsConfig: {}\n  # -- Alternative DNS policy for Server pods\n  dnsPolicy: \"ClusterFirst\"\n\n  # -- Server container-level security context\n  # @default -- See [values.yaml]\n  containerSecurityContext:\n    runAsNonRoot: true\n    readOnlyRootFilesystem: true\n    allowPrivilegeEscalation: false\n    seccompProfile:\n      type: RuntimeDefault\n    capabilities:\n      drop:\n      - ALL\n\n  ## Readiness and liveness probes for default backend\n  ## Ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/\n  readinessProbe:\n    # -- Minimum consecutive failures for the [probe] to be considered failed after having succeeded\n    failureThreshold: 3\n    # -- Number of seconds after the container has started before [probe] is initiated\n    initialDelaySeconds: 10\n    # -- How often (in seconds) to perform the [probe]\n    periodSeconds: 10\n    # -- Minimum consecutive successes for the [probe] to be considered successful after having failed\n    successThreshold: 1\n    # -- Number of seconds after which the [probe] times out\n    timeoutSeconds: 1\n\n  livenessProbe:\n    # -- Minimum consecutive failures for the [probe] to be considered failed after having succeeded\n    failureThreshold: 3\n    # -- Number of seconds after the container has started before [probe] is initiated\n    initialDelaySeconds: 10\n    # -- How often (in seconds) to perform the [probe]\n    periodSeconds: 10\n    # -- Minimum consecutive successes for the [probe] to be considered successful after having failed\n    successThreshold: 1\n    # -- Number of seconds after which the [probe] times out\n    timeoutSeconds: 1\n\n  # -- terminationGracePeriodSeconds for container lifecycle hook\n  terminationGracePeriodSeconds: 30\n\n  # -- Priority class for the Argo CD server pods\n  # @default -- `\"\"` (defaults to global.priorityClassName)\n  priorityClassName: \"\"\n\n  # -- [Node selector]\n  # @default -- `{}` (defaults to global.nodeSelector)\n  nodeSelector: {}\n\n  # -- [Tolerations] for use with node taints\n  # @default -- `[]` (defaults to global.tolerations)\n  tolerations: []\n\n  # -- Assign custom [affinity] rules to the deployment\n  # @default -- `{}` (defaults to global.affinity preset)\n  affinity: {}\n\n  # -- Assign custom [TopologySpreadConstraints] rules to the Argo CD server\n  # @default -- `[]` (defaults to global.topologySpreadConstraints)\n  ## Ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/\n  ## If labelSelector is left out, it will default to the labelSelector configuration of the deployment\n  topologySpreadConstraints: []\n    # - maxSkew: 1\n    #   topologyKey: topology.kubernetes.io/zone\n    #   whenUnsatisfiable: DoNotSchedule\n\n  # -- Deployment strategy to be added to the server Deployment\n  deploymentStrategy: {}\n    # type: RollingUpdate\n    # rollingUpdate:\n    #   maxSurge: 25%\n    #   maxUnavailable: 25%\n\n  # TLS certificate configuration via cert-manager\n  ## Ref: https://argo-cd.readthedocs.io/en/stable/operator-manual/tls/#tls-certificates-used-by-argocd-server\n  certificate:\n    # -- Deploy a Certificate resource (requires cert-manager)\n    enabled: false\n    # -- The name of the Secret that will be automatically created and managed by this Certificate resource\n    secretName: argocd-server-tls\n    # -- Certificate primary domain (commonName)\n    domain: argocd.example.com\n    # -- Certificate Subject Alternate Names (SANs)\n    additionalHosts: []\n    # -- The requested 'duration' (i.e. lifetime) of the certificate.\n    # @default -- `\"\"` (defaults to 2160h = 90d if not specified)\n    ## Ref: https://cert-manager.io/docs/usage/certificate/#renewal\n    duration: \"\"\n    # -- How long before the expiry a certificate should be renewed.\n    # @default -- `\"\"` (defaults to 360h = 15d if not specified)\n    ## Ref: https://cert-manager.io/docs/usage/certificate/#renewal\n    renewBefore: \"\"\n    # Certificate issuer\n    ## Ref: https://cert-manager.io/docs/concepts/issuer\n    issuer:\n      # -- Certificate issuer group. Set if using an external issuer. Eg. `cert-manager.io`\n      group: \"\"\n      # -- Certificate issuer kind. Either `Issuer` or `ClusterIssuer`\n      kind: \"\"\n      # -- Certificate issuer name. Eg. `letsencrypt`\n      name: \"\"\n    # Private key of the certificate\n    privateKey:\n      # -- Rotation policy of private key when certificate is re-issued. Either: `Never` or `Always`\n      rotationPolicy: Never\n      # -- The private key cryptography standards (PKCS) encoding for private key. Either: `PCKS1` or `PKCS8`\n      encoding: PKCS1\n      # -- Algorithm used to generate certificate private key. One of: `RSA`, `Ed25519` or `ECDSA`\n      algorithm: RSA\n      # -- Key bit size of the private key. If algorithm is set to `Ed25519`, size is ignored.\n      size: 2048\n    # -- Annotations to be applied to the Server Certificate\n    annotations: {}\n    # -- Usages for the certificate\n    ### Ref: https://cert-manager.io/docs/reference/api-docs/#cert-manager.io/v1.KeyUsage\n    usages: []\n\n  # TLS certificate configuration via Secret\n  ## Ref: https://argo-cd.readthedocs.io/en/stable/operator-manual/tls/#tls-certificates-used-by-argocd-server\n  certificateSecret:\n    # -- Create argocd-server-tls secret\n    enabled: false\n    # -- Annotations to be added to argocd-server-tls secret\n    annotations: {}\n    # -- Labels to be added to argocd-server-tls secret\n    labels: {}\n    # -- Private Key of the certificate\n    key: ''\n    # -- Certificate data\n    crt: ''\n\n  ## Server service configuration\n  service:\n    # -- Server service annotations\n    annotations: {}\n    # -- Server service labels\n    labels: {}\n    # -- Server service type\n    type: ClusterIP\n    # -- Server service http port for NodePort service type (only if `server.service.type` is set to \"NodePort\")\n    nodePortHttp: 30080\n    # -- Server service https port for NodePort service type (only if `server.service.type` is set to \"NodePort\")\n    nodePortHttps: 30443\n    # -- Server service http port\n    servicePortHttp: 80\n    # -- Server service https port\n    servicePortHttps: 443\n    # -- Server service http port name, can be used to route traffic via istio\n    servicePortHttpName: http\n    # -- Server service https port name, can be used to route traffic via istio\n    servicePortHttpsName: https\n    # -- Server service https port appProtocol. (should be upper case - i.e. HTTPS)\n    # servicePortHttpsAppProtocol: HTTPS\n    # -- LoadBalancer will get created with the IP specified in this field\n    loadBalancerIP: \"\"\n    # -- Source IP ranges to allow access to service from\n    loadBalancerSourceRanges: []\n    # -- Server service external IPs\n    externalIPs: []\n    # -- Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints\n    externalTrafficPolicy: \"\"\n    # -- Used to maintain session affinity. Supports `ClientIP` and `None`\n    sessionAffinity: \"\"\n\n  ## Server metrics service configuration\n  metrics:\n    # -- Deploy metrics service\n    enabled: false\n    service:\n      # -- Metrics service type\n      type: ClusterIP\n      # -- Metrics service clusterIP. `None` makes a \"headless service\" (no virtual IP)\n      clusterIP: \"\"\n      # -- Metrics service annotations\n      annotations: {}\n      # -- Metrics service labels\n      labels: {}\n      # -- Metrics service port\n      servicePort: 8083\n      # -- Metrics service port name\n      portName: http-metrics\n    serviceMonitor:\n      # -- Enable a prometheus ServiceMonitor\n      enabled: false\n      # -- Prometheus ServiceMonitor interval\n      interval: 30s\n      # -- Prometheus [RelabelConfigs] to apply to samples before scraping\n      relabelings: []\n      # -- Prometheus [MetricRelabelConfigs] to apply to samples before ingestion\n      metricRelabelings: []\n      # -- Prometheus ServiceMonitor selector\n      selector: {}\n        # prometheus: kube-prometheus\n\n      # -- Prometheus ServiceMonitor scheme\n      scheme: \"\"\n      # -- Prometheus ServiceMonitor tlsConfig\n      tlsConfig: {}\n      # -- Prometheus ServiceMonitor namespace\n      namespace: \"\"  # monitoring\n      # -- Prometheus ServiceMonitor labels\n      additionalLabels: {}\n      # -- Prometheus ServiceMonitor annotations\n      annotations: {}\n\n  serviceAccount:\n    # -- Create server service account\n    create: true\n    # -- Server service account name\n    name: argocd-server\n    # -- Annotations applied to created service account\n    annotations: {}\n    # -- Labels applied to created service account\n    labels: {}\n    # -- Automount API credentials for the Service Account\n    automountServiceAccountToken: true\n\n  ingress:\n    # -- Enable an ingress resource for the Argo CD server\n    enabled: true\n    # -- Additional ingress annotations\n    annotations: \n      cert-manager.io/cluster-issuer: \"letsencrypt-prod\"\n    # -- Additional ingress labels\n    labels: {}\n    # -- Defines which ingress controller will implement the resource\n    ingressClassName: \"nginx\"\n\n    # -- List of ingress hosts\n    ## Argo Ingress.\n    ## Hostnames must be provided if Ingress is enabled.\n    ## Secrets must be manually created in the namespace\n    hosts: \n      - argocd.koellgma.de\n\n    # -- List of ingress paths\n    paths:\n      - /\n    # -- Ingress path type. One of `Exact`, `Prefix` or `ImplementationSpecific`\n    pathType: Prefix\n    # -- Additional ingress paths\n    extraPaths: []\n      # - path: /*\n      #   pathType: Prefix\n      #   backend:\n      #     service:\n      #       name: ssl-redirect\n      #       port:\n      #         name: use-annotation\n\n    # -- Ingress TLS configuration\n    tls:\n      - secretName: argocd-tls\n        hosts:\n          - argocd.koellgma.de\n\n    # -- Uses `server.service.servicePortHttps` instead `server.service.servicePortHttp`\n    https: false\n\n  # dedicated ingress for gRPC as documented at\n  # Ref: https://argo-cd.readthedocs.io/en/stable/operator-manual/ingress/\n  ingressGrpc:\n    # -- Enable an ingress resource for the Argo CD server for dedicated [gRPC-ingress]\n    enabled: false\n    # -- Setup up gRPC ingress to work with an AWS ALB\n    isAWSALB: false\n    # -- Additional ingress annotations for dedicated [gRPC-ingress]\n    annotations: {}\n    # -- Additional ingress labels for dedicated [gRPC-ingress]\n    labels: {}\n    # -- Defines which ingress controller will implement the resource [gRPC-ingress]\n    ingressClassName: \"\"\n\n    awsALB:\n      # -- Service type for the AWS ALB gRPC service\n      ## Service Type if isAWSALB is set to true\n      ## Can be of type NodePort or ClusterIP depending on which mode you are\n      ## are running. Instance mode needs type NodePort, IP mode needs type\n      ## ClusterIP\n      ## Ref: https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.2/how-it-works/#ingress-traffic\n      serviceType: NodePort\n      # -- Backend protocol version for the AWS ALB gRPC service\n      ## This tells AWS to send traffic from the ALB using HTTP2. Can use gRPC as well if you want to leverage gRPC specific features\n      backendProtocolVersion: HTTP2\n\n    # -- List of ingress hosts for dedicated [gRPC-ingress]\n    ## Argo Ingress.\n    ## Hostnames must be provided if Ingress is enabled.\n    ## Secrets must be manually created in the namespace\n    ##\n    hosts: []\n      # - argocd.example.com\n\n    # -- List of ingress paths for dedicated [gRPC-ingress]\n    paths:\n      - /\n    # -- Ingress path type for dedicated [gRPC-ingress]. One of `Exact`, `Prefix` or `ImplementationSpecific`\n    pathType: Prefix\n    # -- Additional ingress paths for dedicated [gRPC-ingress]\n    extraPaths: []\n      # - path: /*\n      #   pathType: Prefix\n      #   backend:\n      #     service:\n      #       name: ssl-redirect\n      #       port:\n      #         name: use-annotation\n\n    # -- Ingress TLS configuration for dedicated [gRPC-ingress]\n    tls: []\n      # - secretName: your-certificate-name\n      #   hosts:\n      #     - argocd.example.com\n\n    # -- Uses `server.service.servicePortHttps` instead `server.service.servicePortHttp`\n    https: false\n\n  # Create a OpenShift Route with SSL passthrough for UI and CLI\n  # Consider setting 'hostname' e.g. https://argocd.apps-crc.testing/ using your Default Ingress Controller Domain\n  # Find your domain with: kubectl describe --namespace=openshift-ingress-operator ingresscontroller/default | grep Domain:\n  # If 'hostname' is an empty string \"\" OpenShift will create a hostname for you.\n  route:\n    # -- Enable an OpenShift Route for the Argo CD server\n    enabled: false\n    # -- Openshift Route annotations\n    annotations: {}\n    # -- Hostname of OpenShift Route\n    hostname: \"\"\n    # -- Termination type of Openshift Route\n    termination_type: passthrough\n    # -- Termination policy of Openshift Route\n    termination_policy: None\n\n  GKEbackendConfig:\n    # -- Enable BackendConfig custom resource for Google Kubernetes Engine\n    enabled: false\n    # -- [BackendConfigSpec]\n    spec: {}\n  #  spec:\n  #    iap:\n  #      enabled: true\n  #      oauthclientCredentials:\n  #        secretName: argocd-secret\n\n  ## Create a Google Managed Certificate for use with the GKE Ingress Controller\n  ## https://cloud.google.com/kubernetes-engine/docs/how-to/managed-certs\n  GKEmanagedCertificate:\n    # -- Enable ManagedCertificate custom resource for Google Kubernetes Engine.\n    enabled: false\n    # -- Domains for the Google Managed Certificate\n    domains:\n    - argocd.example.com\n\n  ## Create a Google FrontendConfig Custom Resource, for use with the GKE Ingress Controller\n  ## https://cloud.google.com/kubernetes-engine/docs/how-to/ingress-features#configuring_ingress_features_through_frontendconfig_parameters\n  GKEfrontendConfig:\n    # -- Enable FrontConfig custom resource for Google Kubernetes Engine\n    enabled: false\n    # -- [FrontendConfigSpec]\n    spec: {}\n  # spec:\n  #   redirectToHttps:\n  #     enabled: true\n  #     responseCodeName: RESPONSE_CODE\n\n## Repo Server\nrepoServer:\n  # -- Repo server name\n  name: repo-server\n\n  # -- The number of repo server pods to run\n  replicas: 1\n\n  ## Repo server Horizontal Pod Autoscaler\n  autoscaling:\n    # -- Enable Horizontal Pod Autoscaler ([HPA]) for the repo server\n    enabled: false\n    # -- Minimum number of replicas for the repo server [HPA]\n    minReplicas: 1\n    # -- Maximum number of replicas for the repo server [HPA]\n    maxReplicas: 5\n    # -- Average CPU utilization percentage for the repo server [HPA]\n    targetCPUUtilizationPercentage: 50\n    # -- Average memory utilization percentage for the repo server [HPA]\n    targetMemoryUtilizationPercentage: 50\n    # -- Configures the scaling behavior of the target in both Up and Down directions.\n    behavior: {}\n      # scaleDown:\n      #  stabilizationWindowSeconds: 300\n      #  policies:\n      #   - type: Pods\n      #     value: 1\n      #     periodSeconds: 180\n      # scaleUp:\n      #   stabilizationWindowSeconds: 300\n      #   policies:\n      #   - type: Pods\n      #     value: 2\n      #     periodSeconds: 60\n    # -- Configures custom HPA metrics for the Argo CD repo server\n    # Ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/\n    metrics: []\n\n  ## Repo server Pod Disruption Budget\n  ## Ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb/\n  pdb:\n    # -- Deploy a [PodDisruptionBudget] for the repo server\n    enabled: false\n    # -- Labels to be added to repo server pdb\n    labels: {}\n    # -- Annotations to be added to repo server pdb\n    annotations: {}\n    # -- Number of pods that are available after eviction as number or percentage (eg.: 50%)\n    # @default -- `\"\"` (defaults to 0 if not specified)\n    minAvailable: \"\"\n    # -- Number of pods that are unavailable after eviction as number or percentage (eg.: 50%).\n    ## Has higher precedence over `repoServer.pdb.minAvailable`\n    maxUnavailable: \"\"\n\n  ## Repo server image\n  image:\n    # -- Repository to use for the repo server\n    # @default -- `\"\"` (defaults to global.image.repository)\n    repository: \"\"\n    # -- Tag to use for the repo server\n    # @default -- `\"\"` (defaults to global.image.tag)\n    tag: \"\"\n    # -- Image pull policy for the repo server\n    # @default -- `\"\"` (defaults to global.image.imagePullPolicy)\n    imagePullPolicy: \"\"\n\n  # -- Secrets with credentials to pull images from a private registry\n  # @default -- `[]` (defaults to global.imagePullSecrets)\n  imagePullSecrets: []\n\n  # -- Additional command line arguments to pass to repo server\n  extraArgs: []\n\n  # -- Environment variables to pass to repo server\n  env: []\n\n  # -- envFrom to pass to repo server\n  # @default -- `[]` (See [values.yaml])\n  envFrom: []\n  # - configMapRef:\n  #     name: config-map-name\n  # - secretRef:\n  #     name: secret-name\n\n  # -- Specify postStart and preStop lifecycle hooks for your argo-repo-server container\n  lifecycle: {}\n\n  # -- Additional containers to be added to the repo server pod\n  ## Ref: https://argo-cd.readthedocs.io/en/stable/user-guide/config-management-plugins/\n  ## Note: Supports use of custom Helm templates\n  extraContainers: []\n    # - name: cmp-my-plugin\n    #   command:\n    #     - \"/var/run/argocd/argocd-cmp-server\"\n    #   image: busybox\n    #   securityContext:\n    #     runAsNonRoot: true\n    #     runAsUser: 999\n    #   volumeMounts:\n    #     - mountPath: /var/run/argocd\n    #       name: var-files\n    #     - mountPath: /home/argocd/cmp-server/plugins\n    #       name: plugins\n    #     # Remove this volumeMount if you've chosen to bake the config file into the sidecar image.\n    #     - mountPath: /home/argocd/cmp-server/config/plugin.yaml\n    #       subPath: my-plugin.yaml\n    #       name: argocd-cmp-cm\n    #     # Starting with v2.4, do NOT mount the same tmp volume as the repo-server container. The filesystem separation helps\n    #     # mitigate path traversal attacks.\n    #     - mountPath: /tmp\n    #       name: cmp-tmp\n    # - name: cmp-my-plugin2\n    #   command:\n    #     - \"/var/run/argocd/argocd-cmp-server\"\n    #   image: busybox\n    #   securityContext:\n    #     runAsNonRoot: true\n    #     runAsUser: 999\n    #   volumeMounts:\n    #     - mountPath: /var/run/argocd\n    #       name: var-files\n    #     # Remove this volumeMount if you've chosen to bake the config file into the sidecar image.\n    #     - mountPath: /home/argocd/cmp-server/plugins\n    #       name: plugins\n    #     - mountPath: /home/argocd/cmp-server/config/plugin.yaml\n    #       subPath: my-plugin2.yaml\n    #       name: argocd-cmp-cm\n    #     # Starting with v2.4, do NOT mount the same tmp volume as the repo-server container. The filesystem separation helps\n    #     # mitigate path traversal attacks.\n    #     - mountPath: /tmp\n    #       name: cmp-tmp\n\n  # -- Init containers to add to the repo server pods\n  initContainers: []\n\n  # -- Additional volumeMounts to the repo server main container\n  volumeMounts: []\n\n  # -- Additional volumes to the repo server pod\n  volumes: []\n  #  - name: argocd-cmp-cm\n  #    configMap:\n  #      name: argocd-cmp-cm\n  #  - name: cmp-tmp\n  #    emptyDir: {}\n\n  # -- Toggle the usage of a ephemeral Helm working directory\n  useEphemeralHelmWorkingDir: true\n\n  # -- Annotations to be added to repo server Deployment\n  deploymentAnnotations: {}\n\n  # -- Annotations to be added to repo server pods\n  podAnnotations: {}\n\n  # -- Labels to be added to repo server pods\n  podLabels: {}\n\n  # -- Resource limits and requests for the repo server pods\n  resources: {}\n  #  limits:\n  #    cpu: 50m\n  #    memory: 128Mi\n  #  requests:\n  #    cpu: 10m\n  #    memory: 64Mi\n\n  # Repo server container ports\n  containerPorts:\n    # -- Repo server container port\n    server: 8081\n    # -- Metrics container port\n    metrics: 8084\n\n  # -- Host Network for Repo server pods\n  hostNetwork: false\n\n    # -- [DNS configuration]\n  dnsConfig: {}\n  # -- Alternative DNS policy for Repo server pods\n  dnsPolicy: \"ClusterFirst\"\n\n  # -- Repo server container-level security context\n  # @default -- See [values.yaml]\n  containerSecurityContext:\n    runAsNonRoot: true\n    readOnlyRootFilesystem: true\n    allowPrivilegeEscalation: false\n    seccompProfile:\n      type: RuntimeDefault\n    capabilities:\n      drop:\n      - ALL\n\n  ## Readiness and liveness probes for default backend\n  ## Ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/\n  readinessProbe:\n    # -- Minimum consecutive failures for the [probe] to be considered failed after having succeeded\n    failureThreshold: 3\n    # -- Number of seconds after the container has started before [probe] is initiated\n    initialDelaySeconds: 10\n    # -- How often (in seconds) to perform the [probe]\n    periodSeconds: 10\n    # -- Minimum consecutive successes for the [probe] to be considered successful after having failed\n    successThreshold: 1\n    # -- Number of seconds after which the [probe] times out\n    timeoutSeconds: 1\n\n  livenessProbe:\n    # -- Minimum consecutive failures for the [probe] to be considered failed after having succeeded\n    failureThreshold: 3\n    # -- Number of seconds after the container has started before [probe] is initiated\n    initialDelaySeconds: 10\n    # -- How often (in seconds) to perform the [probe]\n    periodSeconds: 10\n    # -- Minimum consecutive successes for the [probe] to be considered successful after having failed\n    successThreshold: 1\n    # -- Number of seconds after which the [probe] times out\n    timeoutSeconds: 1\n\n  # -- terminationGracePeriodSeconds for container lifecycle hook\n  terminationGracePeriodSeconds: 30\n\n  # -- [Node selector]\n  # @default -- `{}` (defaults to global.nodeSelector)\n  nodeSelector: {}\n\n  # -- [Tolerations] for use with node taints\n  # @default -- `[]` (defaults to global.tolerations)\n  tolerations: []\n\n  # -- Assign custom [affinity] rules to the deployment\n  # @default -- `{}` (defaults to global.affinity preset)\n  affinity: {}\n\n  # -- Assign custom [TopologySpreadConstraints] rules to the repo server\n  # @default -- `[]` (defaults to global.topologySpreadConstraints)\n  ## Ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/\n  ## If labelSelector is left out, it will default to the labelSelector configuration of the deployment\n  topologySpreadConstraints: []\n    # - maxSkew: 1\n    #   topologyKey: topology.kubernetes.io/zone\n    #   whenUnsatisfiable: DoNotSchedule\n\n  # -- Deployment strategy to be added to the repo server Deployment\n  deploymentStrategy: {}\n    # type: RollingUpdate\n    # rollingUpdate:\n    #   maxSurge: 25%\n    #   maxUnavailable: 25%\n\n  # -- Priority class for the repo server pods\n  # @default -- `\"\"` (defaults to global.priorityClassName)\n  priorityClassName: \"\"\n\n  # TLS certificate configuration via Secret\n  ## Ref: https://argo-cd.readthedocs.io/en/stable/operator-manual/tls/#configuring-tls-to-argocd-repo-server\n  ## Note: Issuing certificates via cert-manager in not supported right now because it's not possible to restart repo server automatically without extra controllers.\n  certificateSecret:\n    # -- Create argocd-repo-server-tls secret\n    enabled: false\n    # -- Annotations to be added to argocd-repo-server-tls secret\n    annotations: {}\n    # -- Labels to be added to argocd-repo-server-tls secret\n    labels: {}\n    # -- Certificate authority. Required for self-signed certificates.\n    ca: ''\n    # -- Certificate private key\n    key: ''\n    # -- Certificate data. Must contain SANs of Repo service (ie: argocd-repo-server, argocd-repo-server.argo-cd.svc)\n    crt: ''\n\n  ## Repo server service configuration\n  service:\n    # -- Repo server service annotations\n    annotations: {}\n    # -- Repo server service labels\n    labels: {}\n    # -- Repo server service port\n    port: 8081\n    # -- Repo server service port name\n    portName: https-repo-server\n\n  ## Repo server metrics service configuration\n  metrics:\n    # -- Deploy metrics service\n    enabled: false\n    service:\n      # -- Metrics service type\n      type: ClusterIP\n      # -- Metrics service clusterIP. `None` makes a \"headless service\" (no virtual IP)\n      clusterIP: \"\"\n      # -- Metrics service annotations\n      annotations: {}\n      # -- Metrics service labels\n      labels: {}\n      # -- Metrics service port\n      servicePort: 8084\n      # -- Metrics service port name\n      portName: http-metrics\n    serviceMonitor:\n      # -- Enable a prometheus ServiceMonitor\n      enabled: false\n      # -- Prometheus ServiceMonitor interval\n      interval: 30s\n      # -- Prometheus [RelabelConfigs] to apply to samples before scraping\n      relabelings: []\n      # -- Prometheus [MetricRelabelConfigs] to apply to samples before ingestion\n      metricRelabelings: []\n      # -- Prometheus ServiceMonitor selector\n      selector: {}\n        # prometheus: kube-prometheus\n\n      # -- Prometheus ServiceMonitor scheme\n      scheme: \"\"\n      # -- Prometheus ServiceMonitor tlsConfig\n      tlsConfig: {}\n      # -- Prometheus ServiceMonitor namespace\n      namespace: \"\" # \"monitoring\"\n      # -- Prometheus ServiceMonitor labels\n      additionalLabels: {}\n      # -- Prometheus ServiceMonitor annotations\n      annotations: {}\n\n  ## Enable Custom Rules for the Repo server's Cluster Role resource\n  ## Enable this and set the rules: to whatever custom rules you want for the Cluster Role resource.\n  ## Defaults to off\n  clusterRoleRules:\n    # -- Enable custom rules for the Repo server's Cluster Role resource\n    enabled: false\n    # -- List of custom rules for the Repo server's Cluster Role resource\n    rules: []\n\n  ## Repo server service account\n  ## If create is set to true, make sure to uncomment the name and update the rbac section below\n  serviceAccount:\n    # -- Create repo server service account\n    create: true\n    # -- Repo server service account name\n    name: \"\" # \"argocd-repo-server\"\n    # -- Annotations applied to created service account\n    annotations: {}\n    # -- Labels applied to created service account\n    labels: {}\n    # -- Automount API credentials for the Service Account\n    automountServiceAccountToken: true\n\n  # -- Repo server rbac rules\n  rbac: []\n  #   - apiGroups:\n  #     - argoproj.io\n  #     resources:\n  #     - applications\n  #     verbs:\n  #     - get\n  #     - list\n  #     - watch\n\n## ApplicationSet controller\napplicationSet:\n  # -- Enable ApplicationSet controller\n  enabled: true\n\n  # -- ApplicationSet controller name string\n  name: applicationset-controller\n\n  # -- The number of ApplicationSet controller pods to run\n  replicas: 1\n\n  ## ApplicationSet controller Pod Disruption Budget\n  ## Ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb/\n  pdb:\n    # -- Deploy a [PodDisruptionBudget] for the ApplicationSet controller\n    enabled: false\n    # -- Labels to be added to ApplicationSet controller pdb\n    labels: {}\n    # -- Annotations to be added to ApplicationSet controller pdb\n    annotations: {}\n    # -- Number of pods that are available after eviction as number or percentage (eg.: 50%)\n    # @default -- `\"\"` (defaults to 0 if not specified)\n    minAvailable: \"\"\n    # -- Number of pods that are unavailable after eviction as number or percentage (eg.: 50%).\n    ## Has higher precedence over `applicationSet.pdb.minAvailable`\n    maxUnavailable: \"\"\n\n  ## ApplicationSet controller image\n  image:\n    # -- Repository to use for the ApplicationSet controller\n    # @default -- `\"\"` (defaults to global.image.repository)\n    repository: \"\"\n    # -- Tag to use for the ApplicationSet controller\n    # @default -- `\"\"` (defaults to global.image.tag)\n    tag: \"\"\n    # -- Image pull policy for the ApplicationSet controller\n    # @default -- `\"\"` (defaults to global.image.imagePullPolicy)\n    imagePullPolicy: \"\"\n\n  # -- If defined, uses a Secret to pull an image from a private Docker registry or repository.\n  # @default -- `[]` (defaults to global.imagePullSecrets)\n  imagePullSecrets: []\n\n  # -- DEPRECATED - ApplicationSet controller command line flags\n  args: {}\n    # DEPRECATED - Use configs.params.applicationsetcontroller.policy to override\n    # -- How application is synced between the generator and the cluster\n    # policy: sync\n    # DEPRECATED - Use configs.params.applicationsetcontroller.dryrun to override\n    # -- Enable dry run mode\n    # dryRun: false\n\n  # -- List of extra cli args to add\n  extraArgs: []\n\n  # -- Environment variables to pass to the ApplicationSet controller\n  extraEnv: []\n    # - name: \"MY_VAR\"\n    #   value: \"value\"\n\n  # -- envFrom to pass to the ApplicationSet controller\n  # @default -- `[]` (See [values.yaml])\n  extraEnvFrom: []\n    # - configMapRef:\n    #     name: config-map-name\n    # - secretRef:\n    #     name: secret-name\n\n  # -- Additional containers to be added to the ApplicationSet controller pod\n  ## Note: Supports use of custom Helm templates\n  extraContainers: []\n\n  # -- Init containers to add to the ApplicationSet controller pod\n  ## Note: Supports use of custom Helm templates\n  initContainers: []\n\n  # -- List of extra mounts to add (normally used with extraVolumes)\n  extraVolumeMounts: []\n\n  # -- List of extra volumes to add\n  extraVolumes: []\n\n  ## Metrics service configuration\n  metrics:\n    # -- Deploy metrics service\n    enabled: false\n    service:\n      # -- Metrics service type\n      type: ClusterIP\n      # -- Metrics service clusterIP. `None` makes a \"headless service\" (no virtual IP)\n      clusterIP: \"\"\n      # -- Metrics service annotations\n      annotations: {}\n      # -- Metrics service labels\n      labels: {}\n      # -- Metrics service port\n      servicePort: 8080\n      # -- Metrics service port name\n      portName: http-metrics\n    serviceMonitor:\n      # -- Enable a prometheus ServiceMonitor\n      enabled: false\n      # -- Prometheus ServiceMonitor interval\n      interval: 30s\n      # -- Prometheus [RelabelConfigs] to apply to samples before scraping\n      relabelings: []\n      # -- Prometheus [MetricRelabelConfigs] to apply to samples before ingestion\n      metricRelabelings: []\n      # -- Prometheus ServiceMonitor selector\n      selector: {}\n        # prometheus: kube-prometheus\n\n      # -- Prometheus ServiceMonitor scheme\n      scheme: \"\"\n      # -- Prometheus ServiceMonitor tlsConfig\n      tlsConfig: {}\n      # -- Prometheus ServiceMonitor namespace\n      namespace: \"\"  # monitoring\n      # -- Prometheus ServiceMonitor labels\n      additionalLabels: {}\n      # -- Prometheus ServiceMonitor annotations\n      annotations: {}\n\n  ## ApplicationSet service configuration\n  service:\n    # -- ApplicationSet service annotations\n    annotations: {}\n    # -- ApplicationSet service labels\n    labels: {}\n    # -- ApplicationSet service type\n    type: ClusterIP\n    # -- ApplicationSet service port\n    port: 7000\n    # -- ApplicationSet service port name\n    portName: webhook\n\n  serviceAccount:\n    # -- Create ApplicationSet controller service account\n    create: true\n    # -- ApplicationSet controller service account name\n    name: argocd-applicationset-controller\n    # -- Annotations applied to created service account\n    annotations: {}\n    # -- Labels applied to created service account\n    labels: {}\n    # -- Automount API credentials for the Service Account\n    automountServiceAccountToken: true\n\n  # -- Annotations to be added to ApplicationSet controller Deployment\n  deploymentAnnotations: {}\n\n  # -- Annotations for the ApplicationSet controller pods\n  podAnnotations: {}\n\n  # -- Labels for the ApplicationSet controller pods\n  podLabels: {}\n\n  # -- Resource limits and requests for the ApplicationSet controller pods.\n  resources: {}\n    # limits:\n    #   cpu: 100m\n    #   memory: 128Mi\n    # requests:\n    #   cpu: 100m\n    #   memory: 128Mi\n\n  # ApplicationSet controller container ports\n  containerPorts:\n    # -- Metrics container port\n    metrics: 8080\n    # -- Probe container port\n    probe: 8081\n    # -- Webhook container port\n    webhook: 7000\n\n  # -- [DNS configuration]\n  dnsConfig: {}\n  # -- Alternative DNS policy for ApplicationSet controller pods\n  dnsPolicy: \"ClusterFirst\"\n\n  # -- ApplicationSet controller container-level security context\n  # @default -- See [values.yaml]\n  containerSecurityContext:\n    runAsNonRoot: true\n    readOnlyRootFilesystem: true\n    allowPrivilegeEscalation: false\n    seccompProfile:\n      type: RuntimeDefault\n    capabilities:\n      drop:\n      - ALL\n\n  ## Probes for ApplicationSet controller (optional)\n  ## Ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/\n  readinessProbe:\n    # -- Enable Kubernetes liveness probe for ApplicationSet controller\n    enabled: false\n    # -- Number of seconds after the container has started before [probe] is initiated\n    initialDelaySeconds: 10\n    # -- How often (in seconds) to perform the [probe]\n    periodSeconds: 10\n    # -- Number of seconds after which the [probe] times out\n    timeoutSeconds: 1\n    # -- Minimum consecutive successes for the [probe] to be considered successful after having failed\n    successThreshold: 1\n    # -- Minimum consecutive failures for the [probe] to be considered failed after having succeeded\n    failureThreshold: 3\n\n  livenessProbe:\n    # -- Enable Kubernetes liveness probe for ApplicationSet controller\n    enabled: false\n    # -- Number of seconds after the container has started before [probe] is initiated\n    initialDelaySeconds: 10\n    # -- How often (in seconds) to perform the [probe]\n    periodSeconds: 10\n    # -- Number of seconds after which the [probe] times out\n    timeoutSeconds: 1\n    # -- Minimum consecutive successes for the [probe] to be considered successful after having failed\n    successThreshold: 1\n    # -- Minimum consecutive failures for the [probe] to be considered failed after having succeeded\n    failureThreshold: 3\n\n  # -- terminationGracePeriodSeconds for container lifecycle hook\n  terminationGracePeriodSeconds: 30\n\n  # -- [Node selector]\n  # @default -- `{}` (defaults to global.nodeSelector)\n  nodeSelector: {}\n\n  # -- [Tolerations] for use with node taints\n  # @default -- `[]` (defaults to global.tolerations)\n  tolerations: []\n\n  # -- Assign custom [affinity] rules\n  # @default -- `{}` (defaults to global.affinity preset)\n  affinity: {}\n\n  # -- Assign custom [TopologySpreadConstraints] rules to the ApplicationSet controller\n  # @default -- `[]` (defaults to global.topologySpreadConstraints)\n  ## Ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/\n  ## If labelSelector is left out, it will default to the labelSelector configuration of the deployment\n  topologySpreadConstraints: []\n    # - maxSkew: 1\n    #   topologyKey: topology.kubernetes.io/zone\n    #   whenUnsatisfiable: DoNotSchedule\n\n  # -- Deployment strategy to be added to the ApplicationSet controller Deployment\n  deploymentStrategy: {}\n    # type: RollingUpdate\n    # rollingUpdate:\n    #   maxSurge: 25%\n    #   maxUnavailable: 25%\n\n  # -- Priority class for the ApplicationSet controller pods\n  # @default -- `\"\"` (defaults to global.priorityClassName)\n  priorityClassName: \"\"\n\n  ## Webhook for the Git Generator\n  ## Ref: https://argocd-applicationset.readthedocs.io/en/master/Generators-Git/#webhook-configuration)\n  webhook:\n    ingress:\n      # -- Enable an ingress resource for Webhooks\n      enabled: false\n      # -- Additional ingress annotations\n      annotations: {}\n      # -- Additional ingress labels\n      labels: {}\n      # -- Defines which ingress ApplicationSet controller will implement the resource\n      ingressClassName: \"\"\n\n      # -- List of ingress hosts\n      ## Hostnames must be provided if Ingress is enabled.\n      ## Secrets must be manually created in the namespace\n      hosts: []\n        # - argocd-applicationset.example.com\n\n      # -- List of ingress paths\n      paths:\n        - /api/webhook\n      # -- Ingress path type. One of `Exact`, `Prefix` or `ImplementationSpecific`\n      pathType: Prefix\n      # -- Additional ingress paths\n      extraPaths: []\n        # - path: /*\n        #   backend:\n        #     serviceName: ssl-redirect\n        #     servicePort: use-annotation\n        ## for Kubernetes \u003e=1.19 (when \"networking.k8s.io/v1\" is used)\n        # - path: /*\n        #   pathType: Prefix\n        #   backend:\n        #     service:\n        #       name: ssl-redirect\n        #       port:\n        #         name: use-annotation\n\n      # -- Ingress TLS configuration\n      tls: []\n        # - secretName: argocd-applicationset-tls\n        #   hosts:\n        #     - argocd-applicationset.example.com\n\n  # TLS certificate configuration via cert-manager\n  ## Ref: https://argo-cd.readthedocs.io/en/stable/operator-manual/tls/#tls-configuration\n  certificate:\n    # -- Deploy a Certificate resource (requires cert-manager)\n    enabled: false\n    # -- The name of the Secret that will be automatically created and managed by this Certificate resource\n    secretName: argocd-application-controller-tls\n    # -- Certificate primary domain (commonName)\n    domain: argocd.example.com\n    # -- Certificate Subject Alternate Names (SANs)\n    additionalHosts: []\n    # -- The requested 'duration' (i.e. lifetime) of the certificate.\n    # @default -- `\"\"` (defaults to 2160h = 90d if not specified)\n    ## Ref: https://cert-manager.io/docs/usage/certificate/#renewal\n    duration: \"\"\n    # -- How long before the expiry a certificate should be renewed.\n    # @default -- `\"\"` (defaults to 360h = 15d if not specified)\n    ## Ref: https://cert-manager.io/docs/usage/certificate/#renewal\n    renewBefore: \"\"\n    # Certificate issuer\n    ## Ref: https://cert-manager.io/docs/concepts/issuer\n    issuer:\n      # -- Certificate issuer group. Set if using an external issuer. Eg. `cert-manager.io`\n      group: \"\"\n      # -- Certificate issuer kind. Either `Issuer` or `ClusterIssuer`\n      kind: \"\"\n      # -- Certificate issuer name. Eg. `letsencrypt`\n      name: \"\"\n    # Private key of the certificate\n    privateKey:\n      # -- Rotation policy of private key when certificate is re-issued. Either: `Never` or `Always`\n      rotationPolicy: Never\n      # -- The private key cryptography standards (PKCS) encoding for private key. Either: `PCKS1` or `PKCS8`\n      encoding: PKCS1\n      # -- Algorithm used to generate certificate private key. One of: `RSA`, `Ed25519` or `ECDSA`\n      algorithm: RSA\n      # -- Key bit size of the private key. If algorithm is set to `Ed25519`, size is ignored.\n      size: 2048\n    # -- Annotations to be applied to the ApplicationSet Certificate\n    annotations: {}\n\n## Notifications controller\nnotifications:\n  # -- Enable notifications controller\n  enabled: true\n\n  # -- Notifications controller name string\n  name: notifications-controller\n\n  # -- Argo CD dashboard url; used in place of {{.context.argocdUrl}} in templates\n  argocdUrl:\n\n  ## Notifications controller Pod Disruption Budget\n  ## Ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb/\n  pdb:\n    # -- Deploy a [PodDisruptionBudget] for the notifications controller\n    enabled: false\n    # -- Labels to be added to notifications controller pdb\n    labels: {}\n    # -- Annotations to be added to notifications controller pdb\n    annotations: {}\n    # -- Number of pods that are available after eviction as number or percentage (eg.: 50%)\n    # @default -- `\"\"` (defaults to 0 if not specified)\n    minAvailable: \"\"\n    # -- Number of pods that are unavailable after eviction as number or percentage (eg.: 50%).\n    ## Has higher precedence over `notifications.pdb.minAvailable`\n    maxUnavailable: \"\"\n\n  ## Notifications controller image\n  image:\n    # -- Repository to use for the notifications controller\n    # @default -- `\"\"` (defaults to global.image.repository)\n    repository: \"\"\n    # -- Tag to use for the notifications controller\n    # @default -- `\"\"` (defaults to global.image.tag)\n    tag: \"\"\n    # -- Image pull policy for the notifications controller\n    # @default -- `\"\"` (defaults to global.image.imagePullPolicy)\n    imagePullPolicy: \"\"\n\n  # -- Secrets with credentials to pull images from a private registry\n  # @default -- `[]` (defaults to global.imagePullSecrets)\n  imagePullSecrets: []\n\n  # -- Notifications controller log format. Either `text` or `json`\n  # @default -- `\"\"` (defaults to global.logging.format)\n  logFormat: \"\"\n  # -- Notifications controller log level. One of: `debug`, `info`, `warn`, `error`\n  # @default -- `\"\"` (defaults to global.logging.level)\n  logLevel: \"\"\n\n  # -- Extra arguments to provide to the notifications controller\n  extraArgs: []\n\n  # -- Additional container environment variables\n  extraEnv: []\n\n  # -- envFrom to pass to the notifications controller\n  # @default -- `[]` (See [values.yaml])\n  extraEnvFrom: []\n    # - configMapRef:\n    #     name: config-map-name\n    # - secretRef:\n    #     name: secret-name\n\n  # -- Additional containers to be added to the notifications controller pod\n  ## Note: Supports use of custom Helm templates\n  extraContainers: []\n\n  # -- Init containers to add to the notifications controller pod\n  ## Note: Supports use of custom Helm templates\n  initContainers: []\n\n  # -- List of extra mounts to add (normally used with extraVolumes)\n  extraVolumeMounts: []\n\n  # -- List of extra volumes to add\n  extraVolumes: []\n\n  # -- Define user-defined context\n  ## For more information: https://argocd-notifications.readthedocs.io/en/stable/templates/#defining-user-defined-context\n  context: {}\n    # region: east\n    # environmentName: staging\n\n  secret:\n    # -- Whether helm chart creates notifications controller secret\n    create: true\n\n    # -- key:value pairs of annotations to be added to the secret\n    annotations: {}\n\n    # -- key:value pairs of labels to be added to the secret\n    labels: {}\n\n    # -- Generic key:value pairs to be inserted into the secret\n    ## Can be used for templates, notification services etc. Some examples given below.\n    ## For more information: https://argocd-notifications.readthedocs.io/en/stable/services/overview/\n    items: {}\n      # slack-token:\n      #   # For more information: https://argocd-notifications.readthedocs.io/en/stable/services/slack/\n\n      # grafana-apiKey:\n      #   # For more information: https://argocd-notifications.readthedocs.io/en/stable/services/grafana/\n\n      # webhooks-github-token:\n\n      # email-username:\n      # email-password:\n        # For more information: https://argocd-notifications.readthedocs.io/en/stable/services/email/\n\n  metrics:\n    # -- Enables prometheus metrics server\n    enabled: false\n    # -- Metrics port\n    port: 9001\n    service:\n      # -- Metrics service type\n      type: ClusterIP\n      # -- Metrics service clusterIP. `None` makes a \"headless service\" (no virtual IP)\n      clusterIP: \"\"\n      # -- Metrics service annotations\n      annotations: {}\n      # -- Metrics service labels\n      labels: {}\n      # -- Metrics service port name\n      portName: http-metrics\n    serviceMonitor:\n      # -- Enable a prometheus ServiceMonitor\n      enabled: false\n      # -- Prometheus ServiceMonitor selector\n      selector: {}\n        # prometheus: kube-prometheus\n      # -- Prometheus ServiceMonitor labels\n      additionalLabels: {}\n      # -- Prometheus ServiceMonitor annotations\n      annotations: {}\n      # namespace: monitoring\n      # interval: 30s\n      # scrapeTimeout: 10s\n      # -- Prometheus ServiceMonitor scheme\n      scheme: \"\"\n      # -- Prometheus ServiceMonitor tlsConfig\n      tlsConfig: {}\n      # -- Prometheus [RelabelConfigs] to apply to samples before scraping\n      relabelings: []\n      # -- Prometheus [MetricRelabelConfigs] to apply to samples before ingestion\n      metricRelabelings: []\n\n  # -- Configures notification services such as slack, email or custom webhook\n  # @default -- See [values.yaml]\n  ## For more information: https://argocd-notifications.readthedocs.io/en/stable/services/overview/\n  notifiers: {}\n    # service.slack: |\n    #   token: $slack-token\n\n  # -- Annotations to be applied to the notifications controller Deployment\n  deploymentAnnotations: {}\n\n  # -- Annotations to be applied to the notifications controller Pods\n  podAnnotations: {}\n\n  # -- Labels to be applied to the notifications controller Pods\n  podLabels: {}\n\n  # -- Resource limits and requests for the notifications controller\n  resources: {}\n    # limits:\n    #   cpu: 100m\n    #   memory: 128Mi\n    # requests:\n    #   cpu: 100m\n    #   memory: 128Mi\n\n  # Notification controller container ports\n  containerPorts:\n    # -- Metrics container port\n    metrics: 9001\n\n  # -- [DNS configuration]\n  dnsConfig: {}\n  # -- Alternative DNS policy for notifications controller Pods\n  dnsPolicy: \"ClusterFirst\"\n\n  # -- Notification controller container-level security Context\n  # @default -- See [values.yaml]\n  containerSecurityContext:\n    runAsNonRoot: true\n    readOnlyRootFilesystem: true\n    allowPrivilegeEscalation: false\n    seccompProfile:\n      type: RuntimeDefault\n    capabilities:\n      drop:\n      - ALL\n\n  # -- terminationGracePeriodSeconds for container lifecycle hook\n  terminationGracePeriodSeconds: 30\n\n  # -- [Node selector]\n  # @default -- `{}` (defaults to global.nodeSelector)\n  nodeSelector: {}\n\n  # -- [Tolerations] for use with node taints\n  # @default -- `[]` (defaults to global.tolerations)\n  tolerations: []\n\n  # -- Assign custom [affinity] rules\n  # @default -- `{}` (defaults to global.affinity preset)\n  affinity: {}\n\n  # -- Assign custom [TopologySpreadConstraints] rules to the application controller\n  # @default -- `[]` (defaults to global.topologySpreadConstraints)\n  ## Ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/\n  ## If labelSelector is left out, it will default to the labelSelector configuration of the deployment\n  topologySpreadConstraints: []\n    # - maxSkew: 1\n    #   topologyKey: topology.kubernetes.io/zone\n    #   whenUnsatisfiable: DoNotSchedule\n\n  # -- Deployment strategy to be added to the notifications controller Deployment\n  deploymentStrategy:\n    type: Recreate\n\n  # -- Priority class for the notifications controller pods\n  # @default -- `\"\"` (defaults to global.priorityClassName)\n  priorityClassName: \"\"\n\n  serviceAccount:\n    # -- Create notifications controller service account\n    create: true\n    # -- Notification controller service account name\n    name: argocd-notifications-controller\n    # -- Annotations applied to created service account\n    annotations: {}\n    # -- Labels applied to created service account\n    labels: {}\n    # -- Automount API credentials for the Service Account\n    automountServiceAccountToken: true\n\n  cm:\n    # -- Whether helm chart creates notifications controller config map\n    create: true\n\n  ## Enable this and set the rules: to whatever custom rules you want for the Cluster Role resource.\n  ## Defaults to off\n  clusterRoleRules:\n    # -- List of custom rules for the notifications controller's ClusterRole resource\n    rules: []\n\n  # -- Contains centrally managed global application subscriptions\n  ## For more information: https://argocd-notifications.readthedocs.io/en/stable/subscriptions/\n  subscriptions: []\n    # # subscription for on-sync-status-unknown trigger notifications\n    # - recipients:\n    #   - slack:test2\n    #   - email:test@gmail.com\n    #   triggers:\n    #   - on-sync-status-unknown\n    # # subscription restricted to applications with matching labels only\n    # - recipients:\n    #   - slack:test3\n    #   selector: test=true\n    #   triggers:\n    #   - on-sync-status-unknown\n\n  # -- The notification template is used to generate the notification content\n  ## For more information: https://argocd-notifications.readthedocs.io/en/stable/templates/\n  templates: {}\n    # template.app-deployed: |\n    #   email:\n    #     subject: New version of an application {{.app.metadata.name}} is up and running.\n    #   message: |\n    #     {{if eq .serviceType \"slack\"}}:white_check_mark:{{end}} Application {{.app.metadata.name}} is now running new version of deployments manifests.\n    #   slack:\n    #     attachments: |\n    #       [{\n    #         \"title\": \"{{ .app.metadata.name}}\",\n    #         \"title_link\":\"{{.context.argocdUrl}}/applications/{{.app.metadata.name}}\",\n    #         \"color\": \"#18be52\",\n    #         \"fields\": [\n    #         {\n    #           \"title\": \"Sync Status\",\n    #           \"value\": \"{{.app.status.sync.status}}\",\n    #           \"short\": true\n    #         },\n    #         {\n    #           \"title\": \"Repository\",\n    #           \"value\": \"{{.app.spec.source.repoURL}}\",\n    #           \"short\": true\n    #         },\n    #         {\n    #           \"title\": \"Revision\",\n    #           \"value\": \"{{.app.status.sync.revision}}\",\n    #           \"short\": true\n    #         }\n    #         {{range $index, $c := .app.status.conditions}}\n    #         {{if not $index}},{{end}}\n    #         {{if $index}},{{end}}\n    #         {\n    #           \"title\": \"{{$c.type}}\",\n    #           \"value\": \"{{$c.message}}\",\n    #           \"short\": true\n    #         }\n    #         {{end}}\n    #         ]\n    #       }]\n    # template.app-health-degraded: |\n    #   email:\n    #     subject: Application {{.app.metadata.name}} has degraded.\n    #   message: |\n    #     {{if eq .serviceType \"slack\"}}:exclamation:{{end}} Application {{.app.metadata.name}} has degraded.\n    #     Application details: {{.context.argocdUrl}}/applications/{{.app.metadata.name}}.\n    #   slack:\n    #     attachments: |-\n    #       [{\n    #         \"title\": \"{{ .app.metadata.name}}\",\n    #         \"title_link\": \"{{.context.argocdUrl}}/applications/{{.app.metadata.name}}\",\n    #         \"color\": \"#f4c030\",\n    #         \"fields\": [\n    #         {\n    #           \"title\": \"Sync Status\",\n    #           \"value\": \"{{.app.status.sync.status}}\",\n    #           \"short\": true\n    #         },\n    #         {\n    #           \"title\": \"Repository\",\n    #           \"value\": \"{{.app.spec.source.repoURL}}\",\n    #           \"short\": true\n    #         }\n    #         {{range $index, $c := .app.status.conditions}}\n    #         {{if not $index}},{{end}}\n    #         {{if $index}},{{end}}\n    #         {\n    #           \"title\": \"{{$c.type}}\",\n    #           \"value\": \"{{$c.message}}\",\n    #           \"short\": true\n    #         }\n    #         {{end}}\n    #         ]\n    #       }]\n    # template.app-sync-failed: |\n    #   email:\n    #     subject: Failed to sync application {{.app.metadata.name}}.\n    #   message: |\n    #     {{if eq .serviceType \"slack\"}}:exclamation:{{end}}  The sync operation of application {{.app.metadata.name}} has failed at {{.app.status.operationState.finishedAt}} with the following error: {{.app.status.operationState.message}}\n    #     Sync operation details are available at: {{.context.argocdUrl}}/applications/{{.app.metadata.name}}?operation=true .\n    #   slack:\n    #     attachments: |-\n    #       [{\n    #         \"title\": \"{{ .app.metadata.name}}\",\n    #         \"title_link\":\"{{.context.argocdUrl}}/applications/{{.app.metadata.name}}\",\n    #         \"color\": \"#E96D76\",\n    #         \"fields\": [\n    #         {\n    #           \"title\": \"Sync Status\",\n    #           \"value\": \"{{.app.status.sync.status}}\",\n    #           \"short\": true\n    #         },\n    #         {\n    #           \"title\": \"Repository\",\n    #           \"value\": \"{{.app.spec.source.repoURL}}\",\n    #           \"short\": true\n    #         }\n    #         {{range $index, $c := .app.status.conditions}}\n    #         {{if not $index}},{{end}}\n    #         {{if $index}},{{end}}\n    #         {\n    #           \"title\": \"{{$c.type}}\",\n    #           \"value\": \"{{$c.message}}\",\n    #           \"short\": true\n    #         }\n    #         {{end}}\n    #         ]\n    #       }]\n    # template.app-sync-running: |\n    #   email:\n    #     subject: Start syncing application {{.app.metadata.name}}.\n    #   message: |\n    #     The sync operation of application {{.app.metadata.name}} has started at {{.app.status.operationState.startedAt}}.\n    #     Sync operation details are available at: {{.context.argocdUrl}}/applications/{{.app.metadata.name}}?operation=true .\n    #   slack:\n    #     attachments: |-\n    #       [{\n    #         \"title\": \"{{ .app.metadata.name}}\",\n    #         \"title_link\":\"{{.context.argocdUrl}}/applications/{{.app.metadata.name}}\",\n    #         \"color\": \"#0DADEA\",\n    #         \"fields\": [\n    #         {\n    #           \"title\": \"Sync Status\",\n    #           \"value\": \"{{.app.status.sync.status}}\",\n    #           \"short\": true\n    #         },\n    #         {\n    #           \"title\": \"Repository\",\n    #           \"value\": \"{{.app.spec.source.repoURL}}\",\n    #           \"short\": true\n    #         }\n    #         {{range $index, $c := .app.status.conditions}}\n    #         {{if not $index}},{{end}}\n    #         {{if $index}},{{end}}\n    #         {\n    #           \"title\": \"{{$c.type}}\",\n    #           \"value\": \"{{$c.message}}\",\n    #           \"short\": true\n    #         }\n    #         {{end}}\n    #         ]\n    #       }]\n    # template.app-sync-status-unknown: |\n    #   email:\n    #     subject: Application {{.app.metadata.name}} sync status is 'Unknown'\n    #   message: |\n    #     {{if eq .serviceType \"slack\"}}:exclamation:{{end}} Application {{.app.metadata.name}} sync is 'Unknown'.\n    #     Application details: {{.context.argocdUrl}}/applications/{{.app.metadata.name}}.\n    #     {{if ne .serviceType \"slack\"}}\n    #     {{range $c := .app.status.conditions}}\n    #         * {{$c.message}}\n    #     {{end}}\n    #     {{end}}\n    #   slack:\n    #     attachments: |-\n    #       [{\n    #         \"title\": \"{{ .app.metadata.name}}\",\n    #         \"title_link\":\"{{.context.argocdUrl}}/applications/{{.app.metadata.name}}\",\n    #         \"color\": \"#E96D76\",\n    #         \"fields\": [\n    #         {\n    #           \"title\": \"Sync Status\",\n    #           \"value\": \"{{.app.status.sync.status}}\",\n    #           \"short\": true\n    #         },\n    #         {\n    #           \"title\": \"Repository\",\n    #           \"value\": \"{{.app.spec.source.repoURL}}\",\n    #           \"short\": true\n    #         }\n    #         {{range $index, $c := .app.status.conditions}}\n    #         {{if not $index}},{{end}}\n    #         {{if $index}},{{end}}\n    #         {\n    #           \"title\": \"{{$c.type}}\",\n    #           \"value\": \"{{$c.message}}\",\n    #           \"short\": true\n    #         }\n    #         {{end}}\n    #         ]\n    #       }]\n    # template.app-sync-succeeded: |\n    #   email:\n    #     subject: Application {{.app.metadata.name}} has been successfully synced.\n    #   message: |\n    #     {{if eq .serviceType \"slack\"}}:white_check_mark:{{end}} Application {{.app.metadata.name}} has been successfully synced at {{.app.status.operationState.finishedAt}}.\n    #     Sync operation details are available at: {{.context.argocdUrl}}/applications/{{.app.metadata.name}}?operation=true .\n    #   slack:\n    #     attachments: |-\n    #       [{\n    #         \"title\": \"{{ .app.metadata.name}}\",\n    #         \"title_link\":\"{{.context.argocdUrl}}/applications/{{.app.metadata.name}}\",\n    #         \"color\": \"#18be52\",\n    #         \"fields\": [\n    #         {\n    #           \"title\": \"Sync Status\",\n    #           \"value\": \"{{.app.status.sync.status}}\",\n    #           \"short\": true\n    #         },\n    #         {\n    #           \"title\": \"Repository\",\n    #           \"value\": \"{{.app.spec.source.repoURL}}\",\n    #           \"short\": true\n    #         }\n    #         {{range $index, $c := .app.status.conditions}}\n    #         {{if not $index}},{{end}}\n    #         {{if $index}},{{end}}\n    #         {\n    #           \"title\": \"{{$c.type}}\",\n    #           \"value\": \"{{$c.message}}\",\n    #           \"short\": true\n    #         }\n    #         {{end}}\n    #         ]\n    #       }]\n\n  # -- The trigger defines the condition when the notification should be sent\n  ## For more information: https://argocd-notifications.readthedocs.io/en/stable/triggers/\n  triggers: {}\n    # trigger.on-deployed: |\n    #   - description: Application is synced and healthy. Triggered once per commit.\n    #     oncePer: app.status.sync.revision\n    #     send:\n    #     - app-deployed\n    #     when: app.status.operationState.phase in ['Succeeded'] and app.status.health.status == 'Healthy'\n    # trigger.on-health-degraded: |\n    #   - description: Application has degraded\n    #     send:\n    #     - app-health-degraded\n    #     when: app.status.health.status == 'Degraded'\n    # trigger.on-sync-failed: |\n    #   - description: Application syncing has failed\n    #     send:\n    #     - app-sync-failed\n    #     when: app.status.operationState.phase in ['Error', 'Failed']\n    # trigger.on-sync-running: |\n    #   - description: Application is being synced\n    #     send:\n    #     - app-sync-running\n    #     when: app.status.operationState.phase in ['Running']\n    # trigger.on-sync-status-unknown: |\n    #   - description: Application status is 'Unknown'\n    #     send:\n    #     - app-sync-status-unknown\n    #     when: app.status.sync.status == 'Unknown'\n    # trigger.on-sync-succeeded: |\n    #   - description: Application syncing has succeeded\n    #     send:\n    #     - app-sync-succeeded\n    #     when: app.status.operationState.phase in ['Succeeded']\n    #\n    # For more information: https://argocd-notifications.readthedocs.io/en/stable/triggers/#default-triggers\n    # defaultTriggers: |\n    #   - on-sync-status-unknown\n"
            ],
            "verify": false,
            "version": "5.52.0",
            "wait": true,
            "wait_for_jobs": false
          },
          "sensitive_attributes": [],
          "private": "eyJzY2hlbWFfdmVyc2lvbiI6IjEifQ==",
          "dependencies": [
            "kind_cluster.default"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "helm_release",
      "name": "metallb",
      "provider": "provider[\"registry.terraform.io/hashicorp/helm\"]",
      "instances": [
        {
          "schema_version": 1,
          "attributes": {
            "atomic": false,
            "chart": "metallb",
            "cleanup_on_fail": false,
            "create_namespace": true,
            "dependency_update": false,
            "description": null,
            "devel": null,
            "disable_crd_hooks": false,
            "disable_openapi_validation": false,
            "disable_webhooks": false,
            "force_update": false,
            "id": "metallb",
            "keyring": null,
            "lint": false,
            "manifest": null,
            "max_history": 0,
            "metadata": [
              {
                "app_version": "v0.13.12",
                "chart": "metallb",
                "name": "metallb",
                "namespace": "metallb",
                "revision": 1,
                "values": "{\"controller\":{\"affinity\":{},\"enabled\":true,\"extraContainers\":[],\"image\":{\"pullPolicy\":null,\"repository\":\"quay.io/metallb/controller\",\"tag\":null},\"labels\":{},\"livenessProbe\":{\"enabled\":true,\"failureThreshold\":3,\"initialDelaySeconds\":10,\"periodSeconds\":10,\"successThreshold\":1,\"timeoutSeconds\":1},\"logLevel\":\"info\",\"nodeSelector\":{},\"podAnnotations\":{},\"priorityClassName\":\"\",\"readinessProbe\":{\"enabled\":true,\"failureThreshold\":3,\"initialDelaySeconds\":10,\"periodSeconds\":10,\"successThreshold\":1,\"timeoutSeconds\":1},\"resources\":{},\"runtimeClassName\":\"\",\"securityContext\":{\"fsGroup\":65534,\"runAsNonRoot\":true,\"runAsUser\":65534},\"serviceAccount\":{\"annotations\":{},\"create\":true,\"name\":\"\"},\"strategy\":{\"type\":\"RollingUpdate\"},\"tlsCipherSuites\":\"\",\"tlsMinVersion\":\"VersionTLS12\",\"tolerations\":[]},\"crds\":{\"enabled\":true,\"validationFailurePolicy\":\"Fail\"},\"frrk8s\":{\"enabled\":false},\"fullnameOverride\":\"\",\"imagePullSecrets\":[],\"loadBalancerClass\":\"\",\"nameOverride\":\"\",\"prometheus\":{\"controllerMetricsTLSSecret\":\"\",\"metricsPort\":7472,\"namespace\":\"\",\"podMonitor\":{\"additionalLabels\":{},\"annotations\":{},\"enabled\":false,\"interval\":null,\"jobLabel\":\"app.kubernetes.io/name\",\"metricRelabelings\":[],\"relabelings\":[]},\"prometheusRule\":{\"additionalLabels\":{},\"addressPoolExhausted\":{\"enabled\":true,\"labels\":{\"severity\":\"alert\"}},\"addressPoolUsage\":{\"enabled\":true,\"thresholds\":[{\"labels\":{\"severity\":\"warning\"},\"percent\":75},{\"labels\":{\"severity\":\"warning\"},\"percent\":85},{\"labels\":{\"severity\":\"alert\"},\"percent\":95}]},\"annotations\":{},\"bgpSessionDown\":{\"enabled\":true,\"labels\":{\"severity\":\"alert\"}},\"configNotLoaded\":{\"enabled\":true,\"labels\":{\"severity\":\"warning\"}},\"enabled\":false,\"extraAlerts\":[],\"staleConfig\":{\"enabled\":true,\"labels\":{\"severity\":\"warning\"}}},\"rbacPrometheus\":true,\"rbacProxy\":{\"pullPolicy\":null,\"repository\":\"gcr.io/kubebuilder/kube-rbac-proxy\",\"tag\":\"v0.12.0\"},\"scrapeAnnotations\":false,\"serviceAccount\":\"\",\"serviceMonitor\":{\"controller\":{\"additionalLabels\":{},\"annotations\":{},\"tlsConfig\":{\"insecureSkipVerify\":true}},\"enabled\":false,\"interval\":null,\"jobLabel\":\"app.kubernetes.io/name\",\"metricRelabelings\":[],\"relabelings\":[],\"speaker\":{\"additionalLabels\":{},\"annotations\":{},\"tlsConfig\":{\"insecureSkipVerify\":true}}},\"speakerMetricsTLSSecret\":\"\"},\"rbac\":{\"create\":true},\"speaker\":{\"affinity\":{},\"enabled\":true,\"excludeInterfaces\":{\"enabled\":true},\"extraContainers\":[],\"frr\":{\"enabled\":true,\"image\":{\"pullPolicy\":null,\"repository\":\"quay.io/frrouting/frr\",\"tag\":\"8.5.2\"},\"metricsPort\":7473,\"resources\":{}},\"frrMetrics\":{\"resources\":{}},\"image\":{\"pullPolicy\":null,\"repository\":\"quay.io/metallb/speaker\",\"tag\":null},\"labels\":{},\"livenessProbe\":{\"enabled\":true,\"failureThreshold\":3,\"initialDelaySeconds\":10,\"periodSeconds\":10,\"successThreshold\":1,\"timeoutSeconds\":1},\"logLevel\":\"info\",\"memberlist\":{\"enabled\":true,\"mlBindAddrOverride\":\"\",\"mlBindPort\":7946,\"mlSecretKeyPath\":\"/etc/ml_secret_key\"},\"nodeSelector\":{},\"podAnnotations\":{},\"priorityClassName\":\"\",\"readinessProbe\":{\"enabled\":true,\"failureThreshold\":3,\"initialDelaySeconds\":10,\"periodSeconds\":10,\"successThreshold\":1,\"timeoutSeconds\":1},\"reloader\":{\"resources\":{}},\"resources\":{},\"runtimeClassName\":\"\",\"securityContext\":{},\"serviceAccount\":{\"annotations\":{},\"create\":true,\"name\":\"\"},\"startupProbe\":{\"enabled\":true,\"failureThreshold\":30,\"periodSeconds\":5},\"tolerateMaster\":true,\"tolerations\":[],\"updateStrategy\":{\"type\":\"RollingUpdate\"}}}",
                "version": "0.13.12"
              }
            ],
            "name": "metallb",
            "namespace": "metallb",
            "pass_credentials": false,
            "postrender": [],
            "recreate_pods": false,
            "render_subchart_notes": true,
            "replace": false,
            "repository": "https://metallb.github.io/metallb",
            "repository_ca_file": null,
            "repository_cert_file": null,
            "repository_key_file": null,
            "repository_password": null,
            "repository_username": null,
            "reset_values": false,
            "reuse_values": false,
            "set": [],
            "set_list": [],
            "set_sensitive": [],
            "skip_crds": false,
            "status": "deployed",
            "timeout": 300,
            "values": [
              "# Default values for metallb.\n# This is a YAML-formatted file.\n# Declare variables to be passed into your templates.\n\nimagePullSecrets: []\nnameOverride: \"\"\nfullnameOverride: \"\"\nloadBalancerClass: \"\"\n\n# To configure MetalLB, you must specify ONE of the following two\n# options.\n\nrbac:\n  # create specifies whether to install and use RBAC rules.\n  create: true\n\nprometheus:\n  # scrape annotations specifies whether to add Prometheus metric\n  # auto-collection annotations to pods. See\n  # https://github.com/prometheus/prometheus/blob/release-2.1/documentation/examples/prometheus-kubernetes.yml\n  # for a corresponding Prometheus configuration. Alternatively, you\n  # may want to use the Prometheus Operator\n  # (https://github.com/coreos/prometheus-operator) for more powerful\n  # monitoring configuration. If you use the Prometheus operator, this\n  # can be left at false.\n  scrapeAnnotations: false\n\n  # port both controller and speaker will listen on for metrics\n  metricsPort: 7472\n\n  # if set, enables rbac proxy on the controller and speaker to expose\n  # the metrics via tls.\n  # secureMetricsPort: 9120\n\n  # the name of the secret to be mounted in the speaker pod\n  # to expose the metrics securely. If not present, a self signed\n  # certificate to be used.\n  speakerMetricsTLSSecret: \"\"\n\n  # the name of the secret to be mounted in the controller pod\n  # to expose the metrics securely. If not present, a self signed\n  # certificate to be used.\n  controllerMetricsTLSSecret: \"\"\n\n  # prometheus doens't have the permission to scrape all namespaces so we give it permission to scrape metallb's one\n  rbacPrometheus: true\n\n  # the service account used by prometheus\n  # required when \" .Values.prometheus.rbacPrometheus == true \" and \" .Values.prometheus.podMonitor.enabled=true or prometheus.serviceMonitor.enabled=true \"\n  serviceAccount: \"\"\n\n  # the namespace where prometheus is deployed\n  # required when \" .Values.prometheus.rbacPrometheus == true \" and \" .Values.prometheus.podMonitor.enabled=true or prometheus.serviceMonitor.enabled=true \"\n  namespace: \"\"\n\n  # the image to be used for the kuberbacproxy container\n  rbacProxy:\n    repository: gcr.io/kubebuilder/kube-rbac-proxy\n    tag: v0.12.0\n    pullPolicy:\n\n  # Prometheus Operator PodMonitors\n  podMonitor:\n    # enable support for Prometheus Operator\n    enabled: false\n\n    # optional additionnal labels for podMonitors\n    additionalLabels: {}\n\n    # optional annotations for podMonitors\n    annotations: {}\n\n    # Job label for scrape target\n    jobLabel: \"app.kubernetes.io/name\"\n\n    # Scrape interval. If not set, the Prometheus default scrape interval is used.\n    interval:\n\n    # \tmetric relabel configs to apply to samples before ingestion.\n    metricRelabelings: []\n    # - action: keep\n    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'\n    #   sourceLabels: [__name__]\n\n    # \trelabel configs to apply to samples before ingestion.\n    relabelings: []\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\n    #   separator: ;\n    #   regex: ^(.*)$\n    #   target_label: nodename\n    #   replacement: $1\n    #   action: replace\n\n  # Prometheus Operator ServiceMonitors. To be used as an alternative\n  # to podMonitor, supports secure metrics.\n  serviceMonitor:\n    # enable support for Prometheus Operator\n    enabled: false\n\n    speaker:\n      # optional additional labels for the speaker serviceMonitor\n      additionalLabels: {}\n      # optional additional annotations for the speaker serviceMonitor\n      annotations: {}\n      # optional tls configuration for the speaker serviceMonitor, in case\n      # secure metrics are enabled.\n      tlsConfig:\n        insecureSkipVerify: true\n\n    controller:\n      # optional additional labels for the controller serviceMonitor\n      additionalLabels: {}\n      # optional additional annotations for the controller serviceMonitor\n      annotations: {}\n      # optional tls configuration for the controller serviceMonitor, in case\n      # secure metrics are enabled.\n      tlsConfig:\n        insecureSkipVerify: true\n\n    # Job label for scrape target\n    jobLabel: \"app.kubernetes.io/name\"\n\n    # Scrape interval. If not set, the Prometheus default scrape interval is used.\n    interval:\n\n    # \tmetric relabel configs to apply to samples before ingestion.\n    metricRelabelings: []\n    # - action: keep\n    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'\n    #   sourceLabels: [__name__]\n\n    # \trelabel configs to apply to samples before ingestion.\n    relabelings: []\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\n    #   separator: ;\n    #   regex: ^(.*)$\n    #   target_label: nodename\n    #   replacement: $1\n    #   action: replace\n\n  # Prometheus Operator alertmanager alerts\n  prometheusRule:\n    # enable alertmanager alerts\n    enabled: false\n\n    # optional additionnal labels for prometheusRules\n    additionalLabels: {}\n\n    # optional annotations for prometheusRules\n    annotations: {}\n\n    # MetalLBStaleConfig\n    staleConfig:\n      enabled: true\n      labels:\n        severity: warning\n\n    # MetalLBConfigNotLoaded\n    configNotLoaded:\n      enabled: true\n      labels:\n        severity: warning\n\n    # MetalLBAddressPoolExhausted\n    addressPoolExhausted:\n      enabled: true\n      labels:\n        severity: alert\n\n    addressPoolUsage:\n      enabled: true\n      thresholds:\n        - percent: 75\n          labels:\n            severity: warning\n        - percent: 85\n          labels:\n            severity: warning\n        - percent: 95\n          labels:\n            severity: alert\n\n    # MetalLBBGPSessionDown\n    bgpSessionDown:\n      enabled: true\n      labels:\n        severity: alert\n\n    extraAlerts: []\n\n# controller contains configuration specific to the MetalLB cluster\n# controller.\ncontroller:\n  enabled: true\n  # -- Controller log level. Must be one of: `all`, `debug`, `info`, `warn`, `error` or `none`\n  logLevel: info\n  # command: /controller\n  # webhookMode: enabled\n  image:\n    repository: quay.io/metallb/controller\n    tag:\n    pullPolicy:\n  ## @param controller.updateStrategy.type Metallb controller deployment strategy type.\n  ## ref: https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#strategy\n  ## e.g:\n  ## strategy:\n  ##  type: RollingUpdate\n  ##  rollingUpdate:\n  ##    maxSurge: 25%\n  ##    maxUnavailable: 25%\n  ##\n  strategy:\n    type: RollingUpdate\n  serviceAccount:\n    # Specifies whether a ServiceAccount should be created\n    create: true\n    # The name of the ServiceAccount to use. If not set and create is\n    # true, a name is generated using the fullname template\n    name: \"\"\n    annotations: {}\n  securityContext:\n    runAsNonRoot: true\n    # nobody\n    runAsUser: 65534\n    fsGroup: 65534\n  resources: {}\n    # limits:\n      # cpu: 100m\n      # memory: 100Mi\n  nodeSelector: {}\n  tolerations: []\n  priorityClassName: \"\"\n  runtimeClassName: \"\"\n  affinity: {}\n  podAnnotations: {}\n  labels: {}\n  livenessProbe:\n    enabled: true\n    failureThreshold: 3\n    initialDelaySeconds: 10\n    periodSeconds: 10\n    successThreshold: 1\n    timeoutSeconds: 1\n  readinessProbe:\n    enabled: true\n    failureThreshold: 3\n    initialDelaySeconds: 10\n    periodSeconds: 10\n    successThreshold: 1\n    timeoutSeconds: 1\n  tlsMinVersion: \"VersionTLS12\"\n  tlsCipherSuites: \"\"\n\n  extraContainers: []\n\n# speaker contains configuration specific to the MetalLB speaker\n# daemonset.\nspeaker:\n  enabled: true\n  # command: /speaker\n  # -- Speaker log level. Must be one of: `all`, `debug`, `info`, `warn`, `error` or `none`\n  logLevel: info\n  tolerateMaster: true\n  memberlist:\n    enabled: true\n    mlBindPort: 7946\n    mlBindAddrOverride: \"\"\n    mlSecretKeyPath: \"/etc/ml_secret_key\"\n  excludeInterfaces:\n    enabled: true\n  image:\n    repository: quay.io/metallb/speaker\n    tag:\n    pullPolicy:\n  ## @param speaker.updateStrategy.type Speaker daemonset strategy type\n  ## ref: https://kubernetes.io/docs/tasks/manage-daemon/update-daemon-set/\n  ##\n  updateStrategy:\n    ## StrategyType\n    ## Can be set to RollingUpdate or OnDelete\n    ##\n    type: RollingUpdate\n  serviceAccount:\n    # Specifies whether a ServiceAccount should be created\n    create: true\n    # The name of the ServiceAccount to use. If not set and create is\n    # true, a name is generated using the fullname template\n    name: \"\"\n    annotations: {}\n  securityContext: {}\n  ## Defines a secret name for the controller to generate a memberlist encryption secret\n  ## By default secretName: {{ \"metallb.fullname\" }}-memberlist\n  ##\n  # secretName:\n  resources: {}\n    # limits:\n      # cpu: 100m\n      # memory: 100Mi\n  nodeSelector: {}\n  tolerations: []\n  priorityClassName: \"\"\n  affinity: {}\n  ## Selects which runtime class will be used by the pod.\n  runtimeClassName: \"\"\n  podAnnotations: {}\n  labels: {}\n  livenessProbe:\n    enabled: true\n    failureThreshold: 3\n    initialDelaySeconds: 10\n    periodSeconds: 10\n    successThreshold: 1\n    timeoutSeconds: 1\n  readinessProbe:\n    enabled: true\n    failureThreshold: 3\n    initialDelaySeconds: 10\n    periodSeconds: 10\n    successThreshold: 1\n    timeoutSeconds: 1\n  startupProbe:\n    enabled: true\n    failureThreshold: 30\n    periodSeconds: 5\n  # frr contains configuration specific to the MetalLB FRR container,\n  # for speaker running alongside FRR.\n  frr:\n    enabled: true\n    image:\n      repository: quay.io/frrouting/frr\n      tag: 8.5.2\n      pullPolicy:\n    metricsPort: 7473\n    resources: {}\n\n    # if set, enables a rbac proxy sidecar container on the speaker to\n    # expose the frr metrics via tls.\n    # secureMetricsPort: 9121\n\n\n  reloader:\n    resources: {}\n\n  frrMetrics:\n    resources: {}\n\n  extraContainers: []\n\ncrds:\n  enabled: true\n  validationFailurePolicy: Fail\n\n# frrk8s contains the configuration related to using an frrk8s instance\n# (github.com/metallb/frr-k8s) as the backend for the BGP implementation.\n# This allows configuring additional frr parameters in combination to those\n# applied by MetalLB.\nfrrk8s:\n  # if set, enables frrk8s as a backend. This is mutually exclusive to frr\n  # mode.\n  enabled: false"
            ],
            "verify": false,
            "version": "0.13.12",
            "wait": true,
            "wait_for_jobs": false
          },
          "sensitive_attributes": [],
          "private": "eyJzY2hlbWFfdmVyc2lvbiI6IjEifQ==",
          "dependencies": [
            "kind_cluster.default"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "kind_cluster",
      "name": "default",
      "provider": "provider[\"registry.terraform.io/lukekalbfleisch/kind\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "client_certificate": "-----BEGIN CERTIFICATE-----\nMIIDITCCAgmgAwIBAgIIVY7q+v3UG8kwDQYJKoZIhvcNAQELBQAwFTETMBEGA1UE\nAxMKa3ViZXJuZXRlczAeFw0yNDAxMTcxMDQyMjJaFw0yNTAxMTYxMDQyMjNaMDQx\nFzAVBgNVBAoTDnN5c3RlbTptYXN0ZXJzMRkwFwYDVQQDExBrdWJlcm5ldGVzLWFk\nbWluMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAzEoLPLo0oflb1zW+\n0RKqbVv/hDQiveK/1yCI1eqemd3QY257J5QkZ3s+hY6f93wNFPOTiC1qf4c245yx\nME5rd6nmqn5oknXvpncl//gXq6w8unpoZsfuxjh8MHRs1XKHnFO96vnXTI3efYo7\nwXoJf2ELWdx1wU2sgT7S5SnHj1NTcmDTGnUETyDBSgHuC543aO8XvU17XtsE6rfM\nb0mduBrwvimpaTtF3kh/2HlT0VbhaTxAyWh2dczv2P2d10aGq9XAUBBQ18Ge0v+e\n/VokAOEhEdHkMjciFGkb/Na58wnv5xt/J2UybqsUAUou1ecxupF6nyORgLFl8xH7\nGgY5MwIDAQABo1YwVDAOBgNVHQ8BAf8EBAMCBaAwEwYDVR0lBAwwCgYIKwYBBQUH\nAwIwDAYDVR0TAQH/BAIwADAfBgNVHSMEGDAWgBSRDIGIH91xCFPMDzyf7YNFJD0A\nizANBgkqhkiG9w0BAQsFAAOCAQEAmY0iwfOrykwj09O1dk/BGDs30lkRo0GtwNBU\np/1SFO8H/JjXUsVOxNCxa2GZIQh2FXjy/x4nY+y2/2ywpJq7Gl59Met5/Fu+49kV\nUaY9UxgPLaqWdmJx7z9mwqGbAL3e0vG5efsvV0j6XnOPcB0nl9Lo39Z9G/cBERYL\ny0SP0MpCy4mN4wegS9qDAtCSBERAYUdcnbx7vL+Ywx+SBC4HPr/j/wDmU2OWqdX2\naUoiKsXKYwsBQvI1CZ8KHGV7xc4CwpVEUym2hRRaOPgXLeZVNMd2DHkXQejgbrVJ\nYV6tOdtolZKLg0k7MIL0m/trToCd99SoobjzWq1af4O5M1P5oA==\n-----END CERTIFICATE-----\n",
            "client_key": "-----BEGIN RSA PRIVATE KEY-----\nMIIEogIBAAKCAQEAzEoLPLo0oflb1zW+0RKqbVv/hDQiveK/1yCI1eqemd3QY257\nJ5QkZ3s+hY6f93wNFPOTiC1qf4c245yxME5rd6nmqn5oknXvpncl//gXq6w8unpo\nZsfuxjh8MHRs1XKHnFO96vnXTI3efYo7wXoJf2ELWdx1wU2sgT7S5SnHj1NTcmDT\nGnUETyDBSgHuC543aO8XvU17XtsE6rfMb0mduBrwvimpaTtF3kh/2HlT0VbhaTxA\nyWh2dczv2P2d10aGq9XAUBBQ18Ge0v+e/VokAOEhEdHkMjciFGkb/Na58wnv5xt/\nJ2UybqsUAUou1ecxupF6nyORgLFl8xH7GgY5MwIDAQABAoH/XJEwhlPZhYt4EgxI\nanQKxYLqRP1awK/+PXyAhG3Lb+0Sf/uP0l9OH5W4vsvhsEKFqviz4QCzQRYDyR83\nKqxeBdr2qkqdF8BY1xUmItZH44IoAld09x2hFDCEFXCr4AwFZtCLPe33oLioGDqK\n2SBzLUTL3+wFb5HCmM3ewfgsdMR9sWSMJWblvBcD4oTRGsMnZS2E0JCAzySZsII0\n/o/R8RN3pANffAMVG0xknyemKYb7HDcWnpL2npAMZvhga4nKSzYAdS/YzMwMIPVx\nhcstHVZyJ+qAdBJhb/q8Cods3uTebT5y3qu3nZ5ip5fdAZYX4ntZ1d8X781tCnK2\nxpaBAoGBAN2V0UB6Eovprp+kIL+NM8eO/pVpVrragTCeJ4nmgS1jIol9WG9vTHPb\nqM64Vq3CVVE0emrLeMmu1N1tTQK0y9CiTY0WfUFXAvJw0slrGgTtnsDDAsFo0uBR\nWZMU0mfXnRy4yrvg++/GNGetvdVF8sWhSt9Axm+nJQ5AsmP5OBURAoGBAOwEiqDi\ntAeK/bfFzfpcCMNdnwssDlON9Z1/FObxBG84goj7V9EnbZG/k4390ppTB94/W+Fk\nMs10JyZcM00ILzZobRClS8R/Ut9OTKzI5k2fXdQvHSH4rXNlk5GwQ0qauG8ppxtM\nxKeLCFNyffci0xlndG6Hzdc9FpOg/wcvlloDAoGBAJopzbfqNlUZc+CltBMRDmFc\n30rcSvcdZpyIrEGJGX6WZkT57Dk7WhmRTdTk84lYlzs0I6eBgLvw4zBbByVG3DZ/\ndS0f5G09OFOsozp3RVcAa175TkV5xJ1Ee+mUpv6QSELfXk1wrleXsoYfnt9vFwTg\nvtKsomK0C1frMzHTBm3BAoGBAOvvVk2ikul/bkaJvHDsE+6I5naaoe+w17mL0m9S\nvGYajZM1/wGccpHiezTUafXPzuvDVqwoKAjudjyIqtf0uI2hMnYTZ3yU81k7wrZv\nVpp+F6QHPQ46s9nJzw0ysPIhXQHzF5PY2DXMbJROvdGRxIl+vAvKQmFKNgl6Rbb/\nQUPVAoGAYSQ/pjCnQPfLsuq7KKhRIc+Yt11988azuCk5EdXOTq/Z4N22/X7b7kR9\noiOqB0b5EaczFaBftvaInHm5URIeSGqFC1YFbOZ0vC+zjU71l8MP+EoqidT7onVp\n4jUkB5zNzTtE5lPGhPaJujefZO0mQrQhUlB45owFTSnjJBCZkJY=\n-----END RSA PRIVATE KEY-----\n",
            "cluster_ca_certificate": "-----BEGIN CERTIFICATE-----\nMIIC/jCCAeagAwIBAgIBADANBgkqhkiG9w0BAQsFADAVMRMwEQYDVQQDEwprdWJl\ncm5ldGVzMB4XDTI0MDExNzEwNDIyMloXDTM0MDExNDEwNDIyMlowFTETMBEGA1UE\nAxMKa3ViZXJuZXRlczCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBALFd\nKj/4HeYKcdjfTswKAg6a1CaIpFXFMOI2/hfWb4esqG5lzMF8P0ggy3ehb5o8FnqF\n/rzNCNRDgEjkZI84Yu2eAFg71Ddsey7L+jWZveOaKYR1gQtP+A3Re1vXa5U5vKta\ndOdrv/8CEZek1IJ4R7nt5ST81OsZ1DzSFP+sMHtxzv2dQAfBuDuSHbbGQqsDry3G\nkqJXLn+cJC9H5Votd3VVZuz/v1UtBMt5OLbXcIzRyAzy6I3/8ee/jpyXOD79MmQI\nVYpT4nbluL6YHCfBI6PC39BtcISFuVFzhQcMcCCp34yoV08rvzQS50DGtQ4yNpax\nazSfg9Hpg3VpxQSdjDsCAwEAAaNZMFcwDgYDVR0PAQH/BAQDAgKkMA8GA1UdEwEB\n/wQFMAMBAf8wHQYDVR0OBBYEFJEMgYgf3XEIU8wPPJ/tg0UkPQCLMBUGA1UdEQQO\nMAyCCmt1YmVybmV0ZXMwDQYJKoZIhvcNAQELBQADggEBAJ+O5HFjU6xRVrjrIJQg\n+KzqrdnKxnx9icfNn2DytQaINVbWfWYocXPyjHJ60gu9Cgw1hEBY8ysQ9YXnrhQC\nI5Rsq1sFn5OzFM3EX0nR80hMJITQs85SoEmlrxU9wGse6q8QT/GlCefjTl/udM0U\npn/3fxSKOcvB92BoNLzQLhNpM4pg8psnpPrTOH7Em1SWWzJDlaFsTaAIMwisKwlZ\nGMUkww9ZkwrMLuQAPmZnNNl8Z20dLr7AmtngOxXkxR3wC453MMqsD6AoWKFLjAhs\nX5MZgKHDcTwYimJmgM6/X8AfTpAHCSjXNTnpel/6PQUTk0i3aQxZCwXGPiCSCFhp\nm9E=\n-----END CERTIFICATE-----\n",
            "endpoint": "https://dev-1-control-plane:6443",
            "id": "dev-1-kindest/node:v1.27.3",
            "kind_config": "kind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nkubeadmConfigPatches:\n- |-\n  kind: ClusterConfiguration\n  # configure controller-manager bind address\n  controllerManager:\n    extraArgs:\n      bind-address: 0.0.0.0\n  # configure etcd metrics listen address\n  etcd:\n    local:\n      extraArgs:\n        listen-metrics-urls: http://0.0.0.0:2381\n  # configure scheduler bind address\n  scheduler:\n    extraArgs:\n      bind-address: 0.0.0.0\n- |-\n  kind: KubeProxyConfiguration\n  # configure proxy metrics bind address\n  metricsBindAddress: 0.0.0.0\nnodes:\n- role: control-plane\n- role: worker\n  labels:\n    ingress : worker\n  extraPortMappings:\n    - containerPort: 80\n      hostPort: 80\n      protocol: TCP\n    - containerPort: 443\n      hostPort: 443\n      protocol: TCP\n- role: worker\n- role: worker\n",
            "kubeconfig": "apiVersion: v1\nclusters:\n- cluster:\n    certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUMvakNDQWVhZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRJME1ERXhOekV3TkRJeU1sb1hEVE0wTURFeE5ERXdOREl5TWxvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBTEZkCktqLzRIZVlLY2RqZlRzd0tBZzZhMUNhSXBGWEZNT0kyL2hmV2I0ZXNxRzVsek1GOFAwZ2d5M2VoYjVvOEZucUYKL3J6TkNOUkRnRWprWkk4NFl1MmVBRmc3MURkc2V5N0wraldadmVPYUtZUjFnUXRQK0EzUmUxdlhhNVU1dkt0YQpkT2Rydi84Q0VaZWsxSUo0UjdudDVTVDgxT3NaMUR6U0ZQK3NNSHR4enYyZFFBZkJ1RHVTSGJiR1Fxc0RyeTNHCmtxSlhMbitjSkM5SDVWb3RkM1ZWWnV6L3YxVXRCTXQ1T0xiWGNJelJ5QXp5NkkzLzhlZS9qcHlYT0Q3OU1tUUkKVllwVDRuYmx1TDZZSENmQkk2UEMzOUJ0Y0lTRnVWRnpoUWNNY0NDcDM0eW9WMDhydnpRUzUwREd0UTR5TnBheAphelNmZzlIcGczVnB4UVNkakRzQ0F3RUFBYU5aTUZjd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0hRWURWUjBPQkJZRUZKRU1nWWdmM1hFSVU4d1BQSi90ZzBVa1BRQ0xNQlVHQTFVZEVRUU8KTUF5Q0NtdDFZbVZ5Ym1WMFpYTXdEUVlKS29aSWh2Y05BUUVMQlFBRGdnRUJBSitPNUhGalU2eFJWcmpySUpRZworS3pxcmRuS3hueDlpY2ZObjJEeXRRYUlOVmJXZldZb2NYUHlqSEo2MGd1OUNndzFoRUJZOHlzUTlZWG5yaFFDCkk1UnNxMXNGbjVPekZNM0VYMG5SODBoTUpJVFFzODVTb0VtbHJ4VTl3R3NlNnE4UVQvR2xDZWZqVGwvdWRNMFUKcG4vM2Z4U0tPY3ZCOTJCb05MelFMaE5wTTRwZzhwc25wUHJUT0g3RW0xU1dXekpEbGFGc1RhQUlNd2lzS3dsWgpHTVVrd3c5Wmt3ck1MdVFBUG1abk5ObDhaMjBkTHI3QW10bmdPeFhreFIzd0M0NTNNTXFzRDZBb1dLRkxqQWhzClg1TVpnS0hEY1R3WWltSm1nTTYvWDhBZlRwQUhDU2pYTlRucGVsLzZQUVVUazBpM2FReFpDd1hHUGlDU0NGaHAKbTlFPQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==\n    server: https://dev-1-control-plane:6443\n  name: kind-dev-1\ncontexts:\n- context:\n    cluster: kind-dev-1\n    user: kind-dev-1\n  name: kind-dev-1\ncurrent-context: kind-dev-1\nkind: Config\npreferences: {}\nusers:\n- name: kind-dev-1\n  user:\n    client-certificate-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURJVENDQWdtZ0F3SUJBZ0lJVlk3cSt2M1VHOGt3RFFZSktvWklodmNOQVFFTEJRQXdGVEVUTUJFR0ExVUUKQXhNS2EzVmlaWEp1WlhSbGN6QWVGdzB5TkRBeE1UY3hNRFF5TWpKYUZ3MHlOVEF4TVRZeE1EUXlNak5hTURReApGekFWQmdOVkJBb1REbk41YzNSbGJUcHRZWE4wWlhKek1Sa3dGd1lEVlFRREV4QnJkV0psY201bGRHVnpMV0ZrCmJXbHVNSUlCSWpBTkJna3Foa2lHOXcwQkFRRUZBQU9DQVE4QU1JSUJDZ0tDQVFFQXpFb0xQTG8wb2ZsYjF6VysKMFJLcWJWdi9oRFFpdmVLLzF5Q0kxZXFlbWQzUVkyNTdKNVFrWjNzK2hZNmY5M3dORlBPVGlDMXFmNGMyNDV5eApNRTVyZDZubXFuNW9rblh2cG5jbC8vZ1hxNnc4dW5wb1pzZnV4amg4TUhSczFYS0huRk85NnZuWFRJM2VmWW83CndYb0pmMkVMV2R4MXdVMnNnVDdTNVNuSGoxTlRjbURUR25VRVR5REJTZ0h1QzU0M2FPOFh2VTE3WHRzRTZyZk0KYjBtZHVCcnd2aW1wYVR0RjNraC8ySGxUMFZiaGFUeEF5V2gyZGN6djJQMmQxMGFHcTlYQVVCQlExOEdlMHYrZQovVm9rQU9FaEVkSGtNamNpRkdrYi9OYTU4d252NXh0L0oyVXlicXNVQVVvdTFlY3h1cEY2bnlPUmdMRmw4eEg3CkdnWTVNd0lEQVFBQm8xWXdWREFPQmdOVkhROEJBZjhFQkFNQ0JhQXdFd1lEVlIwbEJBd3dDZ1lJS3dZQkJRVUgKQXdJd0RBWURWUjBUQVFIL0JBSXdBREFmQmdOVkhTTUVHREFXZ0JTUkRJR0lIOTF4Q0ZQTUR6eWY3WU5GSkQwQQppekFOQmdrcWhraUc5dzBCQVFzRkFBT0NBUUVBbVkwaXdmT3J5a3dqMDlPMWRrL0JHRHMzMGxrUm8wR3R3TkJVCnAvMVNGTzhIL0pqWFVzVk94TkN4YTJHWklRaDJGWGp5L3g0blkreTIvMnl3cEpxN0dsNTlNZXQ1L0Z1KzQ5a1YKVWFZOVV4Z1BMYXFXZG1KeDd6OW13cUdiQUwzZTB2RzVlZnN2VjBqNlhuT1BjQjBubDlMbzM5WjlHL2NCRVJZTAp5MFNQME1wQ3k0bU40d2VnUzlxREF0Q1NCRVJBWVVkY25ieDd2TCtZd3grU0JDNEhQci9qL3dEbVUyT1dxZFgyCmFVb2lLc1hLWXdzQlF2STFDWjhLSEdWN3hjNEN3cFZFVXltMmhSUmFPUGdYTGVaVk5NZDJESGtYUWVqZ2JyVkoKWVY2dE9kdG9sWktMZzBrN01JTDBtL3RyVG9DZDk5U29vYmp6V3ExYWY0TzVNMVA1b0E9PQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==\n    client-key-data: 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\n",
            "kubeconfig_path": "/Users/manfredkollges/Documents/github/kind/terraform/dev-1-config",
            "name": "dev-1",
            "node_image": "kindest/node:v1.27.3",
            "timeouts": null,
            "wait_for_ready": false
          },
          "sensitive_attributes": [],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjozMDAwMDAwMDAwMDAsImRlbGV0ZSI6MzAwMDAwMDAwMDAwLCJ1cGRhdGUiOjMwMDAwMDAwMDAwMH19"
        }
      ]
    },
    {
      "mode": "managed",
      "type": "kubectl_manifest",
      "name": "deploy-metallb",
      "provider": "provider[\"registry.terraform.io/gavinbunney/kubectl\"]",
      "instances": [
        {
          "index_key": "/apis/metallb.io/v1beta1/namespaces/metallb/ipaddresspools/example",
          "schema_version": 1,
          "attributes": {
            "api_version": "metallb.io/v1beta1",
            "apply_only": false,
            "force_conflicts": false,
            "force_new": false,
            "id": "/apis/metallb.io/v1beta1/namespaces/metallb/ipaddresspools/example",
            "ignore_fields": null,
            "kind": "IPAddressPool",
            "live_manifest_incluster": "d00e69d406bb895a6cfb97d8a32895cb7002f3a415574719adba8b6b51286163",
            "live_uid": "f151a780-3744-4ae8-bb29-50c8bfd5487e",
            "name": "example",
            "namespace": "metallb",
            "override_namespace": null,
            "sensitive_fields": null,
            "server_side_apply": false,
            "timeouts": null,
            "uid": "f151a780-3744-4ae8-bb29-50c8bfd5487e",
            "validate_schema": true,
            "wait": null,
            "wait_for_rollout": true,
            "yaml_body": "apiVersion: metallb.io/v1beta1\nkind: IPAddressPool\nmetadata:\n  name: example\n  namespace: metallb\nspec:\n  addresses:\n  - 172.18.0.200-172.18.0.250\n",
            "yaml_body_parsed": "apiVersion: metallb.io/v1beta1\nkind: IPAddressPool\nmetadata:\n  name: example\n  namespace: metallb\nspec:\n  addresses:\n  - 172.18.0.200-172.18.0.250\n",
            "yaml_incluster": "d00e69d406bb895a6cfb97d8a32895cb7002f3a415574719adba8b6b51286163"
          },
          "sensitive_attributes": [],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjo2MDAwMDAwMDAwMDB9LCJzY2hlbWFfdmVyc2lvbiI6IjEifQ==",
          "dependencies": [
            "data.kubectl_file_documents.metallb",
            "helm_release.metallb"
          ]
        },
        {
          "index_key": "/apis/metallb.io/v1beta1/namespaces/metallb/l2advertisements/empty",
          "schema_version": 1,
          "attributes": {
            "api_version": "metallb.io/v1beta1",
            "apply_only": false,
            "force_conflicts": false,
            "force_new": false,
            "id": "/apis/metallb.io/v1beta1/namespaces/metallb/l2advertisements/empty",
            "ignore_fields": null,
            "kind": "L2Advertisement",
            "live_manifest_incluster": "63316921fa1d2d8fcb1f3f7fe6ac2ee2ffd6379c37b6ba9106ad4e79484e5094",
            "live_uid": "e6f20601-0b6f-4266-8394-4b79d9825780",
            "name": "empty",
            "namespace": "metallb",
            "override_namespace": null,
            "sensitive_fields": null,
            "server_side_apply": false,
            "timeouts": null,
            "uid": "e6f20601-0b6f-4266-8394-4b79d9825780",
            "validate_schema": true,
            "wait": null,
            "wait_for_rollout": true,
            "yaml_body": "apiVersion: metallb.io/v1beta1\nkind: L2Advertisement\nmetadata:\n  name: empty\n  namespace: metallb\nspec: {}\n",
            "yaml_body_parsed": "apiVersion: metallb.io/v1beta1\nkind: L2Advertisement\nmetadata:\n  name: empty\n  namespace: metallb\nspec: {}\n",
            "yaml_incluster": "63316921fa1d2d8fcb1f3f7fe6ac2ee2ffd6379c37b6ba9106ad4e79484e5094"
          },
          "sensitive_attributes": [],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjo2MDAwMDAwMDAwMDB9LCJzY2hlbWFfdmVyc2lvbiI6IjEifQ==",
          "dependencies": [
            "data.kubectl_file_documents.metallb",
            "helm_release.metallb"
          ]
        }
      ]
    }
  ],
  "check_results": null
}
